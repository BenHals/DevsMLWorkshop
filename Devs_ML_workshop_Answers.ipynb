{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Devs-ML-workshop-Answers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c747c885"
      },
      "source": [
        "# Machine Learning for text classification"
      ],
      "id": "c747c885"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab873446"
      },
      "source": [
        "Lets set the stage! After a year full of COVID disruptions the university has made all exams online. Celebrations all around! But wait, what about the cheating you say? Fortunately (or unfortunately), the uni has predicted this, and hired a crack team of Designated Exam Validators (or DEVs for short) to check for plagarism. And you've just been hired! And of course, you're going to use ML to do all your work for you!"
      ],
      "id": "ab873446"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96bd1fa9"
      },
      "source": [
        "# Problem Statement"
      ],
      "id": "96bd1fa9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "407bc8ef"
      },
      "source": [
        "Given a sentence, we need to find the probability a sentence came from each possible source."
      ],
      "id": "407bc8ef"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "796e54a3"
      },
      "source": [
        "For example, given the text \"The University of Auckland began as a constituent college of the University of New Zealand, founded on 23 May 1883 as Auckland University College\" the probability for \"Wikipedia\" = 0.9 and \"Twitter\" = 0.1"
      ],
      "id": "796e54a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dba17488"
      },
      "source": [
        "We need to build a machine learning classifier to do this for us!"
      ],
      "id": "dba17488"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b07db5d4"
      },
      "source": [
        "# 1) Collect data"
      ],
      "id": "b07db5d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5d6b023"
      },
      "source": [
        "First, we need some data from each source. This will allow us to find patterns, i.e., learn what makes *wikipedia* sentences different to *twitter* sentences\n",
        "\n",
        "This is normally the *most important part of ML*, but we've taken care of it! In the github repo there is a data folder containing some datasets from different sources.\n",
        "\n",
        "First we will just load this in."
      ],
      "id": "a5d6b023"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a87ce678",
        "outputId": "f2770ecc-f8da-4451-fb5a-c9712571d000"
      },
      "source": [
        "# Load in some libraries\n",
        "# The pathlib library makes handling filepaths easier, letting us open data files.\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Set the folder containing our data\n",
        "data_path = pathlib.Path('data')\n",
        "\n",
        "# Setup availiable filenames\n",
        "chess_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/chess.txt\"\n",
        "music_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/music.txt\"\n",
        "happy_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/happy_topical_chat.txt\"\n",
        "trumpspeech_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/trumpSpeech.txt\"\n",
        "wallstreetbets_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/wallstreetbets_comments.txt\"\n",
        "javascript_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/javascript.txt\"\n",
        "shakespeare_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/shakespeare.txt\"\n",
        "wiki_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/wiki.txt\"\n",
        "twitter_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/twitter2.txt\"\n",
        "\n",
        "# Just a helper function, don't worry about this one!\n",
        "def get_file_or_cache(path):\n",
        "    cache = None\n",
        "    def get():\n",
        "        nonlocal cache\n",
        "        if cache is None:\n",
        "            # with open(path, 'r', encoding='utf8') as f:\n",
        "            #     cache = f.readlines()\n",
        "            print(path)\n",
        "            df = pd.read_csv(path, delimiter = \"\\n\")\n",
        "            cache = list([str(x[0]) for x in df.values])\n",
        "        return cache\n",
        "    return (get, path.split('/')[-1].split('.')[0])\n",
        "\n",
        "# Construct a list of possible files and their names\n",
        "data_files = [get_file_or_cache(x) for x in [chess_filename, music_filename, happy_filename, trumpspeech_filename, javascript_filename, shakespeare_filename, wiki_filename]]\n",
        "\n",
        "# Construct dataset of sentences and labels from every source\n",
        "# Note! Rather than using the actual names for the datasets, we give each one\n",
        "# its own numeric ID.\n",
        "# We record these so we can swap between human readable name and ID.\n",
        "X = []\n",
        "Y = []\n",
        "source_IDs = {}\n",
        "ID_to_source = {}\n",
        "source_ID = 0\n",
        "for source_constructor, source in data_files:\n",
        "    source_IDs[source] = source_ID\n",
        "    ID_to_source[source_ID] = source\n",
        "    lines = source_constructor()\n",
        "    for line in lines:\n",
        "        X.append(line)\n",
        "        Y.append(source_ID)\n",
        "    source_ID += 1\n",
        "\n",
        "observations = list(zip(X, Y))"
      ],
      "id": "a87ce678",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/chess.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/music.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/happy_topical_chat.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/trumpSpeech.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/javascript.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/shakespeare.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/wiki.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aeb8f97"
      },
      "source": [
        "Now we have our data loaded into X, a list of sentences, and Y, a list of the sources each sentence came from.\n",
        "Lets check out some sample lines from each source!"
      ],
      "id": "0aeb8f97"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce1dc4fc"
      },
      "source": [
        "inspect_index = int(random.random() * len(observations))\n",
        "for index, (line, source) in enumerate(observations[inspect_index:inspect_index+5]):\n",
        "    print(\"Observation\", index)\n",
        "    print(\"X{} = \".format(index), line)\n",
        "    # Source is stored as a number, representing the source (explained later!)\n",
        "    print(\"Y{} =\".format(index), source, \"Source name:\", ID_to_source[source])\n",
        "    print('-----------')"
      ],
      "id": "ce1dc4fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18b3725b"
      },
      "source": [
        "# How do we *actually* predict?\n",
        "\n",
        "The goal is to learn a model which can tell the difference between classes. For example, consider sentences from Wikipedia (W) or Twitter (T). If we show our model a new input sentence (X) where we do NOT know the origin, our model should be able to tell where the sentence came from. \n",
        "\n",
        "Formally, the $i^{th}$ input is called $x_{i}$ and its true class is called $y_{i}$.\n",
        "For us, $x_{i}$ is the $i^{th}$ sentence we need to label and $y_{i}$ is where this sentence actually came from.\n",
        "Our model predicts some label, $\\hat y_{i}$, for this sentence (so *wikipedia* or *twitter*).\n",
        "We want to train it so that *our* label, $\\hat y_{n}$, is close to the *true* label, $y_{i}$.\n",
        "\n",
        "There are many, many different ways to do this prediction, and we will look at a simple one called Naive Bayes.\n",
        "\n",
        "Lets true a manual version to get the idea:\n"
      ],
      "id": "18b3725b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95203343"
      },
      "source": [
        "# Pick two random labels to choose from\n",
        "g, label_opt1 = random.choice(data_files)\n",
        "g, label_opt2 = random.choice(data_files)\n",
        "\n",
        "# Select a random observations from a random source\n",
        "source_gen, true_label = random.choice(data_files)\n",
        "\n",
        "# Shuffle labels\n",
        "label_options = [label_opt1, label_opt2, true_label]\n",
        "random.shuffle(label_options)\n",
        "\n",
        "lines = source_gen()\n",
        "line = random.choice(lines)\n",
        "print(line)\n",
        "predicted_label = input(\"Where did this sentence come from? {} or {} or {}?\\n\".format(*label_options))\n",
        "\n",
        "#****** YOUR CODE HERE! ***********\n",
        "# First challenge to get you started:\n",
        "# How can we tell if the predicted label (what the user typed for now)\n",
        "# matches the true label?\n",
        "prediction_iscorrect = predicted_label == true_label\n",
        "#***********************************\n",
        "\n",
        "# Now we use whether or not the prediction was correct to inform the user.\n",
        "print(\"You were\", \"Right\" if prediction_iscorrect else \"Wrong\", \"!\")\n",
        "print(\"Your predicted label (y^) was: {}. The true label (y) was {}\".format(predicted_label, true_label))"
      ],
      "id": "95203343",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e71eed05"
      },
      "source": [
        "## Using a model to make a prediction\n",
        " \n",
        " How can we *train the computer* to predict a class for us? \n",
        " Lets think of a simple example: We are given a sentence ($x$), and need to predict the most likely class ($\\hat y$), Wikipedia (W) or Twitter (T). We start by asking a friend what class they think is most likely and they say Wikipedia has a 70% chance and Twitter and 30%. \n",
        "\n",
        " In mathmatical terms we can rewrite the \"_probability that the class is Wikipedia given this specific sentence $x$ , $\\hat y = W$, is 0.7_\" as $p(\\hat y = W | x) = 0.7$. Similarly, we can write $p(\\hat y = T | x) = 0.3$ for the sentence being from twitter.\n",
        "\n",
        " Now it is pretty easy to make a prediction, Wikipedia has a 70% chance and Twitter only has a 30% chance so we should predict Wikipedia! \n",
        "\n",
        " ### But how did our friend come up with $p(\\hat y| x)$ in the first place?\n",
        " This is basically what the computer needs to do, find the probability of a sentence coming from each source.\n",
        " This is what Naive Bayes solves!\n",
        "\n",
        " ## Naive Bayes - Probability theory (spooky)\n",
        "\n",
        " Note: Naive Bayes is pretty simple, and is quite intuitive when you wrap your head around it, but if this is your first introduction to probability it can be quite confusing! If you don't understand at first don't get discoraged! I find that drawing diagrams and thinking about it from a few directions helps really understand.\n",
        "\n",
        " The end goal of a model is to calculate $p(y = C|X=x)$. In plain english, this can be read as calculate the _probability that the true class of the input is C given what we know about the sentence_. For a concrete example, lets use the sentence \"The University of Auckland was founded on 23 May 1883\". We want to predict the probability that $y = Wikipedia$ or $y = Twitter$ given that the sentence $x$ = \"The University of Auckland was founded on 23 May 1883\". This can be difficult to calculate, and isn't really how we think about things as humans.\n",
        "\n",
        " Instead of calculating this directly, _Bayes Theorum_ gives us a way to swap things around.\n",
        "\n",
        " $$ p(y=C|X=x) = p(X=x|y=C)p(y=C)$$\n",
        "\n",
        " This is like thinking \"what is the probability that the sentence was found on twitter (or wikipedia)\". This is a bit more natural to think about, and it turns out easier to calculate.\n",
        " \n",
        " ## Intuition\n",
        " Instead of directly trying to work out the probability of wikipedia or twitter, lets look at indivdual words. If we see the words \"follow me!\" we can say twitter has a pretty high probability. On the other hand, if we see the words \"Auckland[1][2]\" we could say wikipedia has a high probability. How did we do this?\n",
        " \n",
        " We looked at the probability of each word (or few words) coming from each source! We know that 'following' is something people on twitter do, so if we see this word we know it is more likely to have come from twitter than wikipedia. This is the $p(x=X|y=C)$ in the formula!\n",
        " \n",
        "Lets see if we can use this to do some machine learning!"
      ],
      "id": "e71eed05"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8624bc10"
      },
      "source": [
        "# Exercise 1: Calculate word probabilities"
      ],
      "id": "8624bc10"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8923fd5b"
      },
      "source": [
        "We need to *learn* 2 things from our data.\n",
        "1. What is the probability of seeing each word overall?\n",
        "    1. This is $p(x=X)$, or the *probability* that a word is X\n",
        "    2. e.g p('and') is high, it occurs a lot! But p('founded') is low, it doesn't occur too often.\n",
        "2. What is the probability of seeing each word *from each source*?\n",
        "    1. This is $p(x=X|y=Y)$, or the *probability* that a word is X if we *know* it is from e.g wikipedia\n",
        "    2. e.g. p('founded'|Twitter) might be pretty low, its an uncommon word on twitter. But p('founded'|Wikipedia) is high, it occurs all the time on wikipedia!"
      ],
      "id": "8923fd5b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL8q3vvUQSJa"
      },
      "source": [
        "## Coding Challenge 1 - Calculate the probability of words\n",
        "\n",
        "Lets start simple - How can we calculate the probability of a word occuring?\n",
        "\n",
        "One way to think about this: We count combine all our training data into a huge list of words. Lets say we have $N$ words overall (including all repetitions). Then I tell you a word, e.g., \"hello\". If you closed your eyes and pointed to a random word in the list, what are the chances you would end up pointing to the word \"hello\"?\n",
        "\n",
        "We can calulate this as: The number of times a word occurs in the training data, divided by the total number of words.\n",
        "\n",
        "### First lets count each word"
      ],
      "id": "wL8q3vvUQSJa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCmieuDQQQQ7"
      },
      "source": [
        "  def get_word_counts(training_sentences):\n",
        "    # fill word_counts so it contains the number of times each word occurs.\n",
        "    # word_counts = {\n",
        "    #       'and': 700,\n",
        "    #       'founded': 15,\n",
        "    #       ....\n",
        "    #}\n",
        "    word_counts = {}\n",
        "    total_num_words = 0\n",
        "\n",
        "    # We want to look though all the training sentences,\n",
        "    for sentence in training_sentences:\n",
        "\n",
        "      # We can use .split to split each sentence into words\n",
        "      words = sentence.split()\n",
        "\n",
        "      for word in words:\n",
        "        # Increment the total\n",
        "        total_num_words += 1\n",
        "\n",
        "        # Add word to dictionary if not already in it\n",
        "        if word not in word_counts:\n",
        "          word_counts[word] = 0\n",
        "        \n",
        "        # Add to the count for the word\n",
        "        word_counts[word] += 1\n",
        "    return word_counts, total_num_words\n",
        "\n",
        "word_counts, total_num_words = get_word_counts(X)\n",
        "print(f\"There are {total_num_words} overall\")\n",
        "print(f\"We saw the word 'hello' {word_counts['hello']} times!\")\n",
        "print(f\"We saw the word 'and' {word_counts['and']} times!\")"
      ],
      "id": "uCmieuDQQQQ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMVF8rn3SkE1"
      },
      "source": [
        "Now lets calculate the probabilities:"
      ],
      "id": "dMVF8rn3SkE1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkejFgvDS3uk"
      },
      "source": [
        "def get_probabilities_from_counts(word_counts, total_num_words):\n",
        "  # fill word probabilities so it contains the probability of each word\n",
        "  # word_probabilities = {\n",
        "  #       'and': 0.75,\n",
        "  #       'founded': 0.01,\n",
        "  #       ....\n",
        "  #}\n",
        "  word_probabilities = {}\n",
        "\n",
        "  for w in word_counts:\n",
        "    # Get the count for the word\n",
        "    w_count = word_counts[w]\n",
        "\n",
        "    # Calc probability, and save to the dictionary\n",
        "    word_probabilities[w] = w_count / total_num_words\n",
        "  return word_probabilities\n",
        "\n",
        "word_counts, total_num_words = get_word_counts(X)\n",
        "word_probabilities = get_probabilities_from_counts(word_counts, total_num_words)\n",
        "\n",
        "print(f\"We saw the word 'hello' {word_probabilities['hello'] * 100}% of the time!\")\n",
        "print(f\"We saw the word 'and' {word_probabilities['and'] * 100}% of the time!\")"
      ],
      "id": "lkejFgvDS3uk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAhtrzwNTdzF"
      },
      "source": [
        "# Coding challenge 2 - Separate probabilities for each class\n",
        "\n",
        "We have just calculated the probabilities of words *overall*.\n",
        "\n",
        "What we actually need for Naive Bayes is the probabilities of words in each *source*.\n",
        "\n",
        "How can we update our code to instead calculate the probabilities per source?\n",
        "\n",
        "*Hint: What do we calculate if we do the same process on only training sentences from one source?*"
      ],
      "id": "qAhtrzwNTdzF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnOH1NM9VfiG"
      },
      "source": [
        "  def calulate_conditionals(X, Y):\n",
        "    # fill word_probs_by_source so it contains a dict for each source\n",
        "    # each containing each word and probability only in that source\n",
        "    # word_probs_by_source = {\n",
        "    #       'Twitter': {\n",
        "    #              'founded': 0.001,\n",
        "    #               ...,\n",
        "    #        },\n",
        "    #       'Wikipedia': {\n",
        "    #              'founded': 0.05,\n",
        "    #              ...,\n",
        "    #       }\n",
        "    #}\n",
        "    \n",
        "    # Get a list of the possible sources\n",
        "    # (Note: this is an ID not a name)\n",
        "    sources = set(Y)\n",
        "    word_probs_by_source = {}\n",
        "\n",
        "    for s in sources:\n",
        "      # Selects all the training examples which come from this source\n",
        "      source_training_data = [x for x,y in zip(X,Y) if y == s]\n",
        "\n",
        "      # Extract the counts and total for the specific source\n",
        "      source_counts, source_total = get_word_counts(source_training_data)\n",
        "\n",
        "      # Transform into probabilities\n",
        "      source_probabilities = get_probabilities_from_counts(source_counts, source_total)\n",
        "\n",
        "      #Save to the dictionary\n",
        "      word_probs_by_source[s] = source_probabilities\n",
        "    return word_probs_by_source\n",
        "\n",
        "word_probs_by_source = calulate_conditionals(X, Y)\n",
        "print(f\"The probability of seeing the word 'wall' in Trumps speechs was {word_probs_by_source[source_IDs['trumpSpeech']]['wall']}\")\n",
        "print(f\"The probability of seeing the word 'thou' in shakespeare was {word_probs_by_source[source_IDs['shakespeare']]['thou']}\")"
      ],
      "id": "DnOH1NM9VfiG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGv0jVLhYzZp"
      },
      "source": [
        "# This will raise an error, because we did not see the word wall!\n",
        "print(f\"The probability of seeing the word 'wall' in chess was {word_probs_by_source[source_IDs['chess']]['wall']}\")"
      ],
      "id": "DGv0jVLhYzZp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A6IajhgW30n"
      },
      "source": [
        "### Problem - what about words which don't appear in some sources?\n",
        "\n",
        "For example, what if we never see the word \"Twitter\" in any sentence collected from wikipedia?\n",
        "\n",
        "We will get a probability p(\"Twitter\"|Wikipedia) = 0!\n",
        "\n",
        "This may be true in our training data, but means that we cannot predict any sentences with Twitter in them...\n",
        "\n",
        "Solution: Add 1 to the count of *every* word! This avoids all 0s! But we also have to add to the denominator to balance this out."
      ],
      "id": "1A6IajhgW30n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OOzpIggY5PQ"
      },
      "source": [
        "  def calulate_conditionals(X, Y, unique_words):\n",
        "    # fill word_probs_by_source so it contains a dict for each source\n",
        "    # each containing each word and probability only in that source\n",
        "    # word_probs_by_source = {\n",
        "    #       'Twitter': {\n",
        "    #              'founded': 0.001,\n",
        "    #               ...,\n",
        "    #        },\n",
        "    #       'Wikipedia': {\n",
        "    #              'founded': 0.05,\n",
        "    #              ...,\n",
        "    #       }\n",
        "    #}\n",
        "    \n",
        "    # Get a list of the possible sources\n",
        "    # (Note: this is an ID not a name)\n",
        "    sources = set(Y)\n",
        "    word_probs_by_source = {}\n",
        "\n",
        "    for s in sources:\n",
        "      # Selects all the training examples which come from this source\n",
        "      source_training_data = [x for x,y in zip(X,Y) if y == s]\n",
        "      # Extract the counts and total for the specific source\n",
        "      source_counts, source_total = get_word_counts(source_training_data)\n",
        "\n",
        "      # Add 1 to the count of all words\n",
        "      for w in unique_words:\n",
        "        if w not in source_counts:\n",
        "          source_counts[w] = 0\n",
        "        source_counts[w] += 1\n",
        "      \n",
        "      # Add since we added +1 for each word to the top of the fraction,\n",
        "      # we need to add to the bottom as well. This is + len(unique_words)\n",
        "      source_total += len(unique_words)\n",
        "\n",
        "      # Transform into probabilities\n",
        "      source_probabilities = get_probabilities_from_counts(source_counts, source_total)\n",
        "\n",
        "      #Save to the dictionary\n",
        "      word_probs_by_source[s] = source_probabilities\n",
        "    return word_probs_by_source\n",
        "\n",
        "word_probs_by_source = calulate_conditionals(X, Y, [word for word in word_counts])\n",
        "print(f\"The probability of seeing the word 'wall' in Trumps speechs was {word_probs_by_source[source_IDs['trumpSpeech']]['wall']}\")\n",
        "print(f\"The probability of seeing the word 'thou' in shakespeare was {word_probs_by_source[source_IDs['shakespeare']]['thou']}\")\n",
        "print(f\"The probability of seeing the word 'wall' in chess was {word_probs_by_source[source_IDs['chess']]['wall']}\")"
      ],
      "id": "1OOzpIggY5PQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tdGO1HOY8Rf"
      },
      "source": [
        " ## This is everything we need to learn from the data to make predictions!\n"
      ],
      "id": "5tdGO1HOY8Rf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjxKHmd-ZGY2"
      },
      "source": [
        "NB_model = calulate_conditionals(X, Y, [word for word in word_counts])"
      ],
      "id": "PjxKHmd-ZGY2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d25ba24"
      },
      "source": [
        "Lets check the output but showing the most likely words for each source"
      ],
      "id": "2d25ba24"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f66049f"
      },
      "source": [
        "for gen, source in data_files:\n",
        "    word_probs = NB_model[source_IDs[source]].items()\n",
        "    print(source)\n",
        "    print(sorted(word_probs, key = lambda x: x[1])[-20:])"
      ],
      "id": "0f66049f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f1df847"
      },
      "source": [
        "Now we can make predictions for single words! Lets write a function which takes a word and returns the most likely source."
      ],
      "id": "4f1df847"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d42a7b2f"
      },
      "source": [
        "def get_word_probabilities(model, word):\n",
        "    \"\"\" Returns a dict giving\n",
        "    the ID of a source and the probability of word\n",
        "    coming from that source.\n",
        "    \"\"\"\n",
        "\n",
        "    # We want to fill source_probs\n",
        "    # with the likelihood of a word coming from each source.\n",
        "    # E.G [('Twitter', 0.9), ('Wikipedia', 0.5)]\n",
        "    source_probs = {}\n",
        "    for source in model:\n",
        "      # Get probability of word:\n",
        "      word_prob = model[source][word]\n",
        "      \n",
        "      # Add to list\n",
        "      source_probs[source] = word_prob\n",
        "    return source_probs\n",
        "\n",
        "def predict_single_word(model, word):\n",
        "    \"\"\" Return the source ID word is most likely\n",
        "    to have come from.\n",
        "    \"\"\"\n",
        "    source_probs = get_word_probabilities(model, word)\n",
        "\n",
        "    # Sort the list\n",
        "    # Max will be the last element\n",
        "    # take the source\n",
        "    most_likely_source = sorted(source_probs.items(), key=lambda x: x[1])[-1][0]\n",
        "    \n",
        "    return most_likely_source\n",
        "\n",
        "        "
      ],
      "id": "d42a7b2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AodI6WR2aaWT"
      },
      "source": [
        "# Lets Test it out! How good are our predictions?\n",
        "\n",
        "Can you beat the computer?"
      ],
      "id": "AodI6WR2aaWT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "222ed67e"
      },
      "source": [
        "# Run this cell to initialize scores\n",
        "word_pred_games_played = 0\n",
        "word_pred_human_correct = 0\n",
        "word_pred_ML_correct = 0"
      ],
      "id": "222ed67e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d853ea6"
      },
      "source": [
        "# Run this cell to play the game!\n",
        "s, label_opt1 = random.choice(data_files)\n",
        "s, label_opt2 = random.choice(data_files)\n",
        "source_gen, true_label = random.choice(data_files)\n",
        "label_options = [label_opt1, label_opt2, true_label]\n",
        "random.shuffle(label_options)\n",
        "lines = source_gen()\n",
        "line = random.choice(lines)\n",
        "word = random.choice(line.split())\n",
        "print(\"Word is\", word)\n",
        "predicted_label = input(\"Where did this word come from? {} or {} or {}\".format(*label_options))\n",
        "ML_predicted_label_ID = predict_single_word(NB_model, word)\n",
        "ML_predicted_label = ID_to_source[ML_predicted_label_ID]\n",
        "print(\"You were\", \"Right\" if true_label == predicted_label else \"Wrong\", \"!\")\n",
        "print(\"The computer was\", \"Right\" if true_label == ML_predicted_label else \"Wrong\", \"!\")\n",
        "print(\"Your predicted label (y^) was: {}. The computer predicted {}. The true label (y) was {}\".format(predicted_label, ML_predicted_label, true_label))\n",
        "word_pred_games_played += 1\n",
        "word_pred_human_correct += (true_label == predicted_label)\n",
        "word_pred_ML_correct += (true_label == ML_predicted_label)\n",
        "print(\"Your score:\", str(word_pred_human_correct / word_pred_games_played), \"AI score:\", str(word_pred_ML_correct / word_pred_games_played))"
      ],
      "id": "7d853ea6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afecc51b"
      },
      "source": [
        "## Predicting Sentences"
      ],
      "id": "afecc51b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7da12bb0"
      },
      "source": [
        "Now that we can classify the source of individual words, how can we upgrade to sentences?"
      ],
      "id": "7da12bb0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28978d36"
      },
      "source": [
        "Lets think about probability a little bit. What is the probability of a heads when a coin is flipped? \n",
        "$$p(H) = \\frac{1}{2}$$\n",
        "Now what is the probability of flipping two heads in a row?\n",
        "$$ p(H) \\times p(H) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$$\n",
        "\n",
        "We can check this is correct, as there are four possible options, HH, HT, TH, TT, and all are equally likely.\n",
        "\n",
        "Can we do something similar with a sentence? We have the probability of each word coming from each source, can we just multiply them all together to get the probability of the sentence coming from a source?\n",
        "\n"
      ],
      "id": "28978d36"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ae70aa6"
      },
      "source": [
        "### Yes! (Kind of)\n",
        "\n",
        "It turns out that yes, we can! If we make some assumptions...\n",
        "\n",
        "If we assume that all words are independant from each other, this works! This is actually exactly how Naive Bayes works!\n",
        "(Though this may not be so realistic...)"
      ],
      "id": "8ae70aa6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7cd6de4"
      },
      "source": [
        "So, lets put all of our intuition together!\n",
        "1. Firstly, we want to predict the probability of a label (wikipedia, twitter, etc,) given some sentence (\"Follow me please!!!\", \"Auckland was founded...\").\n",
        "    1. In statistics, this can be written p(y=Y|x=X)\n",
        "2. This is *hard* to calculate, so we use bayes rule to flip it around. We predict the probability of the sentence we have coming from each of the sources!\n",
        "    1. This is now: p(x=X|y=Y)\n",
        "3. The probability of the sentence coming from a source can be thought of as the product of each *word* coming from that source!\n",
        "    1. p(Follow me please|Twitter) = p(Follow|Twitter) x p(me|Twitter) x p(please|Twitter)\n",
        "4. Once probabilities are calculated, we can take the most likely label as our prediction!\n",
        "    1. If p(Follow me please|Twitter) = 0.2 and p(Follow me please|Wikipedia) = 0.001, we can predict the sentence \"Follow me please\" is from Twitter!"
      ],
      "id": "a7cd6de4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ae0eb0f"
      },
      "source": [
        "import math\n",
        "def get_sentence_probs(model, sentence):\n",
        "    \"\"\" Returns a list of tuples giving\n",
        "    the ID of a source and the probability of sentence\n",
        "    coming from that source.\n",
        "    \"\"\"\n",
        "    source_probs = {}\n",
        "\n",
        "    # Initialize the probabilities to be 1\n",
        "    for source in model:\n",
        "      source_probs[source] = 1\n",
        "\n",
        "    for word in sentence.split():\n",
        "        source_probabilities = get_word_probabilities(model, word)\n",
        "        for source, prob in source_probabilities.items():\n",
        "            source_probs[source] += math.log(prob)\n",
        "\n",
        "    return source_probs.items()\n",
        "\n",
        "def predict_sentence(model, sentence):\n",
        "    source_probs = get_sentence_probs(model, sentence)\n",
        "    return sorted(source_probs, key= lambda x: x[1])[-1][0]"
      ],
      "id": "0ae0eb0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t61a_kBrbmRK"
      },
      "source": [
        "## Game 2: Full Sentences!"
      ],
      "id": "t61a_kBrbmRK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92187b56"
      },
      "source": [
        "sent_pred_games_played = 0\n",
        "sent_pred_human_correct = 0\n",
        "sent_pred_ML_correct = 0"
      ],
      "id": "92187b56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b3c5e46"
      },
      "source": [
        "import random\n",
        "s, label_opt1 = random.choice(data_files)\n",
        "s, label_opt2 = random.choice(data_files)\n",
        "source_gen, true_label = random.choice(data_files)\n",
        "label_options = [label_opt1, label_opt2, true_label]\n",
        "random.shuffle(label_options)\n",
        "lines = source_gen()\n",
        "line = random.choice(lines)\n",
        "print(\"Line is:\", line)\n",
        "predicted_label = input(\"Where did this word come from? {} or {} or {}\".format(*label_options))\n",
        "ML_predicted_label_ID = predict_sentence(NB_model, line)\n",
        "ML_predicted_label = ID_to_source[ML_predicted_label_ID]\n",
        "print(\"You were\", \"Right\" if true_label == predicted_label else \"Wrong\", \"!\")\n",
        "print(\"The computer was\", \"Right\" if true_label == ML_predicted_label else \"Wrong\", \"!\")\n",
        "print(\"Your predicted label (y^) was: {}. The computer predicted {}. The true label (y) was {}\".format(predicted_label, ML_predicted_label, true_label))\n",
        "sent_pred_games_played += 1\n",
        "sent_pred_human_correct += (true_label == predicted_label)\n",
        "sent_pred_ML_correct += (true_label == ML_predicted_label)\n",
        "print(\"Your score:\", str(sent_pred_human_correct / sent_pred_games_played), \"AI score:\", str(sent_pred_ML_correct / sent_pred_games_played))"
      ],
      "id": "1b3c5e46",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34d070e1"
      },
      "source": [
        "## Evaluating\n",
        "\n",
        "So, after playing the ML algorithm, it seems pretty good!\n",
        "The next step is to test *how good*. Evaluation is an important part of machine learning, to test how good our models actually are. This is required for two things:\n",
        "1. Making sure we can actually put it into critical decision making roles\n",
        "2. Making sure we understand *what* its decisions are based on\n",
        "3. Making sure any changes we make actually are improving it!\n",
        "\n",
        "Lets look at the most basic form of evaluation, how well the predicted labels match the known labels. This is called *accuracy*!"
      ],
      "id": "34d070e1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8562c76"
      },
      "source": [
        "num_right = 0\n",
        "num_wrong = 0\n",
        "for x, y in observations:\n",
        "    ML_predicted_label_ID = predict_sentence(NB_model, x)\n",
        "    ML_correct = ML_predicted_label_ID == y\n",
        "    num_right += ML_correct\n",
        "    num_wrong += not ML_correct\n",
        "accuracy = num_right / (num_right + num_wrong)\n",
        "print(accuracy)"
      ],
      "id": "b8562c76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d33dfd83"
      },
      "source": [
        "Lets compare to a library implementation!\n",
        "Scikit-Learn is a very popular, simple to use ML library in python. Lets compare to their built in naive bayes model."
      ],
      "id": "d33dfd83"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "658ebd01"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# SKlearn doesn't work directly on words like our implementation, but needs them to be\n",
        "# vectorized first. This lets it be more general, and work on more than just text!\n",
        "cv = CountVectorizer()\n",
        "X_cv = cv.fit_transform(X)\n",
        "\n",
        "# We are going to shuffle the sentences, and then take 90% to train on\n",
        "# This will be closer to real training you'll see in examples\n",
        "X_cv, X_shuffle, Y_shuffle = shuffle(X_cv, X, Y) \n",
        "train_len = int(len(X) * 0.9)\n",
        "X_cv_train = X_cv[:train_len]\n",
        "X_cv_test = X_cv[train_len:]\n",
        "Y_train = Y_shuffle[:train_len]\n",
        "Y_test = Y_shuffle[train_len:]\n",
        "X_sentence_train = X_shuffle[:train_len]\n",
        "X_sentence_test = X_shuffle[train_len:]\n",
        "\n",
        "# Fit both models to the selected data\n",
        "sklearn_model = MultinomialNB().fit(X_cv_train, Y_train)\n",
        "our_model = calulate_conditionals(X_sentence_train, Y_train, [w for w in word_counts])"
      ],
      "id": "658ebd01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f0037aa"
      },
      "source": [
        "our_right = 0\n",
        "our_wrong = 0\n",
        "sk_right = 0\n",
        "sk_wrong = 0\n",
        "num_to_test = 1000\n",
        "wrong_examples = []\n",
        "\n",
        "# For each example, we predict with both models and increment right if they\n",
        "# get it right, otherwise wrong.\n",
        "for x_cv, x, y in zip(X_cv_test[:num_to_test], X_sentence_test[:num_to_test], Y_test[:num_to_test]):\n",
        "    if len(x.split()) < 1:\n",
        "        continue\n",
        "    ML_predicted_label_ID = predict_sentence(our_model, x)\n",
        "    sk_predicted_label_ID = sklearn_model.predict(x_cv)[0]\n",
        "    our_right += (ML_predicted_label_ID == y)\n",
        "    our_wrong += (ML_predicted_label_ID != y)\n",
        "    sk_right += (sk_predicted_label_ID == y)\n",
        "    sk_wrong += (sk_predicted_label_ID != y)\n",
        "    if ML_predicted_label_ID != y:\n",
        "        wrong_examples.append((x, ID_to_source[y], ID_to_source[ML_predicted_label_ID]))\n",
        "        \n",
        "our_accuracy = our_right / (our_right + our_wrong)\n",
        "sk_accuracy = sk_right / (sk_right + sk_wrong)\n",
        "print(\"Our accuracy:\", our_accuracy, \"SK Learn accuracy:\", sk_accuracy)"
      ],
      "id": "4f0037aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1781033b"
      },
      "source": [
        "# Show some examples of ones we got wrong\n",
        "# Sentence, True label, predicted label\n",
        "wrong_examples[:5]"
      ],
      "id": "1781033b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35a2bda4"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "We've done it! We have build a ML model from scratch to classify where a sentence came from, with over 90% accuracy!\n",
        "\n",
        "As we saw with the Scikit-Learn comparison, our model was pretty simple. There are many many more complex methods out there, its an exciting world!\n",
        "\n",
        "Next, we will look at how we can use very similar ideas to generate *new* text!"
      ],
      "id": "35a2bda4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88f4b104"
      },
      "source": [
        "# Text Generation\n",
        "\n",
        "So far, we have looked at *discriminative* machine learning. This means our model learns patterns which allow it to *discriminate* (or, tell the difference between) different classes. \n",
        "\n",
        "Now, we will look at *generative* machine learning! Our model will learn patterns which allow it to generate *new* data!\n",
        "\n",
        "### Basic Naive Bayes Model\n",
        "\n",
        "Can we *generate* text with our NB model? Lets try just building a sentence by taking the most likely word."
      ],
      "id": "88f4b104"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac5b9a6b"
      },
      "source": [
        "def NB_gen_sentence_1(model, source, num_words=10):\n",
        "    \"\"\" Only predict most likely word\n",
        "    \"\"\"\n",
        "    # Get a list of (word, likelihood) pairs for the source\n",
        "    word_likelihoods = model[source].items()\n",
        "\n",
        "    # sort the list, so the most likely words are at the start of the list (this is reverse of normal sorting)\n",
        "    # The key tells sorted to look at the second element for sorting, the likelihood\n",
        "    sorted_likelihoods = sorted(word_likelihoods, reverse=True, key=lambda x: x[1])\n",
        "\n",
        "    sentence_list = []\n",
        "    for i in range(num_words):\n",
        "      likely_word, likelihood = sorted_likelihoods[0]\n",
        "      sentence_list.append(likely_word)\n",
        "    return ' '.join(sentence_list)\n",
        "    \n",
        "text_source = \"shakespeare\"\n",
        "print(NB_gen_sentence_1(our_model, source_IDs[text_source]))"
      ],
      "id": "ac5b9a6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fed4d59c"
      },
      "source": [
        "We seem to have an issue, if we just pick the most likely word, we get the same thing every time!\n",
        "\n",
        "Lets try sampling different words based on the probability, rather than just taking the top one."
      ],
      "id": "fed4d59c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow0tj6mGIp6F"
      },
      "source": [
        "\n",
        "def NB_gen_sentence_2(model, source, num_words, num_top_word):\n",
        "    \"\"\"Predict a random likely word\n",
        "    \"\"\"\n",
        "    # Get a list of (word, likelihood) pairs for the source\n",
        "    word_likelihoods = model[source].items()\n",
        "\n",
        "    # sort the list, so the most likely words are at the start of the list (this is reverse of normal sorting)\n",
        "    # The key tells sorted to look at the second element for sorting, the likelihood\n",
        "    sorted_likelihoods = sorted(word_likelihoods, reverse=True, key=lambda x: x[1])\n",
        "\n",
        "    sentence_list = []\n",
        "    for i in range(num_words):\n",
        "      # Select the top words\n",
        "      top_words = sorted_likelihoods[:num_top_word]\n",
        "\n",
        "      selected_word, selected_likelihood = random.choice(top_words)\n",
        "      sentence_list.append(selected_word)\n",
        "    return ' '.join(sentence_list)\n",
        "    \n",
        "text_source = \"shakespeare\"\n",
        "print(NB_gen_sentence_2(our_model, source_IDs[text_source], 10, 1000))"
      ],
      "id": "ow0tj6mGIp6F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2223896"
      },
      "source": [
        "This seems to work! But has a major issue.\n",
        "\n",
        "Our Naive Bayes model only captures patterns to do with what words are in a sentence, but not their order. This is enough for a *discriminative* model, but not a *generative* model. We really want patterns which capture relationships between words, and the position of words in a sentence too. (These might also be helpful for disciminative models too!)\n",
        "\n",
        "Lets think about how we might capture these patterns in our data.\n",
        "\n",
        "### N-Grams\n",
        "\n",
        "Up until now, we have only looked at words individually. This is called a *bag-of-words* model. But really, a sentence is a *sequence* of words. The next word in a sentence depends on the rest of the words, not only on the source of the text. \n",
        "\n",
        "N-grams model how the next word in a sentence depends on *previous* words. The *N* refers to how far back we look. Lets start with bi-grams (bi = 2, tri = 3, etc). This means that we need to capture the probability of a word, given the source, given the previous word. Our probability formula for word $w_i$ looks like:\n",
        "$$ p(w_i | w_{i-1}, source)$$.\n",
        "\n",
        "For example, before our probability of the words 'and' and 'the' were pretty high in wikipedia. $p(and|Wikipedia) = high$, $p(the|Wikipedia) = high$. This means that in our generated sentences, the sequence 'the and' or 'and the' are pretty common! But we know that while 'and the' is actually pretty common in real sentences, 'the and' doesn't make much sence. By including the previous word in our probabilities, we can account for this! $$p(the | and, Wikipedia) = High, p(and | the, Wikipedia) = Low$$"
      ],
      "id": "a2223896"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b25ab1b2"
      },
      "source": [
        "So how do we learn this new bi-gram model? Well, we only need to extend our code a little. Rather than counting how many times 'the' appears in Wikipedia as we did to calculate $p(the|Wikipedia)$, we need to include the previous word too. So we need to count, for example, how many times 'the' appeared after 'and' in wikipedia for $p(the|and,Wikipedia)$. We need to count this for every possible word it could appear after, how many times it appeared after $w_{i-1}$, $p(the|w_{i-1},Wikipedia)$\n",
        "\n",
        "Lets update our NB model with bi-grams!\n",
        "\n",
        "Lets start with extracting the ngrams from a sentence:"
      ],
      "id": "b25ab1b2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZcp1j6ZTKPZ"
      },
      "source": [
        "def extract_ngrams(sentence, N):\n",
        "  \"\"\" Sentence is the input string\n",
        "  N is the number of words to consider at one.\n",
        "  So if N = 2, we consider the current word and 1 previous word.\n",
        "  \"\"\"\n",
        "  ngrams = []\n",
        "  sentence_words = sentence.split()\n",
        "  current_ngram = ['' for i in range(N)]\n",
        "  for word in sentence_words:\n",
        "    # Remove the word at the front of the ngram,\n",
        "    # and add the current word to the end.\n",
        "    current_ngram = (*current_ngram[1:], word)\n",
        "    previous_words = current_ngram[:-1]\n",
        "    ngrams.append((previous_words, word))\n",
        "  return ngrams\n",
        "\n",
        "bigrams = extract_ngrams(\"Hello, this is a sentence about bigrams\", 2)\n",
        "trigrams = extract_ngrams(\"Hello, this is a sentence about trigrams\", 3)\n",
        "print(bigrams[:10])\n",
        "print(trigrams[:10])"
      ],
      "id": "LZcp1j6ZTKPZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYyWMhT3spQa"
      },
      "source": [
        "Previously, we stored the count of each word for each source label. Now, we need to store the count of each word for each source label AND set of previous words.\n",
        "\n",
        "We can consider this as a new level in the probability dictionary. First, we look up the source label to get a map of (previous words) -> following word. Then we look up the set of previous words in our sentence, to find possible following words."
      ],
      "id": "GYyWMhT3spQa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQzbeBFET6wb"
      },
      "source": [
        "  def get_ngram_counts(sentences, N):\n",
        "    # fill ngram_counts so it contains the number of times each word occurs for each set of previous words.\n",
        "    # word_counts = {\n",
        "    #       ('and',): {\n",
        "    #           'the': 15,\n",
        "    #           'then': 7,\n",
        "    #        },\n",
        "    #       ('founded',): {\n",
        "    #           'Auckland', \n",
        "    #       },\n",
        "    #       ....\n",
        "    #}\n",
        "    ngram_counts = {}\n",
        "\n",
        "    # We now need to store a total for each set of previous words.\n",
        "    total_num_ngrams = {}\n",
        "\n",
        "    # We want to look though all the training sentences,\n",
        "    for sentence in sentences:\n",
        "      ngrams = extract_ngrams(sentence, N)\n",
        "      for previous_words, word in ngrams:\n",
        "\n",
        "        # Initialize the counts for the ngram if we haven't seen it yet\n",
        "        if previous_words not in ngram_counts:\n",
        "          ngram_counts[previous_words] = {}\n",
        "          total_num_ngrams[previous_words] = 0\n",
        "\n",
        "        # Increment the total\n",
        "        total_num_ngrams[previous_words] += 1\n",
        "\n",
        "        # Add word to dictionary if not already in it\n",
        "        if word not in ngram_counts[previous_words]:\n",
        "          ngram_counts[previous_words][word] = 0\n",
        "      \n",
        "        # Add to the count for the word\n",
        "        ngram_counts[previous_words][word] += 1\n",
        "    return ngram_counts, total_num_ngrams\n",
        "  \n",
        "  # Inspect our ngram counts!\n",
        "  test_sentences = [\"Hello, this is a sentence about bigrams\", \"Hello, this is a sentence about trigrams\"]\n",
        "  test_counts, test_totals = get_ngram_counts(test_sentences, 2)\n",
        "  print(test_counts)\n",
        "  for previous_word in test_counts:\n",
        "    print(f\"If the previous word is {previous_word}, the possible next words were: {test_counts[previous_word]}\")"
      ],
      "id": "WQzbeBFET6wb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg1uDv2bv3Zz"
      },
      "source": [
        "def get_probabilities_from_ngrams(ngram_counts, total_num_ngrams):\n",
        "  ngram_probabilities = {}\n",
        "\n",
        "  for previous_words in ngram_counts:\n",
        "    ngram_probabilities[previous_words] = {}\n",
        "\n",
        "    # Get the total of times this set of previous words was seen\n",
        "    ngram_total = total_num_ngrams[previous_words]\n",
        "\n",
        "    for following_word in ngram_counts[previous_words]:\n",
        "\n",
        "      # Get the number of times the word appeard after the set of previous words\n",
        "      w_count = ngram_counts[previous_words][following_word]\n",
        "\n",
        "      # Calc probability, and save to the dictionary\n",
        "      ngram_probabilities[previous_words][following_word] = w_count / ngram_total\n",
        "  return ngram_probabilities\n",
        "\n",
        "# Inspect our bigrams!\n",
        "bigram_counts, total_bigram_words = get_ngram_counts(X, 2)\n",
        "word_probabilities = get_probabilities_from_ngrams(bigram_counts, total_bigram_words)\n",
        "for previous_word in list(word_probabilities.keys())[:5]:\n",
        "  print(f\"If the previous word is {previous_word}, the possible next words were: {word_probabilities[previous_word]}\")"
      ],
      "id": "Eg1uDv2bv3Zz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGNFDNYgTTDw"
      },
      "source": [
        "  def calulate_conditionals_Ngram(X, Y, N):\n",
        "    # fill word_probs_by_source so it contains a dict for each source\n",
        "    # each containing each word and probability only in that source\n",
        "    # word_probs_by_source = {\n",
        "    #       'Twitter': {\n",
        "    #              'founded': 0.001,\n",
        "    #               ...,\n",
        "    #        },\n",
        "    #       'Wikipedia': {\n",
        "    #              'founded': 0.05,\n",
        "    #              ...,\n",
        "    #       }\n",
        "    #}\n",
        "    \n",
        "    # Get a list of the possible sources\n",
        "    # (Note: this is an ID not a name)\n",
        "    sources = set(Y)\n",
        "    ngram_probs_by_source = {}\n",
        "\n",
        "    for s in sources:\n",
        "      # Selects all the training examples which come from this source\n",
        "      source_training_data = [x for x,y in zip(X, Y) if y == s]\n",
        "\n",
        "      # Extract the counts and total for the specific source\n",
        "      source_counts, source_total = get_ngram_counts(source_training_data, N)\n",
        "\n",
        "      # Transform into probabilities\n",
        "      source_probabilities = get_probabilities_from_ngrams(source_counts, source_total)\n",
        "\n",
        "      #Save to the dictionary\n",
        "      word_probs_by_source[s] = source_probabilities\n",
        "    return word_probs_by_source\n",
        "\n",
        "bigram_model = calulate_conditionals_Ngram(X, Y, 2)"
      ],
      "id": "OGNFDNYgTTDw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2CaA0XR0X6N"
      },
      "source": [
        ""
      ],
      "id": "k2CaA0XR0X6N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20858d8d"
      },
      "source": [
        "Lets test our bigrams! The following cell lets you pick a word and prints the 5 most likely words to follow it."
      ],
      "id": "20858d8d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9e7e09c"
      },
      "source": [
        "\n",
        "source_label = \"shakespeare\"\n",
        "bigram = bigram_model[source_IDs[source_label]]\n",
        "first_word = input(\"Select first word\")\n",
        "sorted_bigrams = sorted(bigram[(first_word,)].items(), key=lambda x: x[1])\n",
        "print(\"Most likely next words:\")\n",
        "print(sorted_bigrams[-5:])"
      ],
      "id": "a9e7e09c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f72f6cf8"
      },
      "source": [
        "Now lets use bigrams to generate our text! Instead of sampling from all words from a source, we take into account the previously generated word!"
      ],
      "id": "f72f6cf8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OsYkGQDx_xG"
      },
      "source": [
        "def NB_gen_sentence_3(model, source, num_words, num_top_word, N):\n",
        "    \"\"\"Predict a random likely word\n",
        "    \"\"\"\n",
        "    # Select the ngrams for this source\n",
        "    ngrams_for_source = model[source]\n",
        "\n",
        "    # Setup the Ngram for the current sentence we are generating\n",
        "    # Remember for an Ngram of N words, we have N-1 previous words\n",
        "    # and the word we are generating is the remaining one\n",
        "    current_ngram = [''] * N\n",
        "\n",
        "    sentence_list = []\n",
        "    for i in range(num_words):\n",
        "      # The previous words are the previous ngram, without the first word (now too old)\n",
        "      previous_words = current_ngram[1:]\n",
        "\n",
        "      # Get a list of (word, likelihood) pairs for the current set of previous words\n",
        "      word_likelihoods = ngrams_for_source[tuple(previous_words)].items()\n",
        "\n",
        "      # sort the list, so the most likely words are at the start of the list (this is reverse of normal sorting)\n",
        "      # The key tells sorted to look at the second element for sorting, the likelihood\n",
        "      sorted_likelihoods = sorted(word_likelihoods, reverse=True, key=lambda x: x[1])\n",
        "\n",
        "    \n",
        "      # Select the top words\n",
        "      top_words = sorted_likelihoods[:num_top_word]\n",
        "\n",
        "      selected_word, selected_likelihood = random.choice(top_words)\n",
        "      sentence_list.append(selected_word)\n",
        "\n",
        "      # Update the current ngram\n",
        "      current_ngram = [*previous_words, selected_word]\n",
        "    return ' '.join(sentence_list)\n",
        "    \n",
        "text_source = \"shakespeare\"\n",
        "print(NB_gen_sentence_3(bigram_model, source_IDs[text_source], 10, 1000, 2))"
      ],
      "id": "1OsYkGQDx_xG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlDf4-T50ia6"
      },
      "source": [
        "We run into an issue again, where we may not have seen a particular ngram before. How can we predict the next word?\n"
      ],
      "id": "VlDf4-T50ia6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGrg-mdf0tRu"
      },
      "source": [
        "def NB_gen_sentence_3(model, source, num_words, num_top_word, N):\n",
        "    \"\"\"Predict a random likely word\n",
        "    \"\"\"\n",
        "    # Select the ngrams for this source\n",
        "    ngrams_for_source = model[source]\n",
        "\n",
        "    # Setup the Ngram for the current sentence we are generating\n",
        "    # Remember for an Ngram of N words, we have N-1 previous words\n",
        "    # and the word we are generating is the remaining one\n",
        "    current_ngram = [''] * N\n",
        "\n",
        "    sentence_list = []\n",
        "    for i in range(num_words):\n",
        "      # The previous words are the previous ngram, without the first word (now too old)\n",
        "      previous_words = current_ngram[1:]\n",
        "\n",
        "      # Check previous_words was seen in the data\n",
        "      # If not, just restart the ngram, like a new sentence.\n",
        "      if tuple(previous_words) not in ngrams_for_source:\n",
        "        current_ngram = [''] * N\n",
        "        previous_words = current_ngram[1:]\n",
        "      # Get a list of (word, likelihood) pairs for the current set of previous words\n",
        "      word_likelihoods = ngrams_for_source[tuple(previous_words)].items()\n",
        "\n",
        "      # sort the list, so the most likely words are at the start of the list (this is reverse of normal sorting)\n",
        "      # The key tells sorted to look at the second element for sorting, the likelihood\n",
        "      sorted_likelihoods = sorted(word_likelihoods, reverse=True, key=lambda x: x[1])\n",
        "\n",
        "    \n",
        "      # Select the top words\n",
        "      top_words = sorted_likelihoods[:num_top_word]\n",
        "\n",
        "      selected_word, selected_likelihood = random.choice(top_words)\n",
        "      sentence_list.append(selected_word)\n",
        "\n",
        "      # Update the current ngram\n",
        "      current_ngram = [*previous_words, selected_word]\n",
        "    return ' '.join(sentence_list)\n",
        "    \n",
        "text_source = \"shakespeare\"\n",
        "print(NB_gen_sentence_3(bigram_model, source_IDs[text_source], 10, 10, 2))"
      ],
      "id": "WGrg-mdf0tRu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac9e8b95"
      },
      "source": [
        "We get more coherent sentences! But we can see that the patterns are not very long, focus tends to switch quickly. This is because our patterns only have a \"memory\" of one word, we can only make decisions one word back. We can increase this by using longer N-grams, lets try a tri-gram!"
      ],
      "id": "ac9e8b95"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-axOXc9G1Y2i"
      },
      "source": [
        "trigram_model = calulate_conditionals_Ngram(X, Y, 3)"
      ],
      "id": "-axOXc9G1Y2i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_l_NtGS1eTf",
        "outputId": "0cdfe7b2-0609-40eb-d23d-99ccaa8d2821"
      },
      "source": [
        "text_source = \"trumpSpeech\"\n",
        "print(NB_gen_sentence_3(trigram_model, source_IDs[text_source], 50, 10, 3))"
      ],
      "id": "f_l_NtGS1eTf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "But the women of suburbia don’t like me. I think they want anything. I don’t think that’s appropriate. When those votes are corrupt, when they’re irregular, when they get to eat very well. So what happened is they made this into a massive event and people are leaving. But if\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b05fdc12"
      },
      "source": [
        "We now see much more coherent sentences, even more so than the bigram model! Can we increase the N-gram length even further?\n",
        "\n",
        "Give it a try! However, we start to run into an issue. The more we increase the length, the less data we have for each N-gram. For example, if we looked for patterns 10 words long, it is likely we only would get the exact sentences that appear in the training data. In other words, our patterns are *too* detailed, so have fit too closely to the training data. \n",
        "\n",
        "This is very very important problem in machine learning! We are interested in patterns which *generalize* to new data. If we look too deeply into our data, we can always find more and more complex patterns, but are they actually useful?"
      ],
      "id": "b05fdc12"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fdd23bd"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this talk, we have focused on simple models for one small area machine learning can be applied to. \n",
        "\n",
        "I encorage you to take what you have learned, and think up some more areas where classification and generation can be applied!"
      ],
      "id": "3fdd23bd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4651f81"
      },
      "source": [
        ""
      ],
      "id": "f4651f81",
      "execution_count": null,
      "outputs": []
    }
  ]
}