{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Devs-ML-workshop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c747c885"
      },
      "source": [
        "# Machine Learning for text classification"
      ],
      "id": "c747c885"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab873446"
      },
      "source": [
        "Lets set the stage! After a year full of COVID disruptions the university has made all exams online. Celebrations all around! But wait, what about the cheating you say? Fortunately (or unfortunately), the uni has predicted this, and hired a crack team of Designated Exam Validators (or DEVs for short) to check for plagarism. And you've just been hired! And of course, you're going to use ML to do all your work for you!"
      ],
      "id": "ab873446"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96bd1fa9"
      },
      "source": [
        "# Problem Statement"
      ],
      "id": "96bd1fa9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "407bc8ef"
      },
      "source": [
        "*Given* a sentence, we need to find the probability a sentence came from each possible source."
      ],
      "id": "407bc8ef"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "796e54a3"
      },
      "source": [
        "For example, given the text \"The University of Auckland began as a constituent college of the University of New Zealand, founded on 23 May 1883 as Auckland University College\" the probability for \"Wikipedia\" = 0.9 and \"Twitter\" = 0.1"
      ],
      "id": "796e54a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dba17488"
      },
      "source": [
        "We need to build a machine learning classifier to do this for us!"
      ],
      "id": "dba17488"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b07db5d4"
      },
      "source": [
        "# 1) Collect data"
      ],
      "id": "b07db5d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5d6b023"
      },
      "source": [
        "First, we need some data from each source. This will allow us to find patterns, i.e., learn what makes *wikipedia* sentences different to *twitter* sentences\n",
        "\n",
        "This is normally the *most important part of ML*, but we've taken care of it! In the github repo there is a data folder containing some datasets from different sources.\n",
        "\n",
        "First we will just load this in."
      ],
      "id": "a5d6b023"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a87ce678",
        "outputId": "29a25264-a8ae-4e9e-9f04-cdf10a059581"
      },
      "source": [
        "# Load in some libraries\n",
        "# The pathlib library makes handling filepaths easier, letting us open data files.\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Set the folder containing our data\n",
        "data_path = pathlib.Path('data')\n",
        "\n",
        "# Setup availiable filenames\n",
        "chess_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/chess.txt\"\n",
        "music_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/music.txt\"\n",
        "happy_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/happy_topical_chat.txt\"\n",
        "trumpspeech_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/trumpSpeech.txt\"\n",
        "wallstreetbets_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/wallstreetbets_comments.txt\"\n",
        "javascript_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/javascript.txt\"\n",
        "shakespeare_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/shakespeare.txt\"\n",
        "wiki_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/wiki.txt\"\n",
        "twitter_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/twitter2.txt\"\n",
        "\n",
        "# Just a helper function, don't worry about this one!\n",
        "def get_file_or_cache(path):\n",
        "    cache = None\n",
        "    def get():\n",
        "        nonlocal cache\n",
        "        if cache is None:\n",
        "            # with open(path, 'r', encoding='utf8') as f:\n",
        "            #     cache = f.readlines()\n",
        "            print(path)\n",
        "            df = pd.read_csv(path, delimiter = \"\\n\")\n",
        "            cache = list([str(x[0]) for x in df.values])\n",
        "        return cache\n",
        "    return (get, path.split('/')[-1].split('.')[0])\n",
        "\n",
        "# Construct a list of possible files and their names\n",
        "data_files = [get_file_or_cache(x) for x in [chess_filename, music_filename, happy_filename, trumpspeech_filename, javascript_filename, shakespeare_filename, wiki_filename]]\n",
        "\n",
        "# Construct dataset of sentences and labels from every source\n",
        "# Note! Rather than using the actual names for the datasets, we give each one\n",
        "# its own numeric ID.\n",
        "# We record these so we can swap between human readable name and ID.\n",
        "X = []\n",
        "Y = []\n",
        "source_IDs = {}\n",
        "ID_to_source = {}\n",
        "source_ID = 0\n",
        "for source_constructor, source in data_files:\n",
        "    source_IDs[source] = source_ID\n",
        "    ID_to_source[source_ID] = source\n",
        "    lines = source_constructor()\n",
        "    for line in lines:\n",
        "        X.append(line)\n",
        "        Y.append(source_ID)\n",
        "    source_ID += 1\n",
        "\n",
        "observations = list(zip(X, Y))"
      ],
      "id": "a87ce678",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/chess.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/music.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/happy_topical_chat.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/trumpSpeech.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/javascript.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/shakespeare.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aeb8f97"
      },
      "source": [
        "Now we have our data loaded into X, a list of sentences, and Y, a list of the sources each sentence came from.\n",
        "Lets check out some sample lines from each source!"
      ],
      "id": "0aeb8f97"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce1dc4fc",
        "outputId": "10734988-9fec-4729-8675-fb57484a75d3"
      },
      "source": [
        "inspect_index = int(random.random() * len(observations))\n",
        "for index, (line, source) in enumerate(observations[inspect_index:inspect_index+5]):\n",
        "    print(\"Observation\", index)\n",
        "    print(\"X{} = \".format(index), line)\n",
        "    # Source is stored as a number, representing the source (explained later!)\n",
        "    print(\"Y{} =\".format(index), source, \"Source name:\", ID_to_source[source])\n",
        "    print('-----------')"
      ],
      "id": "ce1dc4fc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation 0\n",
            "X0 =   lol No we wouldn't.  If Earths history were viewed in a 24 hour period, humans would only represent 1 minute and 17 seconds.  \n",
            "Y0 = 2 Source name: happy_topical_chat\n",
            "-----------\n",
            "Observation 1\n",
            "X1 =   That is neat!  I wonder who it is?  Do you know when the first drive thru was added by McDonalds?\n",
            "Y1 = 2 Source name: happy_topical_chat\n",
            "-----------\n",
            "Observation 2\n",
            "X2 =   I always thought like that too!  Just kind of always took them for granted.  It's been fun chatting with you!\n",
            "Y2 = 2 Source name: happy_topical_chat\n",
            "-----------\n",
            "Observation 3\n",
            "X3 =   Me too!  Wouldn't catch me there.  Thanks!\n",
            "Y3 = 2 Source name: happy_topical_chat\n",
            "-----------\n",
            "Observation 4\n",
            "X4 =   hi do you watch tv?\n",
            "Y4 = 2 Source name: happy_topical_chat\n",
            "-----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18b3725b"
      },
      "source": [
        "# How do we *actually* predict?\n",
        "\n",
        "The goal is to learn a model which can tell the difference between classes. For example, consider sentences from Wikipedia (W) or Twitter (T). If we show our model a new input sentence (X) where we do NOT know the origin, our model should be able to tell where the sentence came from. \n",
        "\n",
        "Formally, the $i^{th}$ input is called $x_{i}$ and its true class is called $y_{i}$.\n",
        "For us, $x_{i}$ is the $i^{th}$ sentence we need to label and $y_{i}$ is where this sentence actually came from.\n",
        "Our model predicts some label, $\\hat y_{i}$, for this sentence (so *wikipedia* or *twitter*).\n",
        "We want to train it so that *our* label, $\\hat y_{n}$, is close to the *true* label, $y_{i}$.\n",
        "\n",
        "There are many, many different ways to do this prediction, and we will look at a simple one called Naive Bayes.\n",
        "\n",
        "Lets true a manual version to get the idea:\n"
      ],
      "id": "18b3725b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95203343",
        "outputId": "ae482417-5ccf-406e-8303-06bddd3033ea"
      },
      "source": [
        "# Pick two random labels to choose from\n",
        "g, label_opt1 = random.choice(data_files)\n",
        "g, label_opt2 = random.choice(data_files)\n",
        "\n",
        "# Select a random observations from a random source\n",
        "source_gen, true_label = random.choice(data_files)\n",
        "\n",
        "# Shuffle labels\n",
        "label_options = [label_opt1, label_opt2, true_label]\n",
        "random.shuffle(label_options)\n",
        "\n",
        "lines = source_gen()\n",
        "line = random.choice(lines)\n",
        "print(line)\n",
        "predicted_label = input(\"Where did this sentence come from? {} or {} or {}? \\n\".format(*label_options))\n",
        "\n",
        "#****** YOUR CODE HERE! ***********\n",
        "# First challenge to get you started:\n",
        "# How can we tell if the predicted label (what the user typed for now)\n",
        "# matches the true label?\n",
        "prediction_iscorrect = ...\n",
        "#***********************************\n",
        "\n",
        "# Now we use whether or not the prediction was correct to inform the user.\n",
        "print(\"You were\", \"Right\" if prediction_iscorrect else \"Wrong\", \"!\")\n",
        "print(\"Your predicted label (y^) was: {}. The true label (y) was {}\".format(predicted_label, true_label))"
      ],
      "id": "95203343",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " I am not a soccer fan, I live in the US.  I think it's great than the american team has not placed below 3rd place since 1991.  I think we should be more soccer fans rather than US Football fans!\n",
            "Where did this sentence come from? happy_topical_chat or chess or shakespeare? \n",
            "happy_tropical_chat\n",
            "You were Right !\n",
            "Your predicted label (y^) was: happy_tropical_chat. The true label (y) was happy_topical_chat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e71eed05"
      },
      "source": [
        "## Using a model to make a prediction\n",
        " \n",
        " How can we *train the computer* to predict a class for us? \n",
        " Lets think of a simple example: We are given a sentence ($x$), and need to predict the most likely class ($\\hat y$), Wikipedia (W) or Twitter (T). We start by asking a friend what class they think is most likely and they say Wikipedia has a 70% chance and Twitter and 30%. \n",
        "\n",
        " In mathmatical terms we can rewrite the \"_probability that the class is Wikipedia given this specific sentence $x$ , $\\hat y = W$, is 0.7_\" as $p(\\hat y = W | x) = 0.7$. Similarly, we can write $p(\\hat y = T | x) = 0.3$ for the sentence being from twitter.\n",
        "\n",
        " Now it is pretty easy to make a prediction, Wikipedia has a 70% chance and Twitter only has a 30% chance so we should predict Wikipedia! \n",
        "\n",
        " ### But how did our friend come up with $p(\\hat y| x)$ in the first place?\n",
        " This is basically what the computer needs to do, find the probability of a sentence coming from each source.\n",
        " This is what Naive Bayes solves!\n",
        "\n",
        " ## Naive Bayes - Probability theory (spooky)\n",
        "\n",
        " Note: Naive Bayes is pretty simple, and is quite intuitive when you wrap your head around it, but if this is your first introduction to probability it can be quite confusing! If you don't understand at first don't get discoraged! I find that drawing diagrams and thinking about it from a few directions helps really understand.\n",
        "\n",
        " The end goal of a model is to calculate $p(y = C|X=x)$. In plain english, this can be read as calculate the _probability that the true class of the input is C given what we know about the sentence_. For a concrete example, lets use the sentence \"The University of Auckland was founded on 23 May 1883\". We want to predict the probability that $y = Wikipedia$ or $y = Twitter$ given that the sentence $x$ = \"The University of Auckland was founded on 23 May 1883\". This can be difficult to calculate, and isn't really how we think about things as humans.\n",
        "\n",
        " Instead of calculating this directly, _Bayes Theorum_ gives us a way to swap things around.\n",
        "\n",
        " $$ p(y=C|X=x) = p(X=x|y=C)p(y=C)$$\n",
        "\n",
        " This is like thinking \"what is the probability that the sentence was found on twitter (or wikipedia)\". This is a bit more natural to think about, and it turns out easier to calculate.\n",
        " \n",
        " ## Intuition\n",
        " Instead of directly trying to work out the probability of wikipedia or twitter, lets look at indivdual words. If we see the words \"follow me!\" we can say twitter has a pretty high probability. On the other hand, if we see the words \"Auckland[1][2]\" we could say wikipedia has a high probability. How did we do this?\n",
        " \n",
        " We looked at the probability of each word (or few words) coming from each source! We know that 'following' is something people on twitter do, so if we see this word we know it is more likely to have come from twitter than wikipedia. This is the $p(x=X|y=C)$ in the formula!\n",
        " \n",
        "Lets see if we can use this to do some machine learning!"
      ],
      "id": "e71eed05"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8624bc10"
      },
      "source": [
        "# Exercise 1: Calculate word probabilities"
      ],
      "id": "8624bc10"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8923fd5b"
      },
      "source": [
        "We need to *learn* 2 things from our data.\n",
        "1. What is the probability of seeing each word overall?\n",
        "    1. This is $p(x=X)$, or the *probability* that a word is X\n",
        "    2. e.g p('and') is high, it occurs a lot! But p('founded') is low, it doesn't occur too often.\n",
        "2. What is the probability of seeing each word *from each source*?\n",
        "    1. This is $p(x=X|y=Y)$, or the *probability* that a word is X if we *know* it is from e.g wikipedia\n",
        "    2. e.g. p('founded'|Twitter) might be pretty low, its an uncommon word on twitter. But p('founded'|Wikipedia) is high, it occurs all the time on wikipedia!"
      ],
      "id": "8923fd5b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL8q3vvUQSJa"
      },
      "source": [
        "## Coding Challenge 1 - Calculate the probability of words\n",
        "\n",
        "Lets start simple - How can we calculate the probability of a word occuring?\n",
        "\n",
        "One way to think about this: We count combine all our training data into a huge list of words. Lets say we have $N$ words overall (including all repetitions). Then I tell you a word, e.g., \"hello\". If you closed your eyes and pointed to a random word in the list, what are the chances you would end up pointing to the word \"hello\"?\n",
        "\n",
        "We can calulate this as: The number of times a word occurs in the training data, divided by the total number of words.\n",
        "\n",
        "### First lets count each word"
      ],
      "id": "wL8q3vvUQSJa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCmieuDQQQQ7"
      },
      "source": [
        "  def get_word_counts(training_sentences):\n",
        "    # fill word_counts so it contains the number of times each word occurs.\n",
        "    # word_counts = {\n",
        "    #       'and': 700,\n",
        "    #       'founded': 15,\n",
        "    #       ....\n",
        "    #}\n",
        "    word_counts = {}\n",
        "    total_num_words = 0\n",
        "\n",
        "    # We want to look though all the training sentences,\n",
        "    for sentence in training_sentences:\n",
        "\n",
        "      # We can use .split to split each sentence into words\n",
        "      words = sentence.split()\n",
        "\n",
        "      for word in words:\n",
        "        #******** YOUR CODE HERE************\n",
        "        #Add code here to update word_counts and total_num_words\n",
        "        #We want to end up with the number of times we see each word\n",
        "        #**********************************\n",
        "    return word_counts, total_num_words\n",
        "\n",
        "word_counts, total_num_words = get_word_counts(X)\n",
        "print(f\"There are {total_num_words} overall\")\n",
        "print(f\"We saw the word 'hello' {word_counts['hello']} times!\")\n",
        "print(f\"We saw the word 'and' {word_counts['and']} times!\")"
      ],
      "id": "uCmieuDQQQQ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMVF8rn3SkE1"
      },
      "source": [
        "Now lets calculate the probabilities:"
      ],
      "id": "dMVF8rn3SkE1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "lkejFgvDS3uk",
        "outputId": "461fef62-522e-4cd8-9f42-a83bc8bb2f30"
      },
      "source": [
        "def get_probabilities_from_counts(word_counts, total_num_words):\n",
        "  # fill word probabilities so it contains the probability of each word\n",
        "  # word_probabilities = {\n",
        "  #       'and': 0.75,\n",
        "  #       'founded': 0.01,\n",
        "  #       ....\n",
        "  #}\n",
        "  word_probabilities = {}\n",
        "\n",
        "  for w in word_counts:\n",
        "    #******* YOUR CODE HERE********\n",
        "    #Add code here to update word_probabilities\n",
        "    #To contain the probability of each word.\n",
        "    #Remember, this should be the count of each word / total\n",
        "  return word_probabilities\n",
        "\n",
        "word_counts, total_num_words = get_word_counts(X)\n",
        "word_probabilities = get_probabilities_from_counts(word_counts, total_num_words)\n",
        "\n",
        "print(f\"We saw the word 'hello' {word_probabilities['hello'] * 100}% of the time!\")\n",
        "print(f\"We saw the word 'and' {word_probabilities['and'] * 100}% of the time!\")"
      ],
      "id": "lkejFgvDS3uk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-013e1a29fb71>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    #******* YOUR CODE HERE********\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAhtrzwNTdzF"
      },
      "source": [
        "# Coding challenge 2 - Separate probabilities for each class\n",
        "\n",
        "We have just calculated the probabilities of words *overall*.\n",
        "\n",
        "What we actually need for Naive Bayes is the probabilities of words in each *source*.\n",
        "\n",
        "How can we update our code to instead calculate the probabilities per source?\n",
        "\n",
        "*Hint: What do we calculate if we do the same process on only training sentences from one source?*"
      ],
      "id": "qAhtrzwNTdzF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnOH1NM9VfiG"
      },
      "source": [
        "  def calulate_conditionals(X, Y):\n",
        "    # fill word_probs_by_source so it contains a dict for each source\n",
        "    # each containing each word and probability only in that source\n",
        "    # word_probs_by_source = {\n",
        "    #       'Twitter': {\n",
        "    #              'founded': 0.001,\n",
        "    #               ...,\n",
        "    #        },\n",
        "    #       'Wikipedia': {\n",
        "    #              'founded': 0.05,\n",
        "    #              ...,\n",
        "    #       }\n",
        "    #}\n",
        "    \n",
        "    # Get a list of the possible sources\n",
        "    # (Note: this is an ID not a name)\n",
        "    sources = set(Y)\n",
        "    word_probs_by_source = {}\n",
        "\n",
        "    for s in sources:\n",
        "      # Selects all the training examples which come from this source\n",
        "      source_training_data = [x for x,y in zip(X,Y) if y == s]\n",
        "      #*********** YOUR CODE HERE****************\n",
        "      #Add code which calculates the word probabilities for just one source\n",
        "      #And adds it to word_probs_by_source\n",
        "      #******************************************\n",
        "    return word_probs_by_source\n",
        "\n",
        "word_probs_by_source = calulate_conditionals(X, Y)\n",
        "print(f\"The probability of seeing the word 'wall' in Trumps speechs was {word_probs_by_source[source_IDs['trumpSpeech']]['wall']}\")\n",
        "print(f\"The probability of seeing the word 'thou' in shakespeare was {word_probs_by_source[source_IDs['shakespeare']]['thou']}\")"
      ],
      "id": "DnOH1NM9VfiG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGv0jVLhYzZp"
      },
      "source": [
        "# This will raise an error, because we did not see the word wall!\n",
        "print(f\"The probability of seeing the word 'wall' in chess was {word_probs_by_source[source_IDs['chess']]['wall']}\")"
      ],
      "id": "DGv0jVLhYzZp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A6IajhgW30n"
      },
      "source": [
        "### Problem - what about words which don't appear in some sources?\n",
        "\n",
        "For example, what if we never see the word \"Twitter\" in any sentence collected from wikipedia?\n",
        "\n",
        "We will get a probability p(\"Twitter\"|Wikipedia) = 0!\n",
        "\n",
        "This may be true in our training data, but means that we cannot predict any sentences with Twitter in them...\n",
        "\n",
        "Solution: Add 1 to the count of *every* word! This avoids all 0s! But we also have to add to the denominator to balance this out."
      ],
      "id": "1A6IajhgW30n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OOzpIggY5PQ"
      },
      "source": [
        "  def calulate_conditionals(X, Y, unique_words):\n",
        "    # fill word_probs_by_source so it contains a dict for each source\n",
        "    # each containing each word and probability only in that source\n",
        "    # word_probs_by_source = {\n",
        "    #       'Twitter': {\n",
        "    #              'founded': 0.001,\n",
        "    #               ...,\n",
        "    #        },\n",
        "    #       'Wikipedia': {\n",
        "    #              'founded': 0.05,\n",
        "    #              ...,\n",
        "    #       }\n",
        "    #}\n",
        "    \n",
        "    # Get a list of the possible sources\n",
        "    # (Note: this is an ID not a name)\n",
        "    sources = set(Y)\n",
        "    word_probs_by_source = {}\n",
        "\n",
        "    for s in sources:\n",
        "      # Selects all the training examples which come from this source\n",
        "      source_training_data = [x for x,y in zip(X,Y) if y == s]\n",
        "      #*********** YOUR CODE HERE****************\n",
        "      #Add code which calculates the word probabilities for just one source\n",
        "      #And adds it to word_probs_by_source.\n",
        "      #Make sure to add 1 to the count for every word\n",
        "      #And adjust the total number of words!\n",
        "      #******************************************\n",
        "    return word_probs_by_source\n",
        "\n",
        "word_probs_by_source = calulate_conditionals(X, Y, [word for word in word_counts])\n",
        "print(f\"The probability of seeing the word 'wall' in Trumps speechs was {word_probs_by_source[source_IDs['trumpSpeech']]['wall']}\")\n",
        "print(f\"The probability of seeing the word 'thou' in shakespeare was {word_probs_by_source[source_IDs['shakespeare']]['thou']}\")\n",
        "print(f\"The probability of seeing the word 'wall' in chess was {word_probs_by_source[source_IDs['chess']]['wall']}\")"
      ],
      "id": "1OOzpIggY5PQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tdGO1HOY8Rf"
      },
      "source": [
        " ## This is everything we need to learn from the data to make predictions!\n"
      ],
      "id": "5tdGO1HOY8Rf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjxKHmd-ZGY2"
      },
      "source": [
        "NB_model = calulate_conditionals(X, Y, [word for word in word_counts])"
      ],
      "id": "PjxKHmd-ZGY2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d25ba24"
      },
      "source": [
        "Lets check the output but showing the most likely words for each source"
      ],
      "id": "2d25ba24"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f66049f",
        "outputId": "e280bd71-e3ae-4c7f-d2cd-cb32aac8d092"
      },
      "source": [
        "for gen, source in data_files:\n",
        "    word_probs = NB_model[source_IDs[source]].items()\n",
        "    print(source)\n",
        "    print(sorted(word_probs, key = lambda x: x[1])[-20:])"
      ],
      "id": "0f66049f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chess\n",
            "[('16.', 0.0016448923594571498), ('d4', 0.001648460455681569), ('15.', 0.0016627328405792448), ('14.', 0.0016859254660379682), ('13.', 0.001700197850935644), ('12.', 0.00171447023583332), ('11.', 0.001718038332057739), ('10.', 0.0017216064282821579), ('9.', 0.0017305266688432055), ('7.', 0.0017447990537408813), ('8.', 0.0017447990537408813), ('6.', 0.0017483671499653002), ('5.', 0.0017590714386385572), ('4.', 0.0017644235829751857), ('3.', 0.0017662076310873951), ('2.', 0.0017715597754240236), ('Nf6', 0.0017751278716484427), ('1.', 0.00178404811220949), ('Nf3', 0.0018464897961368223), ('O-O', 0.0025654611853572467)]\n",
            "music\n",
            "[('z', 0.001087623754584232), ('G', 0.0010907717249303511), ('F2', 0.0011128075173531865), ('E2', 0.0012040986573906474), ('f2', 0.0012953897974281082), ('A', 0.0013552012340043757), ('d', 0.001506303810618104), ('c2', 0.0017266617348464578), ('e2', 0.0018557285190373507), ('g2', 0.0019249838666519762), ('D2', 0.002230336990225552), (':|', 0.0026647568979900208), ('B2', 0.002756048038027482), ('<li><a', 0.0032424094565029196), ('\\\\', 0.003269167204444934), ('||', 0.0038452457777847734), ('A2', 0.00433632915177939), ('G2', 0.004799080792658933), ('d2', 0.004821116585081769), ('|', 0.05647616199455401)]\n",
            "happy_topical_chat\n",
            "[('are', 0.0031255874477810665), ('would', 0.0031401975480355748), ('on', 0.0033194147778242054), ('they', 0.003815184179793842), ('with', 0.003936935015248074), ('like', 0.004409328256810498), ('for', 0.004594389526700932), ('have', 0.005052172668008848), ('it', 0.00571839323961441), ('was', 0.0058469621218540805), ('you', 0.0065901292214667175), ('in', 0.007271933900010422), ('and', 0.0076001741523950336), ('is', 0.007859259930241641), ('that', 0.00874658001903209), ('of', 0.009587147787008114), ('to', 0.012952340878963112), ('a', 0.014612048267875214), ('the', 0.019592144441295156), ('I', 0.02180508762651129)]\n",
            "trumpSpeech\n",
            "[('was', 0.003436028071000991), ('going', 0.0035165670486989337), ('our', 0.003734655741116958), ('We', 0.004065860975245576), ('for', 0.004473080525403713), ('is', 0.004559954029437449), ('they', 0.004587101999447991), ('it', 0.004688454420820683), ('And', 0.005036853369289312), ('we', 0.005945405432308799), ('have', 0.0059680287406509176), ('that', 0.00635533977946799), ('in', 0.006989697345380999), ('you', 0.008020415273447929), ('I', 0.011064607643963423), ('of', 0.011084516155304487), ('a', 0.012449154114501088), ('and', 0.014300645669220085), ('to', 0.015808262937138877), ('the', 0.021587160820049682)]\n",
            "wallstreetbets_comments\n",
            "[('at', 0.005094461562714739), ('my', 0.005126186963346042), ('have', 0.005379990168396464), ('are', 0.005389311548994321), ('be', 0.005512206593192383), ('[removed]', 0.006520714560683333), ('this', 0.007401912093166709), ('that', 0.007692755521119142), ('on', 0.008479758049666278), ('it', 0.008661443204828017), ('for', 0.00900110123117449), ('in', 0.01029333894142477), ('you', 0.011021960191490593), ('of', 0.011622698641073535), ('is', 0.012648786405306056), ('and', 0.01600260279111571), ('I', 0.017022639834609273), ('a', 0.019218560863872847), ('to', 0.02194784474962608), ('the', 0.028766679956447893)]\n",
            "javascript\n",
            "[('&&', 2.0136795967270995e-05), ('||', 2.2374217741412214e-05), (\"'\", 2.2374217741412214e-05), ('break;', 2.4611639515553437e-05), ('case', 3.1323904837977106e-05), ('new', 3.3561326612118325e-05), ('function()', 3.3561326612118325e-05), ('},', 4.027359193454199e-05), (':', 5.369812257938932e-05), ('true,', 6.0410387901812985e-05), ('function', 6.488523145009543e-05), ('+', 6.712265322423665e-05), ('if', 7.383491854666031e-05), ('};', 7.383491854666031e-05), ('});', 7.607234032080153e-05), ('return', 8.27846056432252e-05), ('var', 0.0001141085104812023), ('}', 0.00015885694596402673), ('=', 0.0002975770959607825), ('{', 0.00030652678305734737)]\n",
            "shakespeare\n",
            "[('have', 9.724573555348294e-05), ('he', 0.00010387612661394769), ('it', 0.00010608625696743594), ('me', 0.00011050651767441244), ('his', 0.00011934703908836543), ('And', 0.00012597743014883017), ('with', 0.00012818756050231843), ('your', 0.00013039769085580668), ('is', 0.00013481795156278318), ('in', 0.0001502888640372009), ('not', 0.0001502888640372009), ('that', 0.00016575977651161865), ('you', 0.00018344081933952466), ('my', 0.00021880290499533663), ('a', 0.0002541649906511486), ('of', 0.00030499798878137833), ('to', 0.0003116283798418431), ('and', 0.00036467150832556107), ('I', 0.0003889829422139318), ('the', 0.0004928590688278795)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f1df847"
      },
      "source": [
        "Now we can make predictions for single words! Lets write a function which takes a word and returns the most likely source."
      ],
      "id": "4f1df847"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d42a7b2f"
      },
      "source": [
        "def get_word_probabilities(model, word):\n",
        "    \"\"\" Returns a dict giving\n",
        "    the ID of a source and the probability of word\n",
        "    coming from that source.\n",
        "    \"\"\"\n",
        "\n",
        "    # We want to fill source_probs\n",
        "    # with the likelihood of a word coming from each source.\n",
        "    # E.G [('Twitter', 0.9), ('Wikipedia', 0.5)]\n",
        "    source_probs = {}\n",
        "    for source in model:\n",
        "      #************* YOUR CODE HERE ******************\n",
        "      #Add code which retrieves the probability of 'word'\n",
        "      # from source 'source'. (Remember 'model' here is storing our probabilities)\n",
        "      #Then add this to the list to return!\n",
        "    return source_probs\n",
        "\n",
        "def predict_single_word(model, word):\n",
        "    \"\"\" Return the source ID word is most likely\n",
        "    to have come from.\n",
        "    \"\"\"\n",
        "    source_probs = get_word_probabilities(model, word)\n",
        "\n",
        "    #************* YOUR CODE HERE **************\n",
        "    #Given a list of tuples as above, find the source which\n",
        "    #is most likely\n",
        "    most_likely_source = ...\n",
        "    #**********************************\n",
        "    \n",
        "    return most_likely_source\n",
        "\n",
        "        "
      ],
      "id": "d42a7b2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AodI6WR2aaWT"
      },
      "source": [
        "# Lets Test it out! How good are our predictions?\n",
        "\n",
        "Can you beat the computer?"
      ],
      "id": "AodI6WR2aaWT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "222ed67e"
      },
      "source": [
        "# Run this cell to initialize scores\n",
        "word_pred_games_played = 0\n",
        "word_pred_human_correct = 0\n",
        "word_pred_ML_correct = 0"
      ],
      "id": "222ed67e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d853ea6",
        "outputId": "e68fafe3-9b8e-4fc5-e716-7c1c02ead648"
      },
      "source": [
        "# Run this cell to play the game!\n",
        "s, label_opt1 = random.choice(data_files)\n",
        "s, label_opt2 = random.choice(data_files)\n",
        "source_gen, true_label = random.choice(data_files)\n",
        "label_options = [label_opt1, label_opt2, true_label]\n",
        "random.shuffle(label_options)\n",
        "lines = source_gen()\n",
        "line = random.choice(lines)\n",
        "word = random.choice(line.split())\n",
        "print(\"Word is\", word)\n",
        "predicted_label = input(\"Where did this word come from? {} or {} or {}\".format(*label_options))\n",
        "ML_predicted_label_ID = predict_single_word(NB_model, word)\n",
        "ML_predicted_label = ID_to_source[ML_predicted_label_ID]\n",
        "print(\"You were\", \"Right\" if true_label == predicted_label else \"Wrong\", \"!\")\n",
        "print(\"The computer was\", \"Right\" if true_label == ML_predicted_label else \"Wrong\", \"!\")\n",
        "print(\"Your predicted label (y^) was: {}. The computer predicted {}. The true label (y) was {}\".format(predicted_label, ML_predicted_label, true_label))\n",
        "word_pred_games_played += 1\n",
        "word_pred_human_correct += (true_label == predicted_label)\n",
        "word_pred_ML_correct += (true_label == ML_predicted_label)\n",
        "print(\"Your score:\", str(word_pred_human_correct / word_pred_games_played), \"AI score:\", str(word_pred_ML_correct / word_pred_games_played))"
      ],
      "id": "7d853ea6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word is h4\n",
            "Where did this word come from? happy_topical_chat or shakespeare or chesschess\n",
            "You were Right !\n",
            "The computer was Right !\n",
            "Your predicted label (y^) was: chess. The computer predicted chess. The true label (y) was chess\n",
            "Your score: 0.8 AI score: 0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afecc51b"
      },
      "source": [
        "## Predicting Sentences"
      ],
      "id": "afecc51b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7da12bb0"
      },
      "source": [
        "Now that we can classify the source of individual words, how can we upgrade to sentences?"
      ],
      "id": "7da12bb0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28978d36"
      },
      "source": [
        "Lets think about probability a little bit. What is the probability of a heads when a coin is flipped? \n",
        "$$p(H) = \\frac{1}{2}$$\n",
        "Now what is the probability of flipping two heads in a row?\n",
        "$$ p(H) \\times p(H) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$$\n",
        "\n",
        "We can check this is correct, as there are four possible options, HH, HT, TH, TT, and all are equally likely.\n",
        "\n",
        "Can we do something similar with a sentence? We have the probability of each word coming from each source, can we just multiply them all together to get the probability of the sentence coming from a source?\n",
        "\n"
      ],
      "id": "28978d36"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ae70aa6"
      },
      "source": [
        "### Yes! (Kind of)\n",
        "\n",
        "It turns out that yes, we can! If we make some assumptions...\n",
        "\n",
        "If we assume that all words are independant from each other, this works! This is actually exactly how Naive Bayes works!\n",
        "(Though this may not be so realistic...)"
      ],
      "id": "8ae70aa6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7cd6de4"
      },
      "source": [
        "So, lets put all of our intuition together!\n",
        "1. Firstly, we want to predict the probability of a label (wikipedia, twitter, etc,) given some sentence (\"Follow me please!!!\", \"Auckland was founded...\").\n",
        "    1. In statistics, this can be written p(y=Y|x=X)\n",
        "2. This is *hard* to calculate, so we use bayes rule to flip it around. We predict the probability of the sentence we have coming from each of the sources!\n",
        "    1. This is now: p(x=X|y=Y)\n",
        "3. The probability of the sentence coming from a source can be thought of as the product of each *word* coming from that source!\n",
        "    1. p(Follow me please|Twitter) = p(Follow|Twitter) x p(me|Twitter) x p(please|Twitter)\n",
        "4. Once probabilities are calculated, we can take the most likely label as our prediction!\n",
        "    1. If p(Follow me please|Twitter) = 0.2 and p(Follow me please|Wikipedia) = 0.001, we can predict the sentence \"Follow me please\" is from Twitter!"
      ],
      "id": "a7cd6de4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ae0eb0f"
      },
      "source": [
        "import math\n",
        "def get_sentence_probs(model, sentence):\n",
        "    \"\"\" Returns a list of tuples giving\n",
        "    the ID of a source and the probability of sentence\n",
        "    coming from that source.\n",
        "    \"\"\"\n",
        "    source_probs = {}\n",
        "\n",
        "    # Initialize the probabilities to be 1\n",
        "    for source in model:\n",
        "      sentence_prob = 1\n",
        "      for word in sentence.split():\n",
        "        #*********** YOUR CODE HERE ***************\n",
        "        # Add code which incorporates word probabilities\n",
        "        # into the probability of each source.\n",
        "        # 1) get the likelihood of the word given each source\n",
        "        # 2) Multiply with the current probability estimate to update\n",
        "        # 3) Store final result in dict\n",
        "    return source_probs\n",
        "\n",
        "def predict_sentence(model, sentence):\n",
        "    source_probs = get_sentence_probs(model, sentence)\n",
        "\n",
        "    # Gets dict as a list, then sorts accoring to probabilities\n",
        "    # Selects the last element (the max) and returns the source\n",
        "    return sorted(source_probs.items(), key= lambda x: x[1])[-1][0]"
      ],
      "id": "0ae0eb0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t61a_kBrbmRK"
      },
      "source": [
        "## Game 2: Full Sentences!"
      ],
      "id": "t61a_kBrbmRK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92187b56"
      },
      "source": [
        "sent_pred_games_played = 0\n",
        "sent_pred_human_correct = 0\n",
        "sent_pred_ML_correct = 0"
      ],
      "id": "92187b56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b3c5e46",
        "outputId": "14ac8789-2b0c-461a-967b-f00aa334b1b2"
      },
      "source": [
        "import random\n",
        "s, label_opt1 = random.choice(data_files)\n",
        "s, label_opt2 = random.choice(data_files)\n",
        "source_gen, true_label = random.choice(data_files)\n",
        "label_options = [label_opt1, label_opt2, true_label]\n",
        "random.shuffle(label_options)\n",
        "lines = source_gen()\n",
        "line = random.choice(lines)\n",
        "print(\"Line is:\", line)\n",
        "predicted_label = input(\"Where did this word come from? {} or {} or {}\".format(*label_options))\n",
        "ML_predicted_label_ID = predict_sentence(NB_model, line)\n",
        "ML_predicted_label = ID_to_source[ML_predicted_label_ID]\n",
        "print(\"You were\", \"Right\" if true_label == predicted_label else \"Wrong\", \"!\")\n",
        "print(\"The computer was\", \"Right\" if true_label == ML_predicted_label else \"Wrong\", \"!\")\n",
        "print(\"Your predicted label (y^) was: {}. The computer predicted {}. The true label (y) was {}\".format(predicted_label, ML_predicted_label, true_label))\n",
        "sent_pred_games_played += 1\n",
        "sent_pred_human_correct += (true_label == predicted_label)\n",
        "sent_pred_ML_correct += (true_label == ML_predicted_label)\n",
        "print(\"Your score:\", str(sent_pred_human_correct / sent_pred_games_played), \"AI score:\", str(sent_pred_ML_correct / sent_pred_games_played))"
      ],
      "id": "1b3c5e46",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Line is: 1. e4 d6 2. d4 Nf6 3. Nc3 g6 4. Nf3 Bg7 5. Be2 O-O 6. O-O Bg4 7. h3 Bxf3 8. Bxf3 e5 9. d5 c5 10. Be3 Na6 11. Qd2 Qd7 12. Ne2 h5 13. Kh1 Qb5 14. Rab1 Qa4 15. a3 Nxe4 16. Bxe4 Qxe4 17. f3 Qa4 18. c3 f5 19. Ng3 f4 20. Ne4 fxe3 21. Qxe3 Nc7 22. Qg5 Rad8 23. Qxg6 Qe8 24. Qg3 Nxd5 25. Rbe1 Qb5 26. Rf2 Nf4 27. Qh4 Nd3 28. Ref1 d5 29. Ng3 Nxf2+ 30. Rxf2 e4 31. Nxh5 e3 32. Rc2 Qf1+ 33. Kh2 e2 34. Qg3 Rf7 {White resigns} 0-1\n",
            "Where did this word come from? music or chess or shakespearechess\n",
            "You were Right !\n",
            "The computer was Right !\n",
            "Your predicted label (y^) was: chess. The computer predicted chess. The true label (y) was chess\n",
            "Your score: 1.0 AI score: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34d070e1"
      },
      "source": [
        "## Evaluating\n",
        "\n",
        "So, after playing the ML algorithm, it seems pretty good!\n",
        "The next step is to test *how good*. Evaluation is an important part of machine learning, to test how good our models actually are. This is required for two things:\n",
        "1. Making sure we can actually put it into critical decision making roles\n",
        "2. Making sure we understand *what* its decisions are based on\n",
        "3. Making sure any changes we make actually are improving it!\n",
        "\n",
        "Lets look at the most basic form of evaluation, how well the predicted labels match the known labels. This is called *accuracy*!"
      ],
      "id": "34d070e1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8562c76",
        "outputId": "73dcf008-02eb-45ec-e305-99b2b4618a13"
      },
      "source": [
        "num_right = 0\n",
        "num_wrong = 0\n",
        "for x, y in observations:\n",
        "    ML_predicted_label_ID = predict_sentence(NB_model, x)\n",
        "    ML_correct = ML_predicted_label_ID == y\n",
        "    #********** YOUR CODE HERE **************\n",
        "    # Add code to update num_right and num_wrong\n",
        "# Add code to calculate accuracy.\n",
        "# Accuracy is the number of correct predictions divided by the total number of predictions\n",
        "print(accuracy)"
      ],
      "id": "b8562c76",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8994011331037898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d33dfd83"
      },
      "source": [
        "Lets compare to a library implementation!\n",
        "Scikit-Learn is a very popular, simple to use ML library in python. Lets compare to their built in naive bayes model."
      ],
      "id": "d33dfd83"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "658ebd01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "2e4b6298-1152-4f50-8401-2076daaacb9b"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# SKlearn doesn't work directly on words like our implementation, but needs them to be\n",
        "# vectorized first. This lets it be more general, and work on more than just text!\n",
        "cv = CountVectorizer()\n",
        "X_cv = cv.fit_transform(X)\n",
        "\n",
        "# We are going to shuffle the sentences, and then take 90% to train on\n",
        "# This will be closer to real training you'll see in examples\n",
        "X_cv, X_shuffle, Y_shuffle = shuffle(X_cv, X, Y) \n",
        "train_len = int(len(X) * 0.9)\n",
        "X_cv_train = X_cv[:train_len]\n",
        "X_cv_test = X_cv[train_len:]\n",
        "Y_train = Y_shuffle[:train_len]\n",
        "Y_test = Y_shuffle[train_len:]\n",
        "X_sentence_train = X_shuffle[:train_len]\n",
        "X_sentence_test = X_shuffle[train_len:]\n",
        "\n",
        "# Fit both models to the selected data\n",
        "sklearn_model = MultinomialNB().fit(X_cv_train, Y_train)\n",
        "our_model = calulate_conditionals(X_sentence_train, Y_train, [w for w in word_counts])"
      ],
      "id": "658ebd01",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a56241ac8dc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# vectorized first. This lets it be more general, and work on more than just text!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# We are going to shuffle the sentences, and then take 90% to train on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f0037aa",
        "outputId": "d10501d7-09ac-4987-b388-d08f52fbc282"
      },
      "source": [
        "our_right = 0\n",
        "our_wrong = 0\n",
        "sk_right = 0\n",
        "sk_wrong = 0\n",
        "num_to_test = 1000\n",
        "wrong_examples = []\n",
        "\n",
        "# For each example, we predict with both models and increment right if they\n",
        "# get it right, otherwise wrong.\n",
        "for x_cv, x, y in zip(X_cv_train[:num_to_test], X_sentence_train[:num_to_test], Y_train[:num_to_test]):\n",
        "    if len(x.split()) < 1:\n",
        "        continue\n",
        "    ML_predicted_label_ID = predict_sentence(our_model, x)\n",
        "    sk_predicted_label_ID = sklearn_model.predict(x_cv)[0]\n",
        "    our_right += (ML_predicted_label_ID == y)\n",
        "    our_wrong += (ML_predicted_label_ID != y)\n",
        "    sk_right += (sk_predicted_label_ID == y)\n",
        "    sk_wrong += (sk_predicted_label_ID != y)\n",
        "    if ML_predicted_label_ID != y:\n",
        "        wrong_examples.append((x, ID_to_source[y], ID_to_source[ML_predicted_label_ID]))\n",
        "our_accuracy = our_right / (our_right + our_wrong)\n",
        "sk_accuracy = sk_right / (sk_right + sk_wrong)\n",
        "print(\"Our accuracy:\", our_accuracy, \"SK Learn accuracy:\", sk_accuracy)"
      ],
      "id": "4f0037aa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our accuracy: 0.908 SK Learn accuracy: 0.979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1781033b",
        "outputId": "7fd2acb9-e334-4799-fea4-822548345bfa"
      },
      "source": [
        "# Show some examples of ones we got wrong\n",
        "# Sentence, True label, predicted label\n",
        "wrong_examples[:5]"
      ],
      "id": "1781033b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"72,800 a year lol. Maybe he meant 1/2 a month's...?\",\n",
              "  'wallstreetbets_comments',\n",
              "  'happy_topical_chat'),\n",
              " ('$TSLA Go2DaMoon', 'wallstreetbets_comments', 'javascript'),\n",
              " ('&gt;missed most of the party ', 'wallstreetbets_comments', 'trumpSpeech'),\n",
              " ('$53,566.04. ', 'wallstreetbets_comments', 'javascript'),\n",
              " ('DEALER WINS', 'wallstreetbets_comments', 'javascript')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35a2bda4"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "We've done it! We have build a ML model from scratch to classify where a sentence came from, with over 90% accuracy!\n",
        "\n",
        "As we saw with the Scikit-Learn comparison, our model was pretty simple. There are many many more complex methods out there, its an exciting world!\n",
        "\n",
        "Next, we will look at how we can use very similar ideas to generate *new* text!"
      ],
      "id": "35a2bda4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88f4b104"
      },
      "source": [
        "# Text Generation\n",
        "\n",
        "So far, we have looked at *discriminative* machine learning. This means our model learns patterns which allow it to *discriminate* (or, tell the difference between) different classes. \n",
        "\n",
        "Now, we will look at *generative* machine learning! Our model will learn patterns which allow it to generate *new* data!\n",
        "\n",
        "### Basic Naive Bayes Model\n",
        "\n",
        "Can we *generate* text with our NB model? Lets try just building a sentence by taking the most likely word."
      ],
      "id": "88f4b104"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac5b9a6b",
        "outputId": "5ab23dfa-a89f-4cc9-d172-21cefb1e5c19"
      },
      "source": [
        "def NB_gen_sentence_1(model, source, num_words):\n",
        "    \"\"\" Only predict most likely word\n",
        "    \"\"\"\n",
        "    # Get a list of (word, likelihood) pairs for the source\n",
        "    word_likelihoods = model[source].items()\n",
        "\n",
        "    # sort the list, so the most likely words are at the start of the list (this is reverse of normal sorting)\n",
        "    # The key tells sorted to look at the second element for sorting, the likelihood\n",
        "    sorted_likelihoods = sorted(word_likelihoods, reverse=True, key=lambda x: x[1])\n",
        "\n",
        "    sentence_list = []\n",
        "    for i in range(num_words):\n",
        "      #************** YOUR CODE HERE ****************\n",
        "      # Add code to select the next word in the sentence. You should select the most likely word given the class.\n",
        "      # Add this word to sentence_list\n",
        "    return ' '.join(sentence_list)\n",
        "    \n",
        "text_source = \"shakespeare\"\n",
        "print(NB_gen_sentence_1(our_model, source_IDs[text_source], 10))"
      ],
      "id": "ac5b9a6b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the the the the the the the the the the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fed4d59c"
      },
      "source": [
        "We seem to have an issue, if we just pick the most likely word, we get the same thing every time!\n",
        "\n",
        "Lets try sampling different words based on the probability, rather than just taking the top one."
      ],
      "id": "fed4d59c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d4f7294",
        "outputId": "7be26f30-5fee-468d-f238-b9513e10a41e"
      },
      "source": [
        "def NB_gen_sentence_2(model, source, num_words, num_top_word):\n",
        "    \"\"\"Predict a random likely word\n",
        "    \"\"\"\n",
        "    # Get a list of (word, likelihood) pairs for the source\n",
        "    word_likelihoods = model[source].items()\n",
        "\n",
        "    # sort the list, so the most likely words are at the start of the list (this is reverse of normal sorting)\n",
        "    # The key tells sorted to look at the second element for sorting, the likelihood\n",
        "    sorted_likelihoods = sorted(word_likelihoods, reverse=True, key=lambda x: x[1])\n",
        "\n",
        "    sentence_list = []\n",
        "    for i in range(num_words):\n",
        "      #************** YOUR CODE HERE ****************\n",
        "      # Add code to select the next word in the sentence.\n",
        "      # We should sample from the \n",
        "      # Add this word to sentence_list\n",
        "    return ' '.join(sentence_list)\n",
        "    \n",
        "text_source = \"shakespeare\"\n",
        "print(NB_gen_sentence_2(our_model, source_IDs[text_source], 10, 1000))"
      ],
      "id": "1d4f7294",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And that bad best been the they China a will\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2223896"
      },
      "source": [
        "This seems to work! But has a major issue.\n",
        "\n",
        "Our Naive Bayes model only captures patterns to do with what words are in a sentence, but not their order. This is enough for a *discriminative* model, but not a *generative* model. We really want patterns which capture relationships between words, and the position of words in a sentence too. (These might also be helpful for disciminative models too!)\n",
        "\n",
        "Lets think about how we might capture these patterns in our data.\n",
        "\n",
        "### N-Grams - From Naive Bayes to Markov Chains\n",
        "\n",
        "Up until now, we have only looked at words individually. This is called a *bag-of-words* model. But really, a sentence is a *sequence* of words. The next word in a sentence depends on the rest of the words, not only on the source of the text. \n",
        "\n",
        "N-grams model how the next word in a sentence depends on *previous* words. The *N* refers to how far back we look. Lets start with bi-grams (bi = 2, tri = 3, etc). This means that we need to capture the probability of a word, given the source, given the previous word. Our probability formula for word $w_i$ looks like:\n",
        "$$ p(w_i | w_{i-1}, source)$$.\n",
        "\n",
        "For example, before our probability of the words 'and' and 'the' were pretty high in wikipedia. $p(and|Wikipedia) = high$, $p(the|Wikipedia) = high$. This means that in our generated sentences, the sequence 'the and' or 'and the' are pretty common! But we know that while 'and the' is actually pretty common in real sentences, 'the and' doesn't make much sence. By including the previous word in our probabilities, we can account for this! $$p(the | and, Wikipedia) = High, p(and | the, Wikipedia) = Low$$"
      ],
      "id": "a2223896"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b25ab1b2"
      },
      "source": [
        "So how do we learn this new bi-gram model? Well, we only need to extend our code a little. Rather than counting how many times 'the' appears in Wikipedia as we did to calculate $p(the|Wikipedia)$, we need to include the previous word too. So we need to count, for example, how many times 'the' appeared after 'and' in wikipedia for $p(the|and,Wikipedia)$. We need to count this for every possible word it could appear after, how many times it appeared after $w_{i-1}$, $p(the|w_{i-1},Wikipedia)$\n",
        "\n",
        "Lets update our NB model with bi-grams! This model for generating text is called a *markov chain*. The *markov* in the name refers to the *markov* property, which essentially means future actions are based on the current state. Using bigrams, the next word is based on the current word. \n",
        "\n",
        "Lets start with extracting the ngrams from a sentence:"
      ],
      "id": "b25ab1b2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "IEiw9XIdQ0-9",
        "outputId": "c1117457-ac71-42fd-e8bc-7c97f3b16ae2"
      },
      "source": [
        "def extract_ngrams(sentence, N):\n",
        "  \"\"\" Sentence is the input string\n",
        "  N is the number of words to consider at one.\n",
        "  So if N = 2, we consider the current word and 1 previous word.\n",
        "  \"\"\"\n",
        "  ngrams = []\n",
        "  sentence_words = sentence.split()\n",
        "  current_ngram = ['' for i in range(N)]\n",
        "  for word in sentence_words:\n",
        "    # Remove the word at the front of the ngram,\n",
        "    # and add the current word to the end.\n",
        "    current_ngram = (*current_ngram[1:], word)\n",
        "    previous_words = current_ngram[:-1]\n",
        "    ngrams.append((previous_words, word))\n",
        "  return ngrams\n",
        "\n",
        "bigrams = extract_ngrams(\"Hello, this is a sentence about bigrams\", 2)\n",
        "trigrams = extract_ngrams(\"Hello, this is a sentence about trigrams\", 3)\n",
        "print(bigrams[:10])\n",
        "print(trigrams[:10])"
      ],
      "id": "IEiw9XIdQ0-9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5f5b012fd35f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mbigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7NtNw7p2hdY"
      },
      "source": [
        "Previously, we stored the count of each word for each source label. Now, we need to store the count of each word for each source label AND set of previous words.\n",
        "\n",
        "We can consider this as a new level in the probability dictionary. First, we look up the source label to get a map of (previous words) -> following word. Then we look up the set of previous words in our sentence, to find possible following words."
      ],
      "id": "Y7NtNw7p2hdY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvxxLvkR2kce"
      },
      "source": [
        "  def get_ngram_counts(sentences, N):\n",
        "    # fill ngram_counts so it contains the number of times each word occurs for each set of previous words.\n",
        "    # word_counts = {\n",
        "    #       ('and',): {\n",
        "    #           'the': 15,\n",
        "    #           'then': 7,\n",
        "    #        },\n",
        "    #       ('founded',): {\n",
        "    #           'Auckland', \n",
        "    #       },\n",
        "    #       ....\n",
        "    #}\n",
        "    ngram_counts = {}\n",
        "\n",
        "    # We now need to store a total for each set of previous words.\n",
        "    total_num_ngrams = {}\n",
        "\n",
        "    # We want to look though all the training sentences,\n",
        "    for sentence in sentences:\n",
        "      ngrams = extract_ngrams(sentence, N)\n",
        "      for previous_words, word in ngrams:\n",
        "        \n",
        "        #********* YOUR CODE HERE ***************\n",
        "        # Initialize the counts for the set of previous words if we haven't seen it yet\n",
        "\n",
        "        # Increment the total for the set of previous words\n",
        "\n",
        "        # Add word to ngram_counts if not already in it\n",
        "      \n",
        "        # Add to the count for the word\n",
        "        #******************************\n",
        "    return ngram_counts, total_num_ngrams\n",
        "  \n",
        "  # Inspect our ngram counts!\n",
        "  test_sentences = [\"Hello, this is a sentence about bigrams\", \"Hello, this is a sentence about trigrams\"]\n",
        "  test_counts, test_totals = get_ngram_counts(test_sentences, 2)\n",
        "  print(test_counts)\n",
        "  for previous_word in test_counts:\n",
        "    print(f\"If the previous word is {previous_word}, the possible next words were: {test_counts[previous_word]}\")"
      ],
      "id": "tvxxLvkR2kce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKWiiq0728Vp"
      },
      "source": [
        "Now, as before, we need to turn our counts into probabilities"
      ],
      "id": "LKWiiq0728Vp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p93-5CM23AzJ"
      },
      "source": [
        "def get_probabilities_from_ngrams(ngram_counts, total_num_ngrams):\n",
        "  ngram_probabilities = {}\n",
        "\n",
        "  for previous_words in ngram_counts:\n",
        "    ngram_probabilities[previous_words] = {}\n",
        "\n",
        "    # Get the total of times this set of previous words was seen\n",
        "    ngram_total = total_num_ngrams[previous_words]\n",
        "\n",
        "    for following_word in ngram_counts[previous_words]:\n",
        "\n",
        "      # Get the number of times the word appeard after the set of previous words\n",
        "      w_count = ngram_counts[previous_words][following_word]\n",
        "\n",
        "      # Calc probability, and save to the dictionary\n",
        "      ngram_probabilities[previous_words][following_word] = w_count / ngram_total\n",
        "  return ngram_probabilities\n",
        "\n",
        "# Inspect our bigrams!\n",
        "bigram_counts, total_bigram_words = get_ngram_counts(X, 2)\n",
        "word_probabilities = get_probabilities_from_ngrams(bigram_counts, total_bigram_words)\n",
        "for previous_word in list(word_probabilities.keys())[:5]:\n",
        "  print(f\"If the previous word is {previous_word}, the possible next words were: {word_probabilities[previous_word]}\")"
      ],
      "id": "p93-5CM23AzJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-CCu92N3KYt"
      },
      "source": [
        "Finally, we build our model in the same way as before. We save a set of probabilities per source label."
      ],
      "id": "P-CCu92N3KYt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3Q_ePHf3Qbh"
      },
      "source": [
        "  def calulate_conditionals_Ngram(X, Y, N):\n",
        "    # fill word_probs_by_source so it contains a dict for each source\n",
        "    # each containing each word and probability only in that source\n",
        "    # word_probs_by_source = {\n",
        "    #       'Twitter': {\n",
        "    #              'founded': 0.001,\n",
        "    #               ...,\n",
        "    #        },\n",
        "    #       'Wikipedia': {\n",
        "    #              'founded': 0.05,\n",
        "    #              ...,\n",
        "    #       }\n",
        "    #}\n",
        "    \n",
        "    # Get a list of the possible sources\n",
        "    # (Note: this is an ID not a name)\n",
        "    sources = set(Y)\n",
        "    ngram_probs_by_source = {}\n",
        "\n",
        "    for s in sources:\n",
        "      # Selects all the training examples which come from this source\n",
        "      source_training_data = [x for x,y in zip(X, Y) if y == s]\n",
        "\n",
        "      # Extract the counts and total for the specific source\n",
        "      source_counts, source_total = get_ngram_counts(source_training_data, N)\n",
        "\n",
        "      # Transform into probabilities\n",
        "      source_probabilities = get_probabilities_from_ngrams(source_counts, source_total)\n",
        "\n",
        "      #Save to the dictionary\n",
        "      word_probs_by_source[s] = source_probabilities\n",
        "    return word_probs_by_source\n",
        "\n",
        "bigram_model = calulate_conditionals_Ngram(X, Y, 2)"
      ],
      "id": "H3Q_ePHf3Qbh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20858d8d"
      },
      "source": [
        "Lets test our bigrams! The following cell lets you pick a word and prints the 5 most likely words to follow it."
      ],
      "id": "20858d8d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYikrY9D3Thh"
      },
      "source": [
        "source_label = \"shakespeare\"\n",
        "bigram = bigram_model[source_IDs[source_label]]\n",
        "first_word = input(\"Select first word\")\n",
        "sorted_bigrams = sorted(bigram[(first_word,)].items(), key=lambda x: x[1])\n",
        "print(\"Most likely next words:\")\n",
        "print(sorted_bigrams[-5:])"
      ],
      "id": "GYikrY9D3Thh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f72f6cf8"
      },
      "source": [
        "Now lets use bigrams to generate our text! Instead of sampling from all words from a source, we take into account the previously generated word!"
      ],
      "id": "f72f6cf8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "089ba808",
        "outputId": "1bf3ec51-36e7-4081-fb10-7986f8075e4e"
      },
      "source": [
        "def NB_gen_sentence_3(model, source, num_words, num_top_word, N):\n",
        "    \"\"\"Predict a random likely word\n",
        "    \"\"\"\n",
        "    # Select the ngrams for this source\n",
        "    ngrams_for_source = model[source]\n",
        "\n",
        "    # Setup the Ngram for the current sentence we are generating\n",
        "    # Remember for an Ngram of N words, we have N-1 previous words\n",
        "    # and the word we are generating is the remaining one\n",
        "    current_ngram = [''] * N\n",
        "\n",
        "    sentence_list = []\n",
        "    for i in range(num_words):\n",
        "      # The previous words are the previous ngram, without the first word (now too old)\n",
        "      previous_words = current_ngram[1:]\n",
        "\n",
        "\n",
        "      #*********** YOUR CODE HERE *******************\n",
        "      # Get a list of (word, likelihood) pairs for the current set of previous words\n",
        "\n",
        "      # sort the list, so the most likely words are at the start of the list (this is reverse of normal sorting)\n",
        "      # The key tells sorted to look at the second element for sorting, the likelihood\n",
        "\n",
        "      # Select the top words\n",
        "\n",
        "      # Choose a random word from the top words\n",
        "      selected_word = ...\n",
        "\n",
        "      # Update the current ngram\n",
        "\n",
        "      # ****************************\n",
        "\n",
        "      \n",
        "      sentence_list.append(selected_word)\n",
        "    return ' '.join(sentence_list)\n",
        "    \n",
        "text_source = \"shakespeare\"\n",
        "print(NB_gen_sentence_3(bigram_model, source_IDs[text_source], 10, 1000, 2))"
      ],
      "id": "089ba808",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "They have a positive, positive thing like people, despite all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUV0cUiX3vOe"
      },
      "source": [
        "We run into an issue again, where we may not have seen a particular ngram before. How can we predict the next word?\n",
        "\n",
        "We could just restart the current ngram, like we are starting a new sentence. Update your code above to perform this refresh!"
      ],
      "id": "JUV0cUiX3vOe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac9e8b95"
      },
      "source": [
        "We get more coherent sentences! But we can see that the patterns are not very long, focus tends to switch quickly. This is because our patterns only have a \"memory\" of one word, we can only make decisions one word back. We can increase this by using longer N-grams, lets try a tri-gram!"
      ],
      "id": "ac9e8b95"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaQVCejs4BHJ"
      },
      "source": [
        "trigram_model = calulate_conditionals_Ngram(X, Y, 3)"
      ],
      "id": "VaQVCejs4BHJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv0DYkaX4CdP"
      },
      "source": [
        "text_source = \"happy_topical_chat\"\n",
        "print(NB_gen_sentence_3(trigram_model, source_IDs[text_source], 50, 10, 3))"
      ],
      "id": "Rv0DYkaX4CdP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b05fdc12"
      },
      "source": [
        "We now see much more coherent sentences, even more so than the bigram model! Can we increase the N-gram length even further?\n",
        "\n",
        "Give it a try! However, we start to run into an issue. The more we increase the length, the less data we have for each N-gram. For example, if we looked for patterns 10 words long, it is likely we only would get the exact sentences that appear in the training data. In other words, our patterns are *too* detailed, so have fit too closely to the training data. \n",
        "\n",
        "This is very very important problem in machine learning! We are interested in patterns which *generalize* to new data. If we look too deeply into our data, we can always find more and more complex patterns, but are they actually useful?"
      ],
      "id": "b05fdc12"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fdd23bd"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this talk, we have focused on simple models for one small area machine learning can be applied to. \n",
        "\n",
        "I encorage you to take what you have learned, and think up some more areas where classification and generation can be applied!"
      ],
      "id": "3fdd23bd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4651f81"
      },
      "source": [
        ""
      ],
      "id": "f4651f81",
      "execution_count": null,
      "outputs": []
    }
  ]
}