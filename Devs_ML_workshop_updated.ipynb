{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Devs-ML-workshop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c747c885"
      },
      "source": [
        "# Machine Learning for text classification"
      ],
      "id": "c747c885"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab873446"
      },
      "source": [
        "Lets set the stage! After a year full of COVID disruptions the university has made all exams online. Celebrations all around! But wait, what about the cheating you say? Fortunately (or unfortunately), the uni has predicted this, and hired a crack team of Designated Exam Validators (or DEVs for short) to check for plagarism. And you've just been hired! And of course, you're going to use ML to do all your work for you!"
      ],
      "id": "ab873446"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96bd1fa9"
      },
      "source": [
        "# Problem Statement"
      ],
      "id": "96bd1fa9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "407bc8ef"
      },
      "source": [
        "*Given* a sentence, we need to find the probability a sentence came from each possible source."
      ],
      "id": "407bc8ef"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "796e54a3"
      },
      "source": [
        "For example, given the text \"The University of Auckland began as a constituent college of the University of New Zealand, founded on 23 May 1883 as Auckland University College\" the probability for \"Wikipedia\" = 0.9 and \"Twitter\" = 0.1"
      ],
      "id": "796e54a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dba17488"
      },
      "source": [
        "We need to build a machine learning classifier to do this for us!"
      ],
      "id": "dba17488"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b07db5d4"
      },
      "source": [
        "# 1) Collect data"
      ],
      "id": "b07db5d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5d6b023"
      },
      "source": [
        "First, we need some data from each source. This will allow us to find patterns, i.e., learn what makes *wikipedia* sentences different to *twitter* sentences\n",
        "\n",
        "This is normally the *most important part of ML*, but we've taken care of it! In the github repo there is a data folder containing some datasets from different sources.\n",
        "\n",
        "First we will just load this in."
      ],
      "id": "a5d6b023"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a87ce678",
        "outputId": "2a81ce99-2b81-4bd3-ed21-1bec682b9370"
      },
      "source": [
        "# Load in some libraries\n",
        "# The pathlib library makes handling filepaths easier, letting us open data files.\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Set the folder containing our data\n",
        "data_path = pathlib.Path('data')\n",
        "\n",
        "# Setup availiable filenames\n",
        "chess_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/chess.txt\"\n",
        "music_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/music.txt\"\n",
        "happy_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/happy_topical_chat.txt\"\n",
        "trumpspeech_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/trumpSpeech.txt\"\n",
        "wallstreetbets_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/wallstreetbets_comments.txt\"\n",
        "javascript_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/javascript.txt\"\n",
        "shakespeare_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/shakespeare.txt\"\n",
        "wiki_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/wiki.txt\"\n",
        "twitter_filename = \"https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/twitter2.txt\"\n",
        "\n",
        "filename_to_name_map = {\n",
        "    'chess': 'chess',\n",
        "    'music': 'music',\n",
        "    'happy_topical_chat': 'happy',\n",
        "    'trumpSpeech': 'trump',\n",
        "    'wallstreetbets_comments': 'wsb',\n",
        "    'javascript': 'js',\n",
        "    'shakespeare': 'shakespeare',\n",
        "    'wiki': 'wiki',\n",
        "    'twitter2': 'twitter'\n",
        "}\n",
        "\n",
        "# Just a helper function, don't worry about this one!\n",
        "def get_file_or_cache(path):\n",
        "    cache = None\n",
        "    def get():\n",
        "        nonlocal cache\n",
        "        if cache is None:\n",
        "            # with open(path, 'r', encoding='utf8') as f:\n",
        "            #     cache = f.readlines()\n",
        "            print(path)\n",
        "            df = pd.read_csv(path, delimiter = \"\\n\", error_bad_lines=False)\n",
        "            cache = list([str(x[0]) for x in df.values])\n",
        "        return cache\n",
        "    fn = path.split('/')[-1].split('.')[0]\n",
        "    return (get, filename_to_name_map[fn])\n",
        "\n",
        "# Construct a list of possible files and their names\n",
        "data_files = [get_file_or_cache(x) for x in [chess_filename, music_filename, happy_filename, trumpspeech_filename, javascript_filename, shakespeare_filename, wiki_filename, twitter_filename]]\n",
        "\n",
        "# Construct dataset of sentences and labels from every source\n",
        "# Note! Rather than using the actual names for the datasets, we give each one\n",
        "# its own numeric ID.\n",
        "# We record these so we can swap between human readable name and ID.\n",
        "X = []\n",
        "Y = []\n",
        "source_IDs = {}\n",
        "ID_to_source = {}\n",
        "source_ID = 0\n",
        "for source_constructor, source in data_files:\n",
        "    source_IDs[source] = source_ID\n",
        "    ID_to_source[source_ID] = source\n",
        "    lines = source_constructor()\n",
        "    for line in lines:\n",
        "        X.append(line)\n",
        "        Y.append(source_ID)\n",
        "    source_ID += 1\n",
        "\n",
        "observations = list(zip(X, Y))"
      ],
      "id": "a87ce678",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/chess.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/music.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/happy_topical_chat.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/trumpSpeech.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/javascript.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/shakespeare.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/wiki.txt\n",
            "https://raw.githubusercontent.com/BenHals/DevsMLWorkshop/main/data/twitter2.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 207: expected 1 fields, saw 2\\nSkipping line 535: expected 1 fields, saw 2\\nSkipping line 735: expected 1 fields, saw 2\\nSkipping line 805: expected 1 fields, saw 2\\nSkipping line 959: expected 1 fields, saw 2\\nSkipping line 1114: expected 1 fields, saw 2\\nSkipping line 1315: expected 1 fields, saw 2\\nSkipping line 1414: expected 1 fields, saw 2\\nSkipping line 1469: expected 1 fields, saw 2\\nSkipping line 1645: expected 1 fields, saw 2\\nSkipping line 1915: expected 1 fields, saw 2\\nSkipping line 1975: expected 1 fields, saw 2\\nSkipping line 2194: expected 1 fields, saw 2\\nSkipping line 2528: expected 1 fields, saw 2\\nSkipping line 2564: expected 1 fields, saw 2\\nSkipping line 2638: expected 1 fields, saw 2\\nSkipping line 2651: expected 1 fields, saw 2\\nSkipping line 2773: expected 1 fields, saw 2\\nSkipping line 3257: expected 1 fields, saw 2\\nSkipping line 3479: expected 1 fields, saw 2\\nSkipping line 3758: expected 1 fields, saw 2\\nSkipping line 3776: expected 1 fields, saw 2\\nSkipping line 3850: expected 1 fields, saw 2\\nSkipping line 4011: expected 1 fields, saw 2\\nSkipping line 4144: expected 1 fields, saw 2\\nSkipping line 4464: expected 1 fields, saw 2\\nSkipping line 4601: expected 1 fields, saw 2\\nSkipping line 5065: expected 1 fields, saw 2\\nSkipping line 5306: expected 1 fields, saw 2\\nSkipping line 5875: expected 1 fields, saw 2\\nSkipping line 5909: expected 1 fields, saw 2\\nSkipping line 6187: expected 1 fields, saw 2\\nSkipping line 6287: expected 1 fields, saw 2\\nSkipping line 6820: expected 1 fields, saw 2\\nSkipping line 7450: expected 1 fields, saw 2\\nSkipping line 7694: expected 1 fields, saw 2\\nSkipping line 7775: expected 1 fields, saw 2\\nSkipping line 7854: expected 1 fields, saw 2\\nSkipping line 8267: expected 1 fields, saw 2\\nSkipping line 8327: expected 1 fields, saw 2\\nSkipping line 8729: expected 1 fields, saw 2\\nSkipping line 8981: expected 1 fields, saw 2\\nSkipping line 9594: expected 1 fields, saw 2\\nSkipping line 9881: expected 1 fields, saw 2\\nSkipping line 9999: expected 1 fields, saw 2\\nSkipping line 10654: expected 1 fields, saw 2\\nSkipping line 10691: expected 1 fields, saw 2\\nSkipping line 10775: expected 1 fields, saw 2\\nSkipping line 10962: expected 1 fields, saw 2\\nSkipping line 11248: expected 1 fields, saw 2\\nSkipping line 11480: expected 1 fields, saw 2\\nSkipping line 11523: expected 1 fields, saw 2\\nSkipping line 11671: expected 1 fields, saw 2\\nSkipping line 11730: expected 1 fields, saw 2\\nSkipping line 11773: expected 1 fields, saw 2\\nSkipping line 11841: expected 1 fields, saw 2\\nSkipping line 11844: expected 1 fields, saw 2\\nSkipping line 12062: expected 1 fields, saw 2\\nSkipping line 12416: expected 1 fields, saw 2\\nSkipping line 12871: expected 1 fields, saw 2\\nSkipping line 12981: expected 1 fields, saw 2\\nSkipping line 13065: expected 1 fields, saw 2\\nSkipping line 13159: expected 1 fields, saw 2\\nSkipping line 13432: expected 1 fields, saw 2\\nSkipping line 13831: expected 1 fields, saw 2\\nSkipping line 13874: expected 1 fields, saw 2\\nSkipping line 13941: expected 1 fields, saw 2\\nSkipping line 14599: expected 1 fields, saw 2\\nSkipping line 14742: expected 1 fields, saw 2\\nSkipping line 14872: expected 1 fields, saw 2\\nSkipping line 15349: expected 1 fields, saw 2\\nSkipping line 15393: expected 1 fields, saw 2\\nSkipping line 15666: expected 1 fields, saw 2\\nSkipping line 15967: expected 1 fields, saw 2\\nSkipping line 16375: expected 1 fields, saw 2\\nSkipping line 16442: expected 1 fields, saw 2\\nSkipping line 16545: expected 1 fields, saw 2\\nSkipping line 16727: expected 1 fields, saw 2\\nSkipping line 17053: expected 1 fields, saw 2\\nSkipping line 17332: expected 1 fields, saw 2\\nSkipping line 17376: expected 1 fields, saw 2\\nSkipping line 17549: expected 1 fields, saw 2\\nSkipping line 17566: expected 1 fields, saw 2\\nSkipping line 17708: expected 1 fields, saw 2\\nSkipping line 17717: expected 1 fields, saw 2\\nSkipping line 17979: expected 1 fields, saw 2\\nSkipping line 18449: expected 1 fields, saw 2\\nSkipping line 18667: expected 1 fields, saw 2\\nSkipping line 18951: expected 1 fields, saw 2\\nSkipping line 19090: expected 1 fields, saw 2\\nSkipping line 19609: expected 1 fields, saw 2\\nSkipping line 19777: expected 1 fields, saw 2\\nSkipping line 20007: expected 1 fields, saw 2\\nSkipping line 20175: expected 1 fields, saw 2\\nSkipping line 20202: expected 1 fields, saw 2\\nSkipping line 20417: expected 1 fields, saw 2\\nSkipping line 20785: expected 1 fields, saw 2\\nSkipping line 20892: expected 1 fields, saw 2\\nSkipping line 20976: expected 1 fields, saw 2\\nSkipping line 21228: expected 1 fields, saw 2\\nSkipping line 21309: expected 1 fields, saw 2\\nSkipping line 21470: expected 1 fields, saw 2\\nSkipping line 21625: expected 1 fields, saw 2\\nSkipping line 21645: expected 1 fields, saw 2\\nSkipping line 21670: expected 1 fields, saw 2\\nSkipping line 21784: expected 1 fields, saw 2\\nSkipping line 22287: expected 1 fields, saw 2\\nSkipping line 22453: expected 1 fields, saw 2\\nSkipping line 22579: expected 1 fields, saw 2\\nSkipping line 22868: expected 1 fields, saw 2\\nSkipping line 23481: expected 1 fields, saw 2\\nSkipping line 23482: expected 1 fields, saw 2\\nSkipping line 23639: expected 1 fields, saw 2\\nSkipping line 23777: expected 1 fields, saw 2\\nSkipping line 23892: expected 1 fields, saw 2\\nSkipping line 24432: expected 1 fields, saw 2\\nSkipping line 24655: expected 1 fields, saw 2\\nSkipping line 24822: expected 1 fields, saw 2\\nSkipping line 25020: expected 1 fields, saw 2\\nSkipping line 25175: expected 1 fields, saw 2\\nSkipping line 25361: expected 1 fields, saw 2\\nSkipping line 25535: expected 1 fields, saw 2\\nSkipping line 25539: expected 1 fields, saw 2\\nSkipping line 25767: expected 1 fields, saw 2\\nSkipping line 25808: expected 1 fields, saw 2\\nSkipping line 25898: expected 1 fields, saw 2\\nSkipping line 25958: expected 1 fields, saw 2\\nSkipping line 26496: expected 1 fields, saw 2\\nSkipping line 26637: expected 1 fields, saw 2\\nSkipping line 27068: expected 1 fields, saw 2\\nSkipping line 27147: expected 1 fields, saw 2\\nSkipping line 27240: expected 1 fields, saw 2\\nSkipping line 27277: expected 1 fields, saw 2\\nSkipping line 27521: expected 1 fields, saw 2\\nSkipping line 28200: expected 1 fields, saw 2\\nSkipping line 28237: expected 1 fields, saw 2\\nSkipping line 28489: expected 1 fields, saw 2\\nSkipping line 28739: expected 1 fields, saw 2\\nSkipping line 28747: expected 1 fields, saw 2\\nSkipping line 28831: expected 1 fields, saw 2\\nSkipping line 29184: expected 1 fields, saw 2\\nSkipping line 29369: expected 1 fields, saw 2\\nSkipping line 29561: expected 1 fields, saw 2\\nSkipping line 29605: expected 1 fields, saw 2\\nSkipping line 30360: expected 1 fields, saw 2\\nSkipping line 30363: expected 1 fields, saw 2\\nSkipping line 31101: expected 1 fields, saw 2\\nSkipping line 31587: expected 1 fields, saw 2\\nSkipping line 31791: expected 1 fields, saw 2\\nSkipping line 32328: expected 1 fields, saw 2\\nSkipping line 32704: expected 1 fields, saw 2\\nSkipping line 32780: expected 1 fields, saw 2\\nSkipping line 32785: expected 1 fields, saw 2\\nSkipping line 32845: expected 1 fields, saw 2\\nSkipping line 34255: expected 1 fields, saw 2\\nSkipping line 34260: expected 1 fields, saw 2\\nSkipping line 34341: expected 1 fields, saw 2\\nSkipping line 34388: expected 1 fields, saw 2\\nSkipping line 34691: expected 1 fields, saw 2\\nSkipping line 34867: expected 1 fields, saw 2\\nSkipping line 35368: expected 1 fields, saw 2\\nSkipping line 35622: expected 1 fields, saw 2\\nSkipping line 35720: expected 1 fields, saw 2\\nSkipping line 35766: expected 1 fields, saw 2\\nSkipping line 35917: expected 1 fields, saw 2\\nSkipping line 36098: expected 1 fields, saw 2\\nSkipping line 36157: expected 1 fields, saw 2\\nSkipping line 36367: expected 1 fields, saw 2\\nSkipping line 36585: expected 1 fields, saw 2\\nSkipping line 36707: expected 1 fields, saw 2\\nSkipping line 37007: expected 1 fields, saw 2\\nSkipping line 37160: expected 1 fields, saw 2\\nSkipping line 37329: expected 1 fields, saw 2\\nSkipping line 37536: expected 1 fields, saw 2\\nSkipping line 37538: expected 1 fields, saw 2\\nSkipping line 37624: expected 1 fields, saw 2\\nSkipping line 37811: expected 1 fields, saw 2\\nSkipping line 37969: expected 1 fields, saw 2\\nSkipping line 38132: expected 1 fields, saw 2\\nSkipping line 38328: expected 1 fields, saw 2\\nSkipping line 38353: expected 1 fields, saw 2\\nSkipping line 38516: expected 1 fields, saw 2\\nSkipping line 39087: expected 1 fields, saw 2\\nSkipping line 39122: expected 1 fields, saw 2\\nSkipping line 39211: expected 1 fields, saw 2\\nSkipping line 39284: expected 1 fields, saw 2\\nSkipping line 39449: expected 1 fields, saw 2\\nSkipping line 39577: expected 1 fields, saw 2\\nSkipping line 39587: expected 1 fields, saw 2\\nSkipping line 39991: expected 1 fields, saw 2\\nSkipping line 40017: expected 1 fields, saw 2\\nSkipping line 40071: expected 1 fields, saw 2\\nSkipping line 40329: expected 1 fields, saw 2\\nSkipping line 40359: expected 1 fields, saw 2\\nSkipping line 40369: expected 1 fields, saw 2\\nSkipping line 40448: expected 1 fields, saw 2\\nSkipping line 40741: expected 1 fields, saw 2\\nSkipping line 40884: expected 1 fields, saw 2\\nSkipping line 40929: expected 1 fields, saw 2\\nSkipping line 41052: expected 1 fields, saw 2\\nSkipping line 41188: expected 1 fields, saw 2\\nSkipping line 41541: expected 1 fields, saw 2\\nSkipping line 41621: expected 1 fields, saw 2\\nSkipping line 41983: expected 1 fields, saw 2\\nSkipping line 42342: expected 1 fields, saw 2\\nSkipping line 42552: expected 1 fields, saw 2\\nSkipping line 42589: expected 1 fields, saw 2\\nSkipping line 42741: expected 1 fields, saw 2\\nSkipping line 42892: expected 1 fields, saw 2\\nSkipping line 42897: expected 1 fields, saw 2\\nSkipping line 42949: expected 1 fields, saw 2\\nSkipping line 43245: expected 1 fields, saw 2\\nSkipping line 43338: expected 1 fields, saw 2\\nSkipping line 43437: expected 1 fields, saw 2\\nSkipping line 43954: expected 1 fields, saw 2\\nSkipping line 44110: expected 1 fields, saw 2\\nSkipping line 44580: expected 1 fields, saw 2\\nSkipping line 44710: expected 1 fields, saw 2\\nSkipping line 44899: expected 1 fields, saw 2\\nSkipping line 45356: expected 1 fields, saw 2\\nSkipping line 45622: expected 1 fields, saw 2\\nSkipping line 45654: expected 1 fields, saw 2\\nSkipping line 45672: expected 1 fields, saw 2\\nSkipping line 46228: expected 1 fields, saw 2\\nSkipping line 46633: expected 1 fields, saw 2\\nSkipping line 46838: expected 1 fields, saw 2\\nSkipping line 47049: expected 1 fields, saw 2\\nSkipping line 47283: expected 1 fields, saw 2\\nSkipping line 47606: expected 1 fields, saw 2\\nSkipping line 47620: expected 1 fields, saw 2\\nSkipping line 47646: expected 1 fields, saw 2\\nSkipping line 47670: expected 1 fields, saw 2\\nSkipping line 47708: expected 1 fields, saw 2\\nSkipping line 47793: expected 1 fields, saw 2\\nSkipping line 47846: expected 1 fields, saw 2\\nSkipping line 48063: expected 1 fields, saw 2\\nSkipping line 48411: expected 1 fields, saw 2\\nSkipping line 48435: expected 1 fields, saw 2\\nSkipping line 49352: expected 1 fields, saw 2\\nSkipping line 49361: expected 1 fields, saw 2\\nSkipping line 49731: expected 1 fields, saw 2\\nSkipping line 49840: expected 1 fields, saw 2\\nSkipping line 49866: expected 1 fields, saw 2\\nSkipping line 49977: expected 1 fields, saw 2\\nSkipping line 50290: expected 1 fields, saw 2\\nSkipping line 50459: expected 1 fields, saw 2\\nSkipping line 50523: expected 1 fields, saw 2\\nSkipping line 50597: expected 1 fields, saw 2\\nSkipping line 50730: expected 1 fields, saw 2\\nSkipping line 50845: expected 1 fields, saw 2\\nSkipping line 51326: expected 1 fields, saw 2\\nSkipping line 51552: expected 1 fields, saw 2\\nSkipping line 51553: expected 1 fields, saw 2\\nSkipping line 51733: expected 1 fields, saw 2\\nSkipping line 51911: expected 1 fields, saw 2\\nSkipping line 52198: expected 1 fields, saw 2\\nSkipping line 52670: expected 1 fields, saw 2\\nSkipping line 53353: expected 1 fields, saw 2\\nSkipping line 53375: expected 1 fields, saw 2\\nSkipping line 53667: expected 1 fields, saw 2\\nSkipping line 53741: expected 1 fields, saw 2\\nSkipping line 53934: expected 1 fields, saw 2\\nSkipping line 54051: expected 1 fields, saw 2\\nSkipping line 54204: expected 1 fields, saw 2\\nSkipping line 54491: expected 1 fields, saw 2\\nSkipping line 54572: expected 1 fields, saw 2\\nSkipping line 54621: expected 1 fields, saw 2\\nSkipping line 54807: expected 1 fields, saw 2\\nSkipping line 54827: expected 1 fields, saw 2\\nSkipping line 54849: expected 1 fields, saw 2\\nSkipping line 54912: expected 1 fields, saw 2\\nSkipping line 54971: expected 1 fields, saw 2\\nSkipping line 55053: expected 1 fields, saw 2\\nSkipping line 55062: expected 1 fields, saw 2\\nSkipping line 56053: expected 1 fields, saw 2\\nSkipping line 56062: expected 1 fields, saw 2\\nSkipping line 56100: expected 1 fields, saw 2\\nSkipping line 56333: expected 1 fields, saw 2\\nSkipping line 56493: expected 1 fields, saw 2\\nSkipping line 56621: expected 1 fields, saw 2\\nSkipping line 56665: expected 1 fields, saw 2\\nSkipping line 56900: expected 1 fields, saw 2\\nSkipping line 57104: expected 1 fields, saw 2\\nSkipping line 57275: expected 1 fields, saw 2\\nSkipping line 57351: expected 1 fields, saw 2\\nSkipping line 57435: expected 1 fields, saw 2\\nSkipping line 58113: expected 1 fields, saw 2\\nSkipping line 58187: expected 1 fields, saw 2\\nSkipping line 58584: expected 1 fields, saw 2\\nSkipping line 58845: expected 1 fields, saw 2\\nSkipping line 59122: expected 1 fields, saw 2\\nSkipping line 59472: expected 1 fields, saw 2\\nSkipping line 59778: expected 1 fields, saw 2\\nSkipping line 60198: expected 1 fields, saw 2\\nSkipping line 60326: expected 1 fields, saw 2\\nSkipping line 60328: expected 1 fields, saw 2\\nSkipping line 60410: expected 1 fields, saw 2\\nSkipping line 60416: expected 1 fields, saw 2\\nSkipping line 60450: expected 1 fields, saw 3\\nSkipping line 61588: expected 1 fields, saw 2\\nSkipping line 61777: expected 1 fields, saw 2\\nSkipping line 61898: expected 1 fields, saw 2\\nSkipping line 61954: expected 1 fields, saw 2\\nSkipping line 62172: expected 1 fields, saw 2\\nSkipping line 62195: expected 1 fields, saw 2\\nSkipping line 62293: expected 1 fields, saw 2\\nSkipping line 62448: expected 1 fields, saw 2\\nSkipping line 63178: expected 1 fields, saw 2\\nSkipping line 63210: expected 1 fields, saw 2\\nSkipping line 63217: expected 1 fields, saw 2\\nSkipping line 63369: expected 1 fields, saw 2\\nSkipping line 63807: expected 1 fields, saw 2\\nSkipping line 63947: expected 1 fields, saw 2\\nSkipping line 64222: expected 1 fields, saw 2\\nSkipping line 64254: expected 1 fields, saw 2\\nSkipping line 64441: expected 1 fields, saw 2\\nSkipping line 64790: expected 1 fields, saw 2\\nSkipping line 64936: expected 1 fields, saw 2\\nSkipping line 64946: expected 1 fields, saw 2\\nSkipping line 64981: expected 1 fields, saw 2\\nSkipping line 65125: expected 1 fields, saw 2\\nSkipping line 65185: expected 1 fields, saw 2\\nSkipping line 65622: expected 1 fields, saw 2\\nSkipping line 65949: expected 1 fields, saw 2\\nSkipping line 66033: expected 1 fields, saw 2\\nSkipping line 66202: expected 1 fields, saw 2\\nSkipping line 66219: expected 1 fields, saw 2\\nSkipping line 66741: expected 1 fields, saw 2\\nSkipping line 66935: expected 1 fields, saw 2\\nSkipping line 67474: expected 1 fields, saw 2\\nSkipping line 67496: expected 1 fields, saw 2\\nSkipping line 67531: expected 1 fields, saw 2\\nSkipping line 67562: expected 1 fields, saw 2\\nSkipping line 68077: expected 1 fields, saw 2\\nSkipping line 68124: expected 1 fields, saw 2\\nSkipping line 68346: expected 1 fields, saw 2\\nSkipping line 68455: expected 1 fields, saw 2\\nSkipping line 68709: expected 1 fields, saw 2\\nSkipping line 69124: expected 1 fields, saw 2\\nSkipping line 69143: expected 1 fields, saw 2\\nSkipping line 69263: expected 1 fields, saw 2\\nSkipping line 69322: expected 1 fields, saw 2\\nSkipping line 69336: expected 1 fields, saw 2\\nSkipping line 69423: expected 1 fields, saw 2\\nSkipping line 69577: expected 1 fields, saw 2\\nSkipping line 69677: expected 1 fields, saw 2\\nSkipping line 69869: expected 1 fields, saw 2\\nSkipping line 69883: expected 1 fields, saw 2\\nSkipping line 69911: expected 1 fields, saw 2\\nSkipping line 70248: expected 1 fields, saw 2\\nSkipping line 70352: expected 1 fields, saw 2\\nSkipping line 70368: expected 1 fields, saw 2\\nSkipping line 70387: expected 1 fields, saw 2\\nSkipping line 70611: expected 1 fields, saw 2\\nSkipping line 70640: expected 1 fields, saw 2\\nSkipping line 70704: expected 1 fields, saw 2\\nSkipping line 70910: expected 1 fields, saw 2\\nSkipping line 70948: expected 1 fields, saw 2\\nSkipping line 71071: expected 1 fields, saw 2\\nSkipping line 71326: expected 1 fields, saw 2\\nSkipping line 71798: expected 1 fields, saw 2\\nSkipping line 71933: expected 1 fields, saw 2\\nSkipping line 72203: expected 1 fields, saw 2\\nSkipping line 72592: expected 1 fields, saw 2\\nSkipping line 72636: expected 1 fields, saw 2\\nSkipping line 73180: expected 1 fields, saw 2\\nSkipping line 73291: expected 1 fields, saw 2\\nSkipping line 73909: expected 1 fields, saw 2\\nSkipping line 74258: expected 1 fields, saw 2\\nSkipping line 74353: expected 1 fields, saw 2\\nSkipping line 74398: expected 1 fields, saw 2\\nSkipping line 74427: expected 1 fields, saw 2\\nSkipping line 74443: expected 1 fields, saw 2\\nSkipping line 74466: expected 1 fields, saw 2\\n'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aeb8f97"
      },
      "source": [
        "Now we have our data loaded into X, a list of sentences, and Y, a list of the sources each sentence came from.\n",
        "Lets check out some sample lines from each source!"
      ],
      "id": "0aeb8f97"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce1dc4fc",
        "outputId": "d75ec484-86e3-4804-e550-18700e93b244"
      },
      "source": [
        "inspect_index = int(random.random() * len(observations))\n",
        "for index, (line, source) in enumerate(observations[inspect_index:inspect_index+5]):\n",
        "    print(\"Observation\", index)\n",
        "    print(\"X{} = \".format(index), line)\n",
        "    # Source is stored as a number, representing the source (explained later!)\n",
        "    print(\"Y{} =\".format(index), source, \"Source name:\", ID_to_source[source])\n",
        "    print('-----------')"
      ],
      "id": "ce1dc4fc",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation 0\n",
            "X0 =   == events == === 270 === ==== by place ==== ====== roman empire ====== emperor claudius ii gothicus dies of plague while preparing to fight the vandals and sarmatians who have invaded pannonia he is succeeded by his brother quintillus who briefly holds power over the roman empire quintillus commits suicide and is succeeded by an associate of his brother lucius domitius aurelianus the military leader who distinguished himself last year at the battle of naissus serbia aurelianus pushes the goths back across the danube and recovers roman territory the romans leave utrecht after regular invasions of germanic tribes crisis of the third century an economic crisis strikes the roman empire due to the partition of the empire invasions and usurpations and the sacking of the countryside and cities by invaders agricultural and industrial production are significantly decreased and mines lie unused a monetary crisis ensues including inflation of up to 1,000 % in certain areas of the empire ====== southeast asia ====== fan hsiung aka pham hung comes to power in champa and raids the chinese-occupied territory of tonkin ====== egypt ====== the kingdom of aksum modern ethiopia begins minting its own gold coins to facilitate international trade following the model of roman coinage anthony the great a christian saint from egypt regarded as father of all monks enters the wilderness to become ascetic ==== by topic ==== ====== technology ====== the chinese invent gunpowder black powder a mixture of sulfur charcoal and potassium nitrate it appears to have first been used only for fireworks === 271 === ==== by place ==== ====== roman empire ====== emperor aurelian pushes the vandals back from pannonia and forces them to withdraw over the danube he withdraws the roman army from dacia modern romania note this may have lasted until 272 both years are mentioned by various sources january battle of placentia the juthungi invade italy and sack the city of piacenza a roman army 15,000 men under emperor aurelian is ambushed and defeated battle of fano the juthungi move towards a defenceless rome aurelian rallies his men and defeats the germanic tribes on the metauro river just inland of fano battle of pavia the roman army pursues the alamanni in lombardy aurelian closes the passes in the alps and encircles the invaders near pavia the alamanni are destroyed and aurelian receives the title germanicus maximus felicissimus financial minister of the state treasury leads an uprising of mint workers against aurelian he is defeated and killed on the caelian hill aurelian begins construction of a new defensive wall to protect rome the aurelian walls 19 kilometers 12 mi enclose the city with fortifications ====== europe ====== victorinus emperor of the gallic empire is assassinated by attitianus reportedly for reasons of personal revenge domitianus presumably serves as emperor for a few days before being replaced by tetricus i ====== syria ====== zenobia proclaims herself to be empress and breaks all relations with the roman empire zenobia gives her son vaballathus the title of augustus ==== by topic ==== ====== arts and sciences ====== king shapur i builds the academy of gundishapur iran which becomes the intellectual center of the sassanid empire the nestorians fleeing religious persecution seek his protection he commissions the refugees to translate greek and syriac works on astronomy medicine and philosophy a magnetic compass is first used in china === 272 === ==== by topic ==== ====== roman empire ====== emperor aurelian sends his commander marcus aurelius probus to restore roman rule in egypt during the siege of tyana emperor aurelian has a dream of apollonius of tyana and spares the city battle of immae aurelian defeats the forces of the palmyrene empire near antioch queen zenobia flees under cover of darkness to emesa syria battle of emesa aurelian destroys the palmyrene heavy cavalry clibanarii and conquers palmyra zenobia escapes to persia but is captured on the euphrates aurelian lays siege to palmyra after a revolt he restores roman control and sacks the city zenobia and her son vabalathus are forced to parade in golden chains through the streets of rome rome forms an alliance with the king of aksum axum dacia is abandoned ====== persia ====== king shapur i of persia dies after a reign of more than 30 years he is succeeded by his eldest son hormizd i ==== by topic ==== ====== religion ====== dometius succeeds titus as patriarch of constantinople saint denis first bishop of paris and two of his disciples are beheaded on the road to the temple of mercury that stands atop a hill outside of the city the hill will later be called montmartre mountain of martyrs in lutetia modern paris paul of samosata is deposed as patriarch of antioch === 273 === ==== by place ==== ====== roman empire ====== emperor aurelian sacks the city of palmyra after putting down a new revolt the kingdom of palmyra is reunited with the roman empire aurelian refuses to wear the imperial crown and coat marcus claudius tacitus future roman emperor is consul in rome tetricus i and tetricus ii are deposed as gallic emperors by aurelian administrative reorganization of italy aurelian adopts as permanent the reforms instituted by caracalla aurelian increases rome 's daily bread ration to nearly 1.5 pounds and adds pig fat to the list of foods distributed free to the populace cassius longinus counselor of queen zenobia is executed by the romans for conspiring against aurelian an indian delegation visits aurelian ====== persia ====== king hormizd i of persia dies after a brief reign in which he has shown tolerance toward the ascetic anti-materialist manichean faith he is succeeded by his brother bahram i who has been governing the province of atropatene === 274 === ==== by place ==== ====== roman empire ====== battle of châlons emperor aurelian reconquers the gallic empire gaul and britain tetricus i surrenders his army near châlons-sur-marne france with the conquests of the palmyrene empire and the gallic empire the roman empire is united again rome greets aurelian as restitutor orbis restorer of the world and accords him a magnificent triumph victory procession which is graced by his captives tetricus i and his son tetricus ii aurelian issues an important reform of roman currency germanic tribes take advantage of the destroyed roman forces of the rhine they pillage and depopulate large areas of gaul including paris the rhine border is lost for 20 years franks live in the area of present southern netherlands northern belgium and rhineland from now on december 25 aurelian has a temple dedicated to sol invictus on the third day after the solstice and day of rebirth of the sun this religion which is in essence monotheistic becomes the state religion of rome britain rebels over the value of coinage ====== africa ====== the kingdom of aksum attains great prosperity thanks to its control of red sea trade ==== by topic ==== ====== religion ====== march 2 mani a sage of persia dies at gundeshapur after 30 years of preaching his heresy at the court of the late sassanian king shapur i and on long journeys to khorasan india and china he is executed or allowed to die in prison and claims to be a prophet of god mani combines zoroastrian dualism with christian theology and his disciples gain wide support for manichaeism despite opposition from byzantine and roman emperors december 30 pope felix i dies in rome after a 5-year reign ====== transportation ====== japanese shipwrights build a 100-foot oar-powered vessel for emperor ōjin the japanese will not use sails for another seven centuries === 275 === ==== by place ==== ====== roman empire ====== emperor aurelian prepares a campaign against the sassanids in asia minor in thrace while waiting to cross the bosphorus with his army he hands out severe punishments to corrupt soldiers and makes a list of high-ranking officers marked for execution september aurelian falls victim to a conspiracy of the praetorian guard and is murdered near byzantium turkey september 25 marcus claudius tacitus is proclaimed emperor by the senate his half brother marcus annius florianus becomes praetorian prefect gaul is pillaged by the franks and the alemanni ====== india ====== the pallava dynasty begins in southern india ==== by topic ==== ====== religion ====== january 4 pope eutychian succeeds pope felix i as the 27th pope === 276 === ==== by place ==== ====== roman empire ====== emperor tacitus doubles the silver content of the aurelianianus and halves its tariffing to 2.5 d.c. carry the value marks x.i marcus annius florianus defeats the goths and alans who have invaded asia minor tacitus dies of illness or is murdered at tyana in cappadocia florianus becomes roman emperor he breaks off his campaign against the heruli and marches from the bosporus with support from the roman legions in britain gaul spain and italy to fight an indecisive battle with marcus aurelius probus in cilicia florianus holds power for some weeks but is assassinated by his own troops near tarsus turkey probus age 44 is proclaimed new emperor of rome probus appoints marcus aurelius carus to prefect of the praetorian guard he returns the aurelianianus to the standard and official tariffing of aurelian ====== asia ====== king bahram i of persia dies after a 3-year reign in which the zoroastrian priests at ctesiphon iran put pressure on him to persecute buddhists christians and manichaeans he is succeeded by his son bahram ii reign of mahasena in ceylon orthodox and unpopular he tries to introduce mahayana buddhism to the country === 277 === ==== by place ==== ====== roman empire ====== emperor probus travels with his army west across the sea of marmara turkey and through the provinces of thrace moesia and pannonia to defeat the goths along the lower danube he acquires from the troops the title of gothicus probus enters rome to have his position as emperor ratified by the senate ====== asia ====== tuoba xilu succeeds his father tuoba liwei as chieftain of the tuoba clan === 278 === ==== by place ==== ====== roman empire ====== emperor probus defeats the alamanni advancing through the neckar valley he expels the franks from gaul and reorganizes the roman defenses on the rhine probus resettles the germanic tribes in the devastated provinces of the roman empire he adopts the titles of gothicus maximus and germanicus maximus piracy along the coast of lycia and pamphylia the romans besiege the city of cremna pisidia and kill the isaurian robber lydius === 279 === ==== by place ==== ====== roman empire ====== emperor probus defeats the burgundians and vandals in raetia and pannonia modern switzerland and hungary ====== china ====== winter sima yan emperor of the jin dynasty launches the jin offensive and attacks along the yangzi river from jianye to jiangling whilst the sichuan fleet sails downriver to the jing province they capture the border cities and the wu forces collapse this marks the end of the three kingdoms china\n",
            "Y0 = 6 Source name: wiki\n",
            "-----------\n",
            "Observation 1\n",
            "X1 =   1799 mdccxcix was a common year starting on tuesday of the gregorian calendar and a common year starting on saturday dominical letter b of the julian calendar the 1799th year of the common era ce and anno domini ad designations the 799th year of the 2nd millennium the 99th year of the 18th century and the 10th and last year of the 1790s decade as of the start of 1799 the gregorian calendar was 11 days ahead of the julian calendar which remained in localized use until 1923\n",
            "Y1 = 6 Source name: wiki\n",
            "-----------\n",
            "Observation 2\n",
            "X2 =   year 1471 mcdlxxi was a common year starting on tuesday link will display the full calendar of the julian calendar\n",
            "Y2 = 6 Source name: wiki\n",
            "-----------\n",
            "Observation 3\n",
            "X3 =   2002 mmii was a common year starting on tuesday of the gregorian calendar the 2002nd year of the common era ce and anno domini ad designations the 2nd year of the 3rd millennium the 2nd year of the 21st century and the 3rd year of the 2000s decade 2002 was designated as international year of ecotourism international year of mountains\n",
            "Y3 = 6 Source name: wiki\n",
            "-----------\n",
            "Observation 4\n",
            "X4 =   51-forth is an implementation of the forth programming language for the intel 8051 microcontroller it was created in 1989 by scott gehmlich of idacom electronics which was acquired by hewlett-packard in 1990 and sent to giovanni moretti of massey university from whom it was propagated widely the original 51forth.zip package is available from many archive sites along with several other implementations of forth this implementation is subroutine-threaded with about 20 words written in assembly language and the complete system occupying a total of about 8k of ram it was cross-developed from a vax to an rtx2000 forth system connected to dual-ported ram accessible to the microcontroller the sources and documentation are in the public domain\n",
            "Y4 = 6 Source name: wiki\n",
            "-----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18b3725b"
      },
      "source": [
        "# How do we *actually* predict?\n",
        "\n",
        "The goal is to learn a model which can tell the difference between classes. For example, consider sentences from Wikipedia (W) or Twitter (T). If we show our model a new input sentence (X) where we do NOT know the origin, our model should be able to tell where the sentence came from. \n",
        "\n",
        "Formally, the $i^{th}$ input is called $x_{i}$ and its true class is called $y_{i}$.\n",
        "For us, $x_{i}$ is the $i^{th}$ sentence we need to label and $y_{i}$ is where this sentence actually came from.\n",
        "Our model predicts some label, $\\hat y_{i}$, for this sentence (so *wikipedia* or *twitter*).\n",
        "We want to train it so that *our* label, $\\hat y_{n}$, is close to the *true* label, $y_{i}$.\n",
        "\n",
        "There are many, many different ways to do this prediction, and we will look at a simple one called Naive Bayes.\n",
        "\n",
        "Lets true a manual version to get the idea:\n"
      ],
      "id": "18b3725b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95203343",
        "outputId": "fcf1845f-6cd4-4f1d-c574-e87368f720f8"
      },
      "source": [
        "# Pick two random labels to choose from\n",
        "g, label_opt1 = random.choice(data_files)\n",
        "g, label_opt2 = random.choice(data_files)\n",
        "\n",
        "# Select a random observations from a random source\n",
        "source_gen, true_label = random.choice(data_files)\n",
        "\n",
        "# Shuffle labels\n",
        "label_options = [label_opt1, label_opt2, true_label]\n",
        "random.shuffle(label_options)\n",
        "\n",
        "lines = source_gen()\n",
        "line = random.choice(lines)\n",
        "print(line)\n",
        "predicted_label = input(\"Where did this sentence come from? {} or {} or {}? \\n\".format(*label_options))\n",
        "\n",
        "#****** YOUR CODE HERE! ***********\n",
        "# First challenge to get you started:\n",
        "# How can we tell if the predicted label (what the user typed for now)\n",
        "# matches the true label?\n",
        "prediction_iscorrect = ...\n",
        "#***********************************\n",
        "\n",
        "# Now we use whether or not the prediction was correct to inform the user.\n",
        "print(\"You were\", \"Right\" if prediction_iscorrect else \"Wrong\", \"!\")\n",
        "print(\"Your predicted label (y^) was: {}. The true label (y) was {}\".format(predicted_label, true_label))"
      ],
      "id": "95203343",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@USER My other sale did not have the funds withheld\n",
            "Where did this sentence come from? shakespeare or twitter2 or chess? \n",
            "twitter2\n",
            "You were Right !\n",
            "Your predicted label (y^) was: twitter2. The true label (y) was twitter2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e71eed05"
      },
      "source": [
        "## Using a model to make a prediction\n",
        " \n",
        " How can we *train the computer* to predict a class for us? \n",
        " Lets think of a simple example: We are given a sentence ($x$), and need to predict the most likely class ($\\hat y$), Wikipedia (W) or Twitter (T). We start by asking a friend what class they think is most likely and they say Wikipedia has a 70% chance and Twitter and 30%. \n",
        "\n",
        " In mathmatical terms we can rewrite the \"_probability that the class is Wikipedia given this specific sentence $x$ , $\\hat y = W$, is 0.7_\" as $p(\\hat y = W | x) = 0.7$. Similarly, we can write $p(\\hat y = T | x) = 0.3$ for the sentence being from twitter.\n",
        "\n",
        " Now it is pretty easy to make a prediction, Wikipedia has a 70% chance and Twitter only has a 30% chance so we should predict Wikipedia! \n",
        "\n",
        " ### But how did our friend come up with $p(\\hat y| x)$ in the first place?\n",
        " This is basically what the computer needs to do, find the probability of a sentence coming from each source.\n",
        " This is what Naive Bayes solves!\n",
        "\n",
        " ## Naive Bayes - Probability theory (spooky)\n",
        "\n",
        " Note: Naive Bayes is pretty simple, and is quite intuitive when you wrap your head around it, but if this is your first introduction to probability it can be quite confusing! If you don't understand at first don't get discoraged! I find that drawing diagrams and thinking about it from a few directions helps really understand.\n",
        "\n",
        " The end goal of a model is to calculate $p(y = C|X=x)$. In plain english, this can be read as calculate the _probability that the true class of the input is C given what we know about the sentence_. For a concrete example, lets use the sentence \"The University of Auckland was founded on 23 May 1883\". We want to predict the probability that $y = Wikipedia$ or $y = Twitter$ given that the sentence $x$ = \"The University of Auckland was founded on 23 May 1883\". This can be difficult to calculate, and isn't really how we think about things as humans.\n",
        "\n",
        " Instead of calculating this directly, _Bayes Theorum_ gives us a way to swap things around.\n",
        "\n",
        " $$ p(y=C|X=x) = p(X=x|y=C)p(y=C)$$\n",
        "\n",
        " This is like thinking \"what is the probability that the sentence was found on twitter (or wikipedia)\". This is a bit more natural to think about, and it turns out easier to calculate.\n",
        " \n",
        " ## Intuition\n",
        " Instead of directly trying to work out the probability of wikipedia or twitter, lets look at indivdual words. If we see the words \"follow me!\" we can say twitter has a pretty high probability. On the other hand, if we see the words \"Auckland[1][2]\" we could say wikipedia has a high probability. How did we do this?\n",
        " \n",
        " We looked at the probability of each word (or few words) coming from each source! We know that 'following' is something people on twitter do, so if we see this word we know it is more likely to have come from twitter than wikipedia. This is the $p(x=X|y=C)$ in the formula!\n",
        " \n",
        "Lets see if we can use this to do some machine learning!"
      ],
      "id": "e71eed05"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8624bc10"
      },
      "source": [
        "# Exercise 1: Calculate word probabilities"
      ],
      "id": "8624bc10"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8923fd5b"
      },
      "source": [
        "We need to *learn* 2 things from our data.\n",
        "1. What is the probability of seeing each word overall?\n",
        "    1. This is $p(x=X)$, or the *probability* that a word is X\n",
        "    2. e.g p('and') is high, it occurs a lot! But p('founded') is low, it doesn't occur too often.\n",
        "2. What is the probability of seeing each word *from each source*?\n",
        "    1. This is $p(x=X|y=Y)$, or the *probability* that a word is X if we *know* it is from e.g wikipedia\n",
        "    2. e.g. p('founded'|Twitter) might be pretty low, its an uncommon word on twitter. But p('founded'|Wikipedia) is high, it occurs all the time on wikipedia!"
      ],
      "id": "8923fd5b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL8q3vvUQSJa"
      },
      "source": [
        "## Coding Challenge 1 - Calculate the probability of words\n",
        "\n",
        "Lets start simple - How can we calculate the probability of a word occuring?\n",
        "\n",
        "One way to think about this: We count combine all our training data into a huge list of words. Lets say we have $N$ words overall (including all repetitions). Then I tell you a word, e.g., \"hello\". If you closed your eyes and pointed to a random word in the list, what are the chances you would end up pointing to the word \"hello\"?\n",
        "\n",
        "We can calulate this as: The number of times a word occurs in the training data, divided by the total number of words.\n",
        "\n",
        "### First lets count each word"
      ],
      "id": "wL8q3vvUQSJa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCmieuDQQQQ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "5136719d-3be6-4f5b-8617-54188b0650ab"
      },
      "source": [
        "  def get_word_counts(training_sentences):\n",
        "    # fill word_counts so it contains the number of times each word occurs.\n",
        "    # word_counts = {\n",
        "    #       'and': 700,\n",
        "    #       'founded': 15,\n",
        "    #       ....\n",
        "    #}\n",
        "    word_counts = {}\n",
        "    total_num_words = 0\n",
        "\n",
        "    # We want to look though all the training sentences,\n",
        "    for sentence in training_sentences:\n",
        "\n",
        "      # We can use .split to split each sentence into words\n",
        "      words = sentence.split()\n",
        "\n",
        "      for word in words:\n",
        "        #******** YOUR CODE HERE************\n",
        "        #Add code here to update word_counts and total_num_words\n",
        "        #We want to end up with the number of times we see each word\n",
        "        #**********************************\n",
        "    return word_counts, total_num_words\n",
        "\n",
        "word_counts, total_num_words = get_word_counts(X)\n",
        "print(f\"There are {total_num_words} overall\")\n",
        "print(f\"We saw the word 'hello' {word_counts['hello']} times!\")\n",
        "print(f\"We saw the word 'and' {word_counts['and']} times!\")"
      ],
      "id": "uCmieuDQQQQ7",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-845b6280ad44>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    return word_counts, total_num_words\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMVF8rn3SkE1"
      },
      "source": [
        "Now lets calculate the probabilities:"
      ],
      "id": "dMVF8rn3SkE1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "lkejFgvDS3uk",
        "outputId": "461fef62-522e-4cd8-9f42-a83bc8bb2f30"
      },
      "source": [
        "def get_probabilities_from_counts(word_counts, total_num_words):\n",
        "  # fill word probabilities so it contains the probability of each word\n",
        "  # word_probabilities = {\n",
        "  #       'and': 0.75,\n",
        "  #       'founded': 0.01,\n",
        "  #       ....\n",
        "  #}\n",
        "  word_probabilities = {}\n",
        "\n",
        "  for w in word_counts:\n",
        "    #******* YOUR CODE HERE********\n",
        "    #Add code here to update word_probabilities\n",
        "    #To contain the probability of each word.\n",
        "    #Remember, this should be the count of each word / total\n",
        "  return word_probabilities\n",
        "\n",
        "word_counts, total_num_words = get_word_counts(X)\n",
        "word_probabilities = get_probabilities_from_counts(word_counts, total_num_words)\n",
        "\n",
        "print(f\"We saw the word 'hello' {word_probabilities['hello'] * 100}% of the time!\")\n",
        "print(f\"We saw the word 'and' {word_probabilities['and'] * 100}% of the time!\")"
      ],
      "id": "lkejFgvDS3uk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-013e1a29fb71>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    #******* YOUR CODE HERE********\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAhtrzwNTdzF"
      },
      "source": [
        "# Coding challenge 2 - Separate probabilities for each class\n",
        "\n",
        "We have just calculated the probabilities of words *overall*.\n",
        "\n",
        "What we actually need for Naive Bayes is the probabilities of words in each *source*.\n",
        "\n",
        "How can we update our code to instead calculate the probabilities per source?\n",
        "\n",
        "*Hint: What do we calculate if we do the same process on only training sentences from one source?*"
      ],
      "id": "qAhtrzwNTdzF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnOH1NM9VfiG"
      },
      "source": [
        "  def calulate_conditionals(X, Y):\n",
        "    # fill word_probs_by_source so it contains a dict for each source\n",
        "    # each containing each word and probability only in that source\n",
        "    # word_probs_by_source = {\n",
        "    #       'Twitter': {\n",
        "    #              'founded': 0.001,\n",
        "    #               ...,\n",
        "    #        },\n",
        "    #       'Wikipedia': {\n",
        "    #              'founded': 0.05,\n",
        "    #              ...,\n",
        "    #       }\n",
        "    #}\n",
        "    \n",
        "    # Get a list of the possible sources\n",
        "    # (Note: this is an ID not a name)\n",
        "    sources = set(Y)\n",
        "    word_probs_by_source = {}\n",
        "\n",
        "    for s in sources:\n",
        "      # Selects all the training examples which come from this source\n",
        "      source_training_data = [x for x,y in zip(X,Y) if y == s]\n",
        "      #*********** YOUR CODE HERE****************\n",
        "      #Add code which calculates the word probabilities for just one source\n",
        "      #And adds it to word_probs_by_source\n",
        "      #******************************************\n",
        "    return word_probs_by_source\n",
        "\n",
        "word_probs_by_source = calulate_conditionals(X, Y)\n",
        "print(f\"The probability of seeing the word 'wall' in Trumps speechs was {word_probs_by_source[source_IDs['trump']]['wall']}\")\n",
        "print(f\"The probability of seeing the word 'thou' in shakespeare was {word_probs_by_source[source_IDs['shakespeare']]['thou']}\")"
      ],
      "id": "DnOH1NM9VfiG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGv0jVLhYzZp"
      },
      "source": [
        "# This will raise an error, because we did not see the word wall!\n",
        "print(f\"The probability of seeing the word 'wall' in chess was {word_probs_by_source[source_IDs['chess']]['wall']}\")"
      ],
      "id": "DGv0jVLhYzZp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A6IajhgW30n"
      },
      "source": [
        "### Problem - what about words which don't appear in some sources?\n",
        "\n",
        "For example, what if we never see the word \"Twitter\" in any sentence collected from wikipedia?\n",
        "\n",
        "We will get a probability p(\"Twitter\"|Wikipedia) = 0!\n",
        "\n",
        "This may be true in our training data, but means that we cannot predict any sentences with Twitter in them...\n",
        "\n",
        "Solution: Add 1 to the count of *every* word! This avoids all 0s! But we also have to add to the denominator to balance this out."
      ],
      "id": "1A6IajhgW30n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OOzpIggY5PQ"
      },
      "source": [
        "  def calulate_conditionals(X, Y, unique_words):\n",
        "    # fill word_probs_by_source so it contains a dict for each source\n",
        "    # each containing each word and probability only in that source\n",
        "    # word_probs_by_source = {\n",
        "    #       'Twitter': {\n",
        "    #              'founded': 0.001,\n",
        "    #               ...,\n",
        "    #        },\n",
        "    #       'Wikipedia': {\n",
        "    #              'founded': 0.05,\n",
        "    #              ...,\n",
        "    #       }\n",
        "    #}\n",
        "    \n",
        "    # Get a list of the possible sources\n",
        "    # (Note: this is an ID not a name)\n",
        "    sources = set(Y)\n",
        "    word_probs_by_source = {}\n",
        "\n",
        "    for s in sources:\n",
        "      # Selects all the training examples which come from this source\n",
        "      source_training_data = [x for x,y in zip(X,Y) if y == s]\n",
        "      #*********** YOUR CODE HERE****************\n",
        "      #Add code which calculates the word probabilities for just one source\n",
        "      #And adds it to word_probs_by_source.\n",
        "      #Make sure to add 1 to the count for every word\n",
        "      #And adjust the total number of words!\n",
        "      #******************************************\n",
        "    return word_probs_by_source\n",
        "\n",
        "word_probs_by_source = calulate_conditionals(X, Y, [word for word in word_counts])\n",
        "print(f\"The probability of seeing the word 'wall' in Trumps speechs was {word_probs_by_source[source_IDs['trump']]['wall']}\")\n",
        "print(f\"The probability of seeing the word 'thou' in shakespeare was {word_probs_by_source[source_IDs['shakespeare']]['thou']}\")\n",
        "print(f\"The probability of seeing the word 'wall' in chess was {word_probs_by_source[source_IDs['chess']]['wall']}\")"
      ],
      "id": "1OOzpIggY5PQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tdGO1HOY8Rf"
      },
      "source": [
        " ## This is everything we need to learn from the data to make predictions!\n"
      ],
      "id": "5tdGO1HOY8Rf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjxKHmd-ZGY2"
      },
      "source": [
        "NB_model = calulate_conditionals(X, Y, [word for word in word_counts])"
      ],
      "id": "PjxKHmd-ZGY2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d25ba24"
      },
      "source": [
        "Lets check the output but showing the most likely words for each source"
      ],
      "id": "2d25ba24"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f66049f",
        "outputId": "e280bd71-e3ae-4c7f-d2cd-cb32aac8d092"
      },
      "source": [
        "for gen, source in data_files:\n",
        "    word_probs = NB_model[source_IDs[source]].items()\n",
        "    print(source)\n",
        "    print(sorted(word_probs, key = lambda x: x[1])[-20:])"
      ],
      "id": "0f66049f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chess\n",
            "[('16.', 0.0016448923594571498), ('d4', 0.001648460455681569), ('15.', 0.0016627328405792448), ('14.', 0.0016859254660379682), ('13.', 0.001700197850935644), ('12.', 0.00171447023583332), ('11.', 0.001718038332057739), ('10.', 0.0017216064282821579), ('9.', 0.0017305266688432055), ('7.', 0.0017447990537408813), ('8.', 0.0017447990537408813), ('6.', 0.0017483671499653002), ('5.', 0.0017590714386385572), ('4.', 0.0017644235829751857), ('3.', 0.0017662076310873951), ('2.', 0.0017715597754240236), ('Nf6', 0.0017751278716484427), ('1.', 0.00178404811220949), ('Nf3', 0.0018464897961368223), ('O-O', 0.0025654611853572467)]\n",
            "music\n",
            "[('z', 0.001087623754584232), ('G', 0.0010907717249303511), ('F2', 0.0011128075173531865), ('E2', 0.0012040986573906474), ('f2', 0.0012953897974281082), ('A', 0.0013552012340043757), ('d', 0.001506303810618104), ('c2', 0.0017266617348464578), ('e2', 0.0018557285190373507), ('g2', 0.0019249838666519762), ('D2', 0.002230336990225552), (':|', 0.0026647568979900208), ('B2', 0.002756048038027482), ('<li><a', 0.0032424094565029196), ('\\\\', 0.003269167204444934), ('||', 0.0038452457777847734), ('A2', 0.00433632915177939), ('G2', 0.004799080792658933), ('d2', 0.004821116585081769), ('|', 0.05647616199455401)]\n",
            "happy_topical_chat\n",
            "[('are', 0.0031255874477810665), ('would', 0.0031401975480355748), ('on', 0.0033194147778242054), ('they', 0.003815184179793842), ('with', 0.003936935015248074), ('like', 0.004409328256810498), ('for', 0.004594389526700932), ('have', 0.005052172668008848), ('it', 0.00571839323961441), ('was', 0.0058469621218540805), ('you', 0.0065901292214667175), ('in', 0.007271933900010422), ('and', 0.0076001741523950336), ('is', 0.007859259930241641), ('that', 0.00874658001903209), ('of', 0.009587147787008114), ('to', 0.012952340878963112), ('a', 0.014612048267875214), ('the', 0.019592144441295156), ('I', 0.02180508762651129)]\n",
            "trumpSpeech\n",
            "[('was', 0.003436028071000991), ('going', 0.0035165670486989337), ('our', 0.003734655741116958), ('We', 0.004065860975245576), ('for', 0.004473080525403713), ('is', 0.004559954029437449), ('they', 0.004587101999447991), ('it', 0.004688454420820683), ('And', 0.005036853369289312), ('we', 0.005945405432308799), ('have', 0.0059680287406509176), ('that', 0.00635533977946799), ('in', 0.006989697345380999), ('you', 0.008020415273447929), ('I', 0.011064607643963423), ('of', 0.011084516155304487), ('a', 0.012449154114501088), ('and', 0.014300645669220085), ('to', 0.015808262937138877), ('the', 0.021587160820049682)]\n",
            "wallstreetbets_comments\n",
            "[('at', 0.005094461562714739), ('my', 0.005126186963346042), ('have', 0.005379990168396464), ('are', 0.005389311548994321), ('be', 0.005512206593192383), ('[removed]', 0.006520714560683333), ('this', 0.007401912093166709), ('that', 0.007692755521119142), ('on', 0.008479758049666278), ('it', 0.008661443204828017), ('for', 0.00900110123117449), ('in', 0.01029333894142477), ('you', 0.011021960191490593), ('of', 0.011622698641073535), ('is', 0.012648786405306056), ('and', 0.01600260279111571), ('I', 0.017022639834609273), ('a', 0.019218560863872847), ('to', 0.02194784474962608), ('the', 0.028766679956447893)]\n",
            "javascript\n",
            "[('&&', 2.0136795967270995e-05), ('||', 2.2374217741412214e-05), (\"'\", 2.2374217741412214e-05), ('break;', 2.4611639515553437e-05), ('case', 3.1323904837977106e-05), ('new', 3.3561326612118325e-05), ('function()', 3.3561326612118325e-05), ('},', 4.027359193454199e-05), (':', 5.369812257938932e-05), ('true,', 6.0410387901812985e-05), ('function', 6.488523145009543e-05), ('+', 6.712265322423665e-05), ('if', 7.383491854666031e-05), ('};', 7.383491854666031e-05), ('});', 7.607234032080153e-05), ('return', 8.27846056432252e-05), ('var', 0.0001141085104812023), ('}', 0.00015885694596402673), ('=', 0.0002975770959607825), ('{', 0.00030652678305734737)]\n",
            "shakespeare\n",
            "[('have', 9.724573555348294e-05), ('he', 0.00010387612661394769), ('it', 0.00010608625696743594), ('me', 0.00011050651767441244), ('his', 0.00011934703908836543), ('And', 0.00012597743014883017), ('with', 0.00012818756050231843), ('your', 0.00013039769085580668), ('is', 0.00013481795156278318), ('in', 0.0001502888640372009), ('not', 0.0001502888640372009), ('that', 0.00016575977651161865), ('you', 0.00018344081933952466), ('my', 0.00021880290499533663), ('a', 0.0002541649906511486), ('of', 0.00030499798878137833), ('to', 0.0003116283798418431), ('and', 0.00036467150832556107), ('I', 0.0003889829422139318), ('the', 0.0004928590688278795)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f1df847"
      },
      "source": [
        "Now we can make predictions for single words! Lets write a function which takes a word and returns the most likely source."
      ],
      "id": "4f1df847"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d42a7b2f"
      },
      "source": [
        "def get_word_probabilities(model, word):\n",
        "    \"\"\" Returns a dict giving\n",
        "    the ID of a source and the probability of word\n",
        "    coming from that source.\n",
        "    \"\"\"\n",
        "\n",
        "    # We want to fill source_probs\n",
        "    # with the likelihood of a word coming from each source.\n",
        "    # E.G [('Twitter', 0.9), ('Wikipedia', 0.5)]\n",
        "    source_probs = {}\n",
        "    for source in model:\n",
        "      #************* YOUR CODE HERE ******************\n",
        "      #Add code which retrieves the probability of 'word'\n",
        "      # from source 'source'. (Remember 'model' here is storing our probabilities)\n",
        "      #Then add this to the list to return!\n",
        "    return source_probs\n",
        "\n",
        "def predict_single_word(model, word):\n",
        "    \"\"\" Return the source ID word is most likely\n",
        "    to have come from.\n",
        "    \"\"\"\n",
        "    source_probs = get_word_probabilities(model, word)\n",
        "\n",
        "    #************* YOUR CODE HERE **************\n",
        "    #Given a list of tuples as above, find the source which\n",
        "    #is most likely\n",
        "    most_likely_source = ...\n",
        "    #**********************************\n",
        "    \n",
        "    return most_likely_source\n",
        "\n",
        "        "
      ],
      "id": "d42a7b2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AodI6WR2aaWT"
      },
      "source": [
        "# Lets Test it out! How good are our predictions?\n",
        "\n",
        "Can you beat the computer?"
      ],
      "id": "AodI6WR2aaWT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "222ed67e"
      },
      "source": [
        "# Run this cell to initialize scores\n",
        "word_pred_games_played = 0\n",
        "word_pred_human_correct = 0\n",
        "word_pred_ML_correct = 0"
      ],
      "id": "222ed67e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d853ea6",
        "outputId": "e68fafe3-9b8e-4fc5-e716-7c1c02ead648"
      },
      "source": [
        "# Run this cell to play the game!\n",
        "s, label_opt1 = random.choice(data_files)\n",
        "s, label_opt2 = random.choice(data_files)\n",
        "source_gen, true_label = random.choice(data_files)\n",
        "label_options = [label_opt1, label_opt2, true_label]\n",
        "random.shuffle(label_options)\n",
        "lines = source_gen()\n",
        "line = random.choice(lines)\n",
        "word = random.choice(line.split())\n",
        "print(\"Word is\", word)\n",
        "predicted_label = input(\"Where did this word come from? {} or {} or {}\".format(*label_options))\n",
        "ML_predicted_label_ID = predict_single_word(NB_model, word)\n",
        "ML_predicted_label = ID_to_source[ML_predicted_label_ID]\n",
        "print(\"You were\", \"Right\" if true_label == predicted_label else \"Wrong\", \"!\")\n",
        "print(\"The computer was\", \"Right\" if true_label == ML_predicted_label else \"Wrong\", \"!\")\n",
        "print(\"Your predicted label (y^) was: {}. The computer predicted {}. The true label (y) was {}\".format(predicted_label, ML_predicted_label, true_label))\n",
        "word_pred_games_played += 1\n",
        "word_pred_human_correct += (true_label == predicted_label)\n",
        "word_pred_ML_correct += (true_label == ML_predicted_label)\n",
        "print(\"Your score:\", str(word_pred_human_correct / word_pred_games_played), \"AI score:\", str(word_pred_ML_correct / word_pred_games_played))"
      ],
      "id": "7d853ea6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word is h4\n",
            "Where did this word come from? happy_topical_chat or shakespeare or chesschess\n",
            "You were Right !\n",
            "The computer was Right !\n",
            "Your predicted label (y^) was: chess. The computer predicted chess. The true label (y) was chess\n",
            "Your score: 0.8 AI score: 0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afecc51b"
      },
      "source": [
        "## Predicting Sentences"
      ],
      "id": "afecc51b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7da12bb0"
      },
      "source": [
        "Now that we can classify the source of individual words, how can we upgrade to sentences?"
      ],
      "id": "7da12bb0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28978d36"
      },
      "source": [
        "Lets think about probability a little bit. What is the probability of a heads when a coin is flipped? \n",
        "$$p(H) = \\frac{1}{2}$$\n",
        "Now what is the probability of flipping two heads in a row?\n",
        "$$ p(H) \\times p(H) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$$\n",
        "\n",
        "We can check this is correct, as there are four possible options, HH, HT, TH, TT, and all are equally likely.\n",
        "\n",
        "Can we do something similar with a sentence? We have the probability of each word coming from each source, can we just multiply them all together to get the probability of the sentence coming from a source?\n",
        "\n"
      ],
      "id": "28978d36"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ae70aa6"
      },
      "source": [
        "### Yes! (Kind of)\n",
        "\n",
        "It turns out that yes, we can! If we make some assumptions...\n",
        "\n",
        "If we assume that all words are independant from each other, this works! This is actually exactly how Naive Bayes works!\n",
        "(Though this may not be so realistic...)"
      ],
      "id": "8ae70aa6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7cd6de4"
      },
      "source": [
        "So, lets put all of our intuition together!\n",
        "1. Firstly, we want to predict the probability of a label (wikipedia, twitter, etc,) given some sentence (\"Follow me please!!!\", \"Auckland was founded...\").\n",
        "    1. In statistics, this can be written p(y=Y|x=X)\n",
        "2. This is *hard* to calculate, so we use bayes rule to flip it around. We predict the probability of the sentence we have coming from each of the sources!\n",
        "    1. This is now: p(x=X|y=Y)\n",
        "3. The probability of the sentence coming from a source can be thought of as the product of each *word* coming from that source!\n",
        "    1. p(Follow me please|Twitter) = p(Follow|Twitter) x p(me|Twitter) x p(please|Twitter)\n",
        "4. Once probabilities are calculated, we can take the most likely label as our prediction!\n",
        "    1. If p(Follow me please|Twitter) = 0.2 and p(Follow me please|Wikipedia) = 0.001, we can predict the sentence \"Follow me please\" is from Twitter!"
      ],
      "id": "a7cd6de4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ae0eb0f"
      },
      "source": [
        "import math\n",
        "def get_sentence_probs(model, sentence):\n",
        "    \"\"\" Returns a list of tuples giving\n",
        "    the ID of a source and the probability of sentence\n",
        "    coming from that source.\n",
        "    \"\"\"\n",
        "    source_probs = {}\n",
        "\n",
        "    # Initialize the probabilities to be 1\n",
        "    for source in model:\n",
        "      sentence_prob = 1\n",
        "      for word in sentence.split():\n",
        "        #*********** YOUR CODE HERE ***************\n",
        "        # Add code which incorporates word probabilities\n",
        "        # into the probability of each source.\n",
        "        # 1) get the likelihood of the word given each source\n",
        "        # 2) Multiply with the current probability estimate to update\n",
        "        # 3) Store final result in dict\n",
        "    return source_probs\n",
        "\n",
        "def predict_sentence(model, sentence):\n",
        "    source_probs = get_sentence_probs(model, sentence)\n",
        "\n",
        "    # Gets dict as a list, then sorts accoring to probabilities\n",
        "    # Selects the last element (the max) and returns the source\n",
        "    return sorted(source_probs.items(), key= lambda x: x[1])[-1][0]"
      ],
      "id": "0ae0eb0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t61a_kBrbmRK"
      },
      "source": [
        "## Game 2: Full Sentences!"
      ],
      "id": "t61a_kBrbmRK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92187b56"
      },
      "source": [
        "sent_pred_games_played = 0\n",
        "sent_pred_human_correct = 0\n",
        "sent_pred_ML_correct = 0"
      ],
      "id": "92187b56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b3c5e46",
        "outputId": "14ac8789-2b0c-461a-967b-f00aa334b1b2"
      },
      "source": [
        "import random\n",
        "s, label_opt1 = random.choice(data_files)\n",
        "s, label_opt2 = random.choice(data_files)\n",
        "source_gen, true_label = random.choice(data_files)\n",
        "label_options = [label_opt1, label_opt2, true_label]\n",
        "random.shuffle(label_options)\n",
        "lines = source_gen()\n",
        "line = random.choice(lines)\n",
        "print(\"Line is:\", line)\n",
        "predicted_label = input(\"Where did this word come from? {} or {} or {}\".format(*label_options))\n",
        "ML_predicted_label_ID = predict_sentence(NB_model, line)\n",
        "ML_predicted_label = ID_to_source[ML_predicted_label_ID]\n",
        "print(\"You were\", \"Right\" if true_label == predicted_label else \"Wrong\", \"!\")\n",
        "print(\"The computer was\", \"Right\" if true_label == ML_predicted_label else \"Wrong\", \"!\")\n",
        "print(\"Your predicted label (y^) was: {}. The computer predicted {}. The true label (y) was {}\".format(predicted_label, ML_predicted_label, true_label))\n",
        "sent_pred_games_played += 1\n",
        "sent_pred_human_correct += (true_label == predicted_label)\n",
        "sent_pred_ML_correct += (true_label == ML_predicted_label)\n",
        "print(\"Your score:\", str(sent_pred_human_correct / sent_pred_games_played), \"AI score:\", str(sent_pred_ML_correct / sent_pred_games_played))"
      ],
      "id": "1b3c5e46",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Line is: 1. e4 d6 2. d4 Nf6 3. Nc3 g6 4. Nf3 Bg7 5. Be2 O-O 6. O-O Bg4 7. h3 Bxf3 8. Bxf3 e5 9. d5 c5 10. Be3 Na6 11. Qd2 Qd7 12. Ne2 h5 13. Kh1 Qb5 14. Rab1 Qa4 15. a3 Nxe4 16. Bxe4 Qxe4 17. f3 Qa4 18. c3 f5 19. Ng3 f4 20. Ne4 fxe3 21. Qxe3 Nc7 22. Qg5 Rad8 23. Qxg6 Qe8 24. Qg3 Nxd5 25. Rbe1 Qb5 26. Rf2 Nf4 27. Qh4 Nd3 28. Ref1 d5 29. Ng3 Nxf2+ 30. Rxf2 e4 31. Nxh5 e3 32. Rc2 Qf1+ 33. Kh2 e2 34. Qg3 Rf7 {White resigns} 0-1\n",
            "Where did this word come from? music or chess or shakespearechess\n",
            "You were Right !\n",
            "The computer was Right !\n",
            "Your predicted label (y^) was: chess. The computer predicted chess. The true label (y) was chess\n",
            "Your score: 1.0 AI score: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34d070e1"
      },
      "source": [
        "## Evaluating\n",
        "\n",
        "So, after playing the ML algorithm, it seems pretty good!\n",
        "The next step is to test *how good*. Evaluation is an important part of machine learning, to test how good our models actually are. This is required for two things:\n",
        "1. Making sure we can actually put it into critical decision making roles\n",
        "2. Making sure we understand *what* its decisions are based on\n",
        "3. Making sure any changes we make actually are improving it!\n",
        "\n",
        "Lets look at the most basic form of evaluation, how well the predicted labels match the known labels. This is called *accuracy*!"
      ],
      "id": "34d070e1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8562c76",
        "outputId": "73dcf008-02eb-45ec-e305-99b2b4618a13"
      },
      "source": [
        "num_right = 0\n",
        "num_wrong = 0\n",
        "for x, y in observations:\n",
        "    ML_predicted_label_ID = predict_sentence(NB_model, x)\n",
        "    ML_correct = ML_predicted_label_ID == y\n",
        "    #********** YOUR CODE HERE **************\n",
        "    # Add code to update num_right and num_wrong\n",
        "# Add code to calculate accuracy.\n",
        "# Accuracy is the number of correct predictions divided by the total number of predictions\n",
        "print(accuracy)"
      ],
      "id": "b8562c76",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8994011331037898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d33dfd83"
      },
      "source": [
        "Lets compare to a library implementation!\n",
        "Scikit-Learn is a very popular, simple to use ML library in python. Lets compare to their built in naive bayes model."
      ],
      "id": "d33dfd83"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "658ebd01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "2e4b6298-1152-4f50-8401-2076daaacb9b"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# SKlearn doesn't work directly on words like our implementation, but needs them to be\n",
        "# vectorized first. This lets it be more general, and work on more than just text!\n",
        "cv = CountVectorizer()\n",
        "X_cv = cv.fit_transform(X)\n",
        "\n",
        "# We are going to shuffle the sentences, and then take 90% to train on\n",
        "# This will be closer to real training you'll see in examples\n",
        "X_cv, X_shuffle, Y_shuffle = shuffle(X_cv, X, Y) \n",
        "train_len = int(len(X) * 0.9)\n",
        "X_cv_train = X_cv[:train_len]\n",
        "X_cv_test = X_cv[train_len:]\n",
        "Y_train = Y_shuffle[:train_len]\n",
        "Y_test = Y_shuffle[train_len:]\n",
        "X_sentence_train = X_shuffle[:train_len]\n",
        "X_sentence_test = X_shuffle[train_len:]\n",
        "\n",
        "# Fit both models to the selected data\n",
        "sklearn_model = MultinomialNB().fit(X_cv_train, Y_train)\n",
        "our_model = calulate_conditionals(X_sentence_train, Y_train, [w for w in word_counts])"
      ],
      "id": "658ebd01",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a56241ac8dc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# vectorized first. This lets it be more general, and work on more than just text!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# We are going to shuffle the sentences, and then take 90% to train on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f0037aa",
        "outputId": "d10501d7-09ac-4987-b388-d08f52fbc282"
      },
      "source": [
        "our_right = 0\n",
        "our_wrong = 0\n",
        "sk_right = 0\n",
        "sk_wrong = 0\n",
        "num_to_test = 1000\n",
        "wrong_examples = []\n",
        "\n",
        "# For each example, we predict with both models and increment right if they\n",
        "# get it right, otherwise wrong.\n",
        "for x_cv, x, y in zip(X_cv_train[:num_to_test], X_sentence_train[:num_to_test], Y_train[:num_to_test]):\n",
        "    if len(x.split()) < 1:\n",
        "        continue\n",
        "    ML_predicted_label_ID = predict_sentence(our_model, x)\n",
        "    sk_predicted_label_ID = sklearn_model.predict(x_cv)[0]\n",
        "    our_right += (ML_predicted_label_ID == y)\n",
        "    our_wrong += (ML_predicted_label_ID != y)\n",
        "    sk_right += (sk_predicted_label_ID == y)\n",
        "    sk_wrong += (sk_predicted_label_ID != y)\n",
        "    if ML_predicted_label_ID != y:\n",
        "        wrong_examples.append((x, ID_to_source[y], ID_to_source[ML_predicted_label_ID]))\n",
        "our_accuracy = our_right / (our_right + our_wrong)\n",
        "sk_accuracy = sk_right / (sk_right + sk_wrong)\n",
        "print(\"Our accuracy:\", our_accuracy, \"SK Learn accuracy:\", sk_accuracy)"
      ],
      "id": "4f0037aa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our accuracy: 0.908 SK Learn accuracy: 0.979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1781033b",
        "outputId": "7fd2acb9-e334-4799-fea4-822548345bfa"
      },
      "source": [
        "# Show some examples of ones we got wrong\n",
        "# Sentence, True label, predicted label\n",
        "wrong_examples[:5]"
      ],
      "id": "1781033b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"72,800 a year lol. Maybe he meant 1/2 a month's...?\",\n",
              "  'wallstreetbets_comments',\n",
              "  'happy_topical_chat'),\n",
              " ('$TSLA Go2DaMoon', 'wallstreetbets_comments', 'javascript'),\n",
              " ('&gt;missed most of the party ', 'wallstreetbets_comments', 'trumpSpeech'),\n",
              " ('$53,566.04. ', 'wallstreetbets_comments', 'javascript'),\n",
              " ('DEALER WINS', 'wallstreetbets_comments', 'javascript')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35a2bda4"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "We've done it! We have build a ML model from scratch to classify where a sentence came from, with over 90% accuracy!\n",
        "\n",
        "As we saw with the Scikit-Learn comparison, our model was pretty simple. There are many many more complex methods out there, its an exciting world!\n",
        "\n",
        "Next, we will look at how we can use very similar ideas to generate *new* text!"
      ],
      "id": "35a2bda4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88f4b104"
      },
      "source": [
        "# Text Generation\n",
        "\n",
        "So far, we have looked at *discriminative* machine learning. This means our model learns patterns which allow it to *discriminate* (or, tell the difference between) different classes. \n",
        "\n",
        "Now, we will look at *generative* machine learning! Our model will learn patterns which allow it to generate *new* data!\n",
        "\n",
        "### Basic Naive Bayes Model\n",
        "\n",
        "Can we *generate* text with our NB model? Lets try just building a sentence by taking the most likely word."
      ],
      "id": "88f4b104"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac5b9a6b",
        "outputId": "5ab23dfa-a89f-4cc9-d172-21cefb1e5c19"
      },
      "source": [
        "def NB_gen_sentence_1(model, source, num_words):\n",
        "    \"\"\" Only predict most likely word\n",
        "    \"\"\"\n",
        "    # Get a list of (word, likelihood) pairs for the source\n",
        "    word_likelihoods = model[source].items()\n",
        "\n",
        "    # sort the list, so the most likely words are at the start of the list (this is reverse of normal sorting)\n",
        "    # The key tells sorted to look at the second element for sorting, the likelihood\n",
        "    sorted_likelihoods = sorted(word_likelihoods, reverse=True, key=lambda x: x[1])\n",
        "\n",
        "    sentence_list = []\n",
        "    for i in range(num_words):\n",
        "      #************** YOUR CODE HERE ****************\n",
        "      # Add code to select the next word in the sentence. You should select the most likely word given the class.\n",
        "      # Add this word to sentence_list\n",
        "    return ' '.join(sentence_list)\n",
        "    \n",
        "text_source = \"shakespeare\"\n",
        "print(NB_gen_sentence_1(our_model, source_IDs[text_source], 10))"
      ],
      "id": "ac5b9a6b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the the the the the the the the the the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fed4d59c"
      },
      "source": [
        "We seem to have an issue, if we just pick the most likely word, we get the same thing every time!\n",
        "\n",
        "Lets try sampling different words based on the probability, rather than just taking the top one."
      ],
      "id": "fed4d59c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d4f7294",
        "outputId": "7be26f30-5fee-468d-f238-b9513e10a41e"
      },
      "source": [
        "def NB_gen_sentence_2(model, source, num_words, num_top_word):\n",
        "    \"\"\"Predict a random likely word\n",
        "    \"\"\"\n",
        "    # Get a list of (word, likelihood) pairs for the source\n",
        "    word_likelihoods = model[source].items()\n",
        "\n",
        "    # sort the list, so the most likely words are at the start of the list (this is reverse of normal sorting)\n",
        "    # The key tells sorted to look at the second element for sorting, the likelihood\n",
        "    sorted_likelihoods = sorted(word_likelihoods, reverse=True, key=lambda x: x[1])\n",
        "\n",
        "    sentence_list = []\n",
        "    for i in range(num_words):\n",
        "      #************** YOUR CODE HERE ****************\n",
        "      # Add code to select the next word in the sentence.\n",
        "      # We should sample from the \n",
        "      # Add this word to sentence_list\n",
        "    return ' '.join(sentence_list)\n",
        "    \n",
        "text_source = \"shakespeare\"\n",
        "print(NB_gen_sentence_2(our_model, source_IDs[text_source], 10, 1000))"
      ],
      "id": "1d4f7294",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And that bad best been the they China a will\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2223896"
      },
      "source": [
        "This seems to work! But has a major issue.\n",
        "\n",
        "Our Naive Bayes model only captures patterns to do with what words are in a sentence, but not their order. This is enough for a *discriminative* model, but not a *generative* model. We really want patterns which capture relationships between words, and the position of words in a sentence too. (These might also be helpful for disciminative models too!)\n",
        "\n",
        "Lets think about how we might capture these patterns in our data.\n",
        "\n",
        "### N-Grams - From Naive Bayes to Markov Chains\n",
        "\n",
        "Up until now, we have only looked at words individually. This is called a *bag-of-words* model. But really, a sentence is a *sequence* of words. The next word in a sentence depends on the rest of the words, not only on the source of the text. \n",
        "\n",
        "N-grams model how the next word in a sentence depends on *previous* words. The *N* refers to how far back we look. Lets start with bi-grams (bi = 2, tri = 3, etc). This means that we need to capture the probability of a word, given the source, given the previous word. Our probability formula for word $w_i$ looks like:\n",
        "$$ p(w_i | w_{i-1}, source)$$.\n",
        "\n",
        "For example, before our probability of the words 'and' and 'the' were pretty high in wikipedia. $p(and|Wikipedia) = high$, $p(the|Wikipedia) = high$. This means that in our generated sentences, the sequence 'the and' or 'and the' are pretty common! But we know that while 'and the' is actually pretty common in real sentences, 'the and' doesn't make much sence. By including the previous word in our probabilities, we can account for this! $$p(the | and, Wikipedia) = High, p(and | the, Wikipedia) = Low$$"
      ],
      "id": "a2223896"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b25ab1b2"
      },
      "source": [
        "So how do we learn this new bi-gram model? Well, we only need to extend our code a little. Rather than counting how many times 'the' appears in Wikipedia as we did to calculate $p(the|Wikipedia)$, we need to include the previous word too. So we need to count, for example, how many times 'the' appeared after 'and' in wikipedia for $p(the|and,Wikipedia)$. We need to count this for every possible word it could appear after, how many times it appeared after $w_{i-1}$, $p(the|w_{i-1},Wikipedia)$\n",
        "\n",
        "Lets update our NB model with bi-grams! This model for generating text is called a *markov chain*. The *markov* in the name refers to the *markov* property, which essentially means future actions are based on the current state. Using bigrams, the next word is based on the current word. \n",
        "\n",
        "Lets start with extracting the ngrams from a sentence:"
      ],
      "id": "b25ab1b2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "IEiw9XIdQ0-9",
        "outputId": "c1117457-ac71-42fd-e8bc-7c97f3b16ae2"
      },
      "source": [
        "def extract_ngrams(sentence, N):\n",
        "  \"\"\" Sentence is the input string\n",
        "  N is the number of words to consider at one.\n",
        "  So if N = 2, we consider the current word and 1 previous word.\n",
        "  \"\"\"\n",
        "  ngrams = []\n",
        "  sentence_words = sentence.split()\n",
        "  current_ngram = ['' for i in range(N)]\n",
        "  for word in sentence_words:\n",
        "    # Remove the word at the front of the ngram,\n",
        "    # and add the current word to the end.\n",
        "    current_ngram = (*current_ngram[1:], word)\n",
        "    previous_words = current_ngram[:-1]\n",
        "    ngrams.append((previous_words, word))\n",
        "  return ngrams\n",
        "\n",
        "bigrams = extract_ngrams(\"Hello, this is a sentence about bigrams\", 2)\n",
        "trigrams = extract_ngrams(\"Hello, this is a sentence about trigrams\", 3)\n",
        "print(bigrams[:10])\n",
        "print(trigrams[:10])"
      ],
      "id": "IEiw9XIdQ0-9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5f5b012fd35f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mbigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7NtNw7p2hdY"
      },
      "source": [
        "Previously, we stored the count of each word for each source label. Now, we need to store the count of each word for each source label AND set of previous words.\n",
        "\n",
        "We can consider this as a new level in the probability dictionary. First, we look up the source label to get a map of (previous words) -> following word. Then we look up the set of previous words in our sentence, to find possible following words."
      ],
      "id": "Y7NtNw7p2hdY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvxxLvkR2kce"
      },
      "source": [
        "  def get_ngram_counts(sentences, N):\n",
        "    # fill ngram_counts so it contains the number of times each word occurs for each set of previous words.\n",
        "    # word_counts = {\n",
        "    #       ('and',): {\n",
        "    #           'the': 15,\n",
        "    #           'then': 7,\n",
        "    #        },\n",
        "    #       ('founded',): {\n",
        "    #           'Auckland', \n",
        "    #       },\n",
        "    #       ....\n",
        "    #}\n",
        "    ngram_counts = {}\n",
        "\n",
        "    # We now need to store a total for each set of previous words.\n",
        "    total_num_ngrams = {}\n",
        "\n",
        "    # We want to look though all the training sentences,\n",
        "    for sentence in sentences:\n",
        "      ngrams = extract_ngrams(sentence, N)\n",
        "      for previous_words, word in ngrams:\n",
        "        \n",
        "        #********* YOUR CODE HERE ***************\n",
        "        # Initialize the counts for the set of previous words if we haven't seen it yet\n",
        "\n",
        "        # Increment the total for the set of previous words\n",
        "\n",
        "        # Add word to ngram_counts if not already in it\n",
        "      \n",
        "        # Add to the count for the word\n",
        "        #******************************\n",
        "    return ngram_counts, total_num_ngrams\n",
        "  \n",
        "  # Inspect our ngram counts!\n",
        "  test_sentences = [\"Hello, this is a sentence about bigrams\", \"Hello, this is a sentence about trigrams\"]\n",
        "  test_counts, test_totals = get_ngram_counts(test_sentences, 2)\n",
        "  print(test_counts)\n",
        "  for previous_word in test_counts:\n",
        "    print(f\"If the previous word is {previous_word}, the possible next words were: {test_counts[previous_word]}\")"
      ],
      "id": "tvxxLvkR2kce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKWiiq0728Vp"
      },
      "source": [
        "Now, as before, we need to turn our counts into probabilities"
      ],
      "id": "LKWiiq0728Vp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p93-5CM23AzJ"
      },
      "source": [
        "def get_probabilities_from_ngrams(ngram_counts, total_num_ngrams):\n",
        "  ngram_probabilities = {}\n",
        "\n",
        "  for previous_words in ngram_counts:\n",
        "    ngram_probabilities[previous_words] = {}\n",
        "\n",
        "    # Get the total of times this set of previous words was seen\n",
        "    ngram_total = total_num_ngrams[previous_words]\n",
        "\n",
        "    for following_word in ngram_counts[previous_words]:\n",
        "\n",
        "      # Get the number of times the word appeard after the set of previous words\n",
        "      w_count = ngram_counts[previous_words][following_word]\n",
        "\n",
        "      # Calc probability, and save to the dictionary\n",
        "      ngram_probabilities[previous_words][following_word] = w_count / ngram_total\n",
        "  return ngram_probabilities\n",
        "\n",
        "# Inspect our bigrams!\n",
        "bigram_counts, total_bigram_words = get_ngram_counts(X, 2)\n",
        "word_probabilities = get_probabilities_from_ngrams(bigram_counts, total_bigram_words)\n",
        "for previous_word in list(word_probabilities.keys())[:5]:\n",
        "  print(f\"If the previous word is {previous_word}, the possible next words were: {word_probabilities[previous_word]}\")"
      ],
      "id": "p93-5CM23AzJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-CCu92N3KYt"
      },
      "source": [
        "Finally, we build our model in the same way as before. We save a set of probabilities per source label."
      ],
      "id": "P-CCu92N3KYt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3Q_ePHf3Qbh"
      },
      "source": [
        "  def calulate_conditionals_Ngram(X, Y, N):\n",
        "    # fill word_probs_by_source so it contains a dict for each source\n",
        "    # each containing each word and probability only in that source\n",
        "    # word_probs_by_source = {\n",
        "    #       'Twitter': {\n",
        "    #              'founded': 0.001,\n",
        "    #               ...,\n",
        "    #        },\n",
        "    #       'Wikipedia': {\n",
        "    #              'founded': 0.05,\n",
        "    #              ...,\n",
        "    #       }\n",
        "    #}\n",
        "    \n",
        "    # Get a list of the possible sources\n",
        "    # (Note: this is an ID not a name)\n",
        "    sources = set(Y)\n",
        "    ngram_probs_by_source = {}\n",
        "\n",
        "    for s in sources:\n",
        "      # Selects all the training examples which come from this source\n",
        "      source_training_data = [x for x,y in zip(X, Y) if y == s]\n",
        "\n",
        "      # Extract the counts and total for the specific source\n",
        "      source_counts, source_total = get_ngram_counts(source_training_data, N)\n",
        "\n",
        "      # Transform into probabilities\n",
        "      source_probabilities = get_probabilities_from_ngrams(source_counts, source_total)\n",
        "\n",
        "      #Save to the dictionary\n",
        "      word_probs_by_source[s] = source_probabilities\n",
        "    return word_probs_by_source\n",
        "\n",
        "bigram_model = calulate_conditionals_Ngram(X, Y, 2)"
      ],
      "id": "H3Q_ePHf3Qbh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20858d8d"
      },
      "source": [
        "Lets test our bigrams! The following cell lets you pick a word and prints the 5 most likely words to follow it."
      ],
      "id": "20858d8d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYikrY9D3Thh"
      },
      "source": [
        "source_label = \"shakespeare\"\n",
        "bigram = bigram_model[source_IDs[source_label]]\n",
        "first_word = input(\"Select first word\")\n",
        "sorted_bigrams = sorted(bigram[(first_word,)].items(), key=lambda x: x[1])\n",
        "print(\"Most likely next words:\")\n",
        "print(sorted_bigrams[-5:])"
      ],
      "id": "GYikrY9D3Thh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f72f6cf8"
      },
      "source": [
        "Now lets use bigrams to generate our text! Instead of sampling from all words from a source, we take into account the previously generated word!"
      ],
      "id": "f72f6cf8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "089ba808",
        "outputId": "1bf3ec51-36e7-4081-fb10-7986f8075e4e"
      },
      "source": [
        "def NB_gen_sentence_3(model, source, num_words, num_top_word, N):\n",
        "    \"\"\"Predict a random likely word\n",
        "    \"\"\"\n",
        "    # Select the ngrams for this source\n",
        "    ngrams_for_source = model[source]\n",
        "\n",
        "    # Setup the Ngram for the current sentence we are generating\n",
        "    # Remember for an Ngram of N words, we have N-1 previous words\n",
        "    # and the word we are generating is the remaining one\n",
        "    current_ngram = [''] * N\n",
        "\n",
        "    sentence_list = []\n",
        "    for i in range(num_words):\n",
        "      # The previous words are the previous ngram, without the first word (now too old)\n",
        "      previous_words = current_ngram[1:]\n",
        "\n",
        "\n",
        "      #*********** YOUR CODE HERE *******************\n",
        "      # Get a list of (word, likelihood) pairs for the current set of previous words\n",
        "\n",
        "      # sort the list, so the most likely words are at the start of the list (this is reverse of normal sorting)\n",
        "      # The key tells sorted to look at the second element for sorting, the likelihood\n",
        "\n",
        "      # Select the top words\n",
        "\n",
        "      # Choose a random word from the top words\n",
        "      selected_word = ...\n",
        "\n",
        "      # Update the current ngram\n",
        "\n",
        "      # ****************************\n",
        "\n",
        "      \n",
        "      sentence_list.append(selected_word)\n",
        "    return ' '.join(sentence_list)\n",
        "    \n",
        "text_source = \"shakespeare\"\n",
        "print(NB_gen_sentence_3(bigram_model, source_IDs[text_source], 10, 1000, 2))"
      ],
      "id": "089ba808",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "They have a positive, positive thing like people, despite all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUV0cUiX3vOe"
      },
      "source": [
        "We run into an issue again, where we may not have seen a particular ngram before. How can we predict the next word?\n",
        "\n",
        "We could just restart the current ngram, like we are starting a new sentence. Update your code above to perform this refresh!"
      ],
      "id": "JUV0cUiX3vOe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac9e8b95"
      },
      "source": [
        "We get more coherent sentences! But we can see that the patterns are not very long, focus tends to switch quickly. This is because our patterns only have a \"memory\" of one word, we can only make decisions one word back. We can increase this by using longer N-grams, lets try a tri-gram!"
      ],
      "id": "ac9e8b95"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaQVCejs4BHJ"
      },
      "source": [
        "trigram_model = calulate_conditionals_Ngram(X, Y, 3)"
      ],
      "id": "VaQVCejs4BHJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv0DYkaX4CdP"
      },
      "source": [
        "text_source = \"happy\"\n",
        "print(NB_gen_sentence_3(trigram_model, source_IDs[text_source], 50, 10, 3))"
      ],
      "id": "Rv0DYkaX4CdP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b05fdc12"
      },
      "source": [
        "We now see much more coherent sentences, even more so than the bigram model! Can we increase the N-gram length even further?\n",
        "\n",
        "Give it a try! However, we start to run into an issue. The more we increase the length, the less data we have for each N-gram. For example, if we looked for patterns 10 words long, it is likely we only would get the exact sentences that appear in the training data. In other words, our patterns are *too* detailed, so have fit too closely to the training data. \n",
        "\n",
        "This is very very important problem in machine learning! We are interested in patterns which *generalize* to new data. If we look too deeply into our data, we can always find more and more complex patterns, but are they actually useful?"
      ],
      "id": "b05fdc12"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fdd23bd"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this talk, we have focused on simple models for one small area machine learning can be applied to. \n",
        "\n",
        "I encorage you to take what you have learned, and think up some more areas where classification and generation can be applied!"
      ],
      "id": "3fdd23bd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4651f81"
      },
      "source": [
        ""
      ],
      "id": "f4651f81",
      "execution_count": null,
      "outputs": []
    }
  ]
}