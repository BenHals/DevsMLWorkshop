{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfcf9282",
   "metadata": {},
   "source": [
    "# Machine Learning for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4adde",
   "metadata": {},
   "source": [
    "Lets set the stage! After a year full of COVID disruptions the university has made all exams online. Celebrations all around! But wait, what about the cheating you say? Fortunately (or unfortunately), the uni has predicted this, and hired a crack team of Designated Exam Validators (or DEVs for short) to check for plagarism. And you've just been hired! And of course, you're going to use ML to do all your work for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4716d71b",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc662ca9",
   "metadata": {},
   "source": [
    "Given a sentence, we need to find the probability a sentence came from each possible source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccec8e7",
   "metadata": {},
   "source": [
    "For example, given the text \"The University of Auckland began as a constituent college of the University of New Zealand, founded on 23 May 1883 as Auckland University College\" the probability for \"Wikipedia\" = 0.9 and \"Twitter\" = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f859c9ca",
   "metadata": {},
   "source": [
    "We need to build a machine learning classifier to do this for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1877e",
   "metadata": {},
   "source": [
    "# 1) Collect data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e1e0c",
   "metadata": {},
   "source": [
    "First, we need some data from each source. This will allow us to find patterns, i.e., learn what makes *wikipedia* sentences different to *twitter* sentences\n",
    "\n",
    "This is normally the *most important part of ML*, but we've taken care of it! In the github repo there is a data folder containing some datasets from different sources.\n",
    "\n",
    "First we will just load this in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5fa9ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in some libraries\n",
    "# The pathlib library makes handling filepaths easier, letting us open data files.\n",
    "import pathlib\n",
    "\n",
    "# Set the folder containing our data\n",
    "data_path = pathlib.Path('data')\n",
    "\n",
    "# Setup availiable filenames\n",
    "chess_filename = \"chess.txt\"\n",
    "music_filename = \"music.txt\"\n",
    "angry_filename = \"angry_topical_chat.txt\"\n",
    "happy_filename = \"happy_topical_chat.txt\"\n",
    "disgusted_filename = \"disgusted_topical_chat.txt\"\n",
    "trumpspeech_filename = \"trumpSpeech.txt\"\n",
    "wallstreetbets_filename = \"wallstreetbets_comments.txt\"\n",
    "javascript_filename = \"javascript.txt\"\n",
    "shakespeare_filename = \"shakespeare.txt\"\n",
    "\n",
    "# Just a helper function, don't worry about this one!\n",
    "def get_file_or_cache(path):\n",
    "    cache = None\n",
    "    def get():\n",
    "        nonlocal cache\n",
    "        if not cache:\n",
    "            with path.open('r', encoding='utf8') as f:\n",
    "                cache = f.readlines()\n",
    "        return cache\n",
    "    return (get, path.stem)\n",
    "\n",
    "# Construct a list of possible files and their names\n",
    "data_files = [get_file_or_cache(data_path / x) for x in [chess_filename, music_filename, happy_filename, trumpspeech_filename, wallstreetbets_filename, javascript_filename, shakespeare_filename]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b9852",
   "metadata": {},
   "source": [
    "No we have to data loaded, lets check out some sample lines from each source!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bbe2e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  chess\n",
      "Lines:\n",
      "1. e4 e6 2. d4 d5 3. Nc3 Nf6 4. e5 Nfd7 5. f4 g6 6. Nf3 Bg7 7. Bd3 O-O 8. O-O Re8 9. Be3 a6 10. Ng5 h6 11. Nf3 Qe7 12. Qd2 Nb6 13. f5 exf5 14. Bxh6 Be6 15. Bxg7 Kxg7 16. Ng5 Rh8 17. Qf4 Rh5 18. h4 c5 19. Be2 Nc6 20. Bxh5 gxh5 21. dxc5 Qxc5+ 22. Kh1 Qe7 23. Qf3 Rh8 24. Qg3 Kf8 25. Rad1 d4 26. Ne2 Qc5 27. c3 dxc3 28. Qxc3 Qxc3 29. Nxc3 Ke7 30. g3 Nc4 31. Rf2 Ne3 32. Rd3 Ng4 33. Re2 Nb4 34. Rd6 Rc8 35. Rb6 Rc4 36. Rxb7+ Kf8 37. Ra7 Rc6 38. Rd2 Bc4 39. b3 Be6 40. Rd4 Nc2 41. Rd6 Rxc3 42. Nxe6+ fxe6 43. Rd8# {Black checkmated} 1-0\n",
      "\n",
      "1. d4 g6 2. c4 Bg7 3. Nc3 d6 4. f3 Nc6 5. Be3 Nf6 6. Qd2 O-O 7. a3 Na5 8. Qd3 Be6 9. d5 Bf5 10. Ne4 Nxe4 11. fxe4 Bg4 12. Rb1 c5 13. h3 Bd7 14. b3 f5 15. b4 fxe4 16. Qc2 Be5 17. bxa5 Qxa5+ 18. Bd2 Bg3+ 19. Kd1 Rxf1+ {White resigns} 0-1\n",
      "\n",
      "1. d4 d5 2. c4 e6 3. Nf3 Nf6 4. Nc3 Be7 5. Bg5 O-O 6. e3 h6 7. Bh4 c6 8. Bd3 dxc4 9. Bxc4 b5 10. Bd3 Nd5 11. Bxe7 Qxe7 12. Nxd5 exd5 13. O-O Nd7 14. Rc1 Qd6 15. Qc2 Bb7 16. Kh1 Nf6 17. Ne5 Rac8 18. Bf5 Rc7 19. Rg1 g5 20. f4 gxf4 21. exf4 Qb4 22. Rgd1 Kh8 23. a3 Qd6 24. Rd3 Ne4 25. Re3 Nf6 26. Qc5 Qxc5 27. Rxc5 a5 28. Rh3 Kg7 29. Rg3+ Kh8 30. Rh3 Kg7 31. Rc1 Rh8 32. Rg3+ Kf8 33. Rb3 Re7 34. Kg1 Ne4 35. Nxc6 Rc7 36. Bxe4 dxe4 37. d5 f5 38. Rxb5 Kf7 39. Ne5+ {Black resigns} 1-0\n",
      "\n",
      "1. e4 c6 2. d4 d5 3. e5 c5 4. Nf3 cxd4 5. Nxd4 a6 6. Nd2 Nc6 7. Nxc6 bxc6 8. Bd3 e6 9. O-O Ne7 10. Qg4 c5 11. b3 Nc6 12. Nf3 Nb4 13. Bd2 a5 14. Rfc1 Ba6 15. Bxa6 Rxa6 16. Ng5 Qc7 17. a3 Nc6 18. Re1 Nd4 19. Bc3 h5 20. Qh3 Rc6 21. Qd3 Be7 22. Bd2 g6 23. c4 dxc4 24. bxc4 O-O 25. Nf3 a4 26. Bc3 Nxf3+ 27. Qxf3 {Black resigns} 1-0\n",
      "\n",
      "1. e4 g6 2. d4 a5 3. Nf3 d6 4. Nc3 c6 5. a4 Nf6 6. h3 Bg7 7. Be3 Na6 8. Be2 O-O 9. O-O Nb4 10. Qd2 Qb6 11. Rad1 Re8 12. Nh4 Bd7 13. f4 Qc7 14. f5 Qc8 15. Bh6 e5 16. fxg6 hxg6 17. Qg5 Nxc2 18. Nxg6 Nh7 19. Ne7+ Rxe7 20. Qxg7# {Black checkmated} 1-0\n",
      "\n",
      "Source:  music\n",
      "Lines:\n",
      "G3-A (Bcd=e) | f4 (g2dB) | ({d}c3-B) G2-E2 | F4 (D2=E^F) |\n",
      "\n",
      "G3-A (Bcd=e) | f4 d2-f2 | (g2a2 b2).g2 | {b}(a2g2 f2).d2 |\n",
      "\n",
      "(d2{ed}c2) B2B2 | (A2G2 {AG}F2).D2 | (GABc) (d2{ed}c>A) | G2G2 G2z ||\n",
      "\n",
      "G | B2c2 (dcAB) | G2G2 G3G | B2d2 (gfdc) | d2g2 (g3ga) |\n",
      "\n",
      "(bagf) (gd)d>c | (B2AG) F-D.D2 | (GABc) d2d2 | (bgfd) cA.F2 |\n",
      "\n",
      "Source:  happy_topical_chat\n",
      "Lines:\n",
      " Freshwater fish only drink water through the skin via Osmosis, Saltwater fish drink water through the mouth. Dolphins are friendly to human beings.\n",
      "\n",
      " Yeah apparently lol! They do that instead of hiring people to mow!\n",
      "\n",
      " no but I would love to! paying hourly to hang out with adorable cats? im in!\n",
      "\n",
      " i guess thats where the phrase \"cat nap\" comes from\n",
      "\n",
      " I love to dance a lot. How about you?\n",
      "\n",
      "Source:  trumpSpeech\n",
      "Lines:\n",
      "\n",
      "\n",
      "My fellow Americans, I want to speak to you tonight about the troubling events of the past week. As I have said, the incursion of the US Capitol struck at the very heart of our Republic. It angered and appalled millions of Americans across the political spectrum. I want to be very clear, I unequivocally condemn the violence that we saw last week. Violence and vandalism have absolutely no place in our country and no place in our movement.\n",
      "\n",
      "Making America Great Again has always been about defending the rule of law, supporting the men and women of law enforcement and upholding our nation’s most sacred traditions and values. Mob violence goes against everything I believe in and everything our movement stands for. No true supporter of mine could ever endorse political violence. No true supporter of mine could ever disrespect law enforcement or our great American flag. No true supporter of mine could ever threaten or harass their fellow Americans. If you do any of these things, you are not supporting our movement, you are attacking it and you are attacking our country. We can not tolerate it.\n",
      "\n",
      "Tragically, over the course of the past year made so difficult because of COVID-19, we have seen political violence spiral out of control. We have seen too many riots, too many mobs, too many acts of intimidation and destruction. It must stop. Whether you are on the right or on the left, a Democrat or a Republican, there is never a justification for violence, no excuses, no exceptions. America is a nation of laws. Those who engaged in the attacks last week will be brought to justice.\n",
      "\n",
      "Now I am asking everyone who has ever believed in our agenda to be thinking of ways to ease tensions, calm tempers, and help to promote peace in our country. There has been reporting that additional demonstrations are being planned in the coming days, both here in Washington and across the country. I have been briefed by the US Secret Service on the potential threats. Every American deserves to have their voice heard in a respectful and peaceful way. That is your First Amendment Right. But I cannot emphasize that there must be no violence, no law breaking and no vandalism of any kind.\n",
      "\n",
      "Source:  wallstreetbets_comments\n",
      "Lines:\n",
      "*sufficiently leveraged for personal risk tolerance\n",
      "\n",
      "Sir, looks like you’re... Stuck.... with GME.\n",
      "\n",
      "Tomorrow: buy short term Palantir and Comcast calls\n",
      "\n",
      "\n",
      "\n",
      "Wednesday: sell Palantir buy long term Discovery and Viacom calls\n",
      "\n",
      "Source:  javascript\n",
      "Lines:\n",
      "moment = this.moment;\n",
      "\n",
      "delete this.moment;\n",
      "\n",
      "var logger = require('./logger');\n",
      "\n",
      "var Storage = exports.Storage = function Storage()\n",
      "\n",
      "{\n",
      "\n",
      "Source:  shakespeare\n",
      "Lines:\n",
      "and had my pocket picked: this house is turned\n",
      "\n",
      "Exit BALTHASAR\n",
      "\n",
      "Giving more light than heat, extinct in both,\n",
      "\n",
      "Part us, Northumberland, I toward the north,\n",
      "\n",
      "Leave us to ourselves, and make yourself some comfort\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data_generator, data_name in data_files:\n",
    "    print(\"Source: \", data_name)\n",
    "    lines = data_generator()\n",
    "    print(\"Lines:\")\n",
    "    for line in lines[:5]:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa48c7f",
   "metadata": {},
   "source": [
    "# How do we *actually* predict?\n",
    "\n",
    "The goal is to learn a model which can tell the difference between classes. For example, consider sentences from Wikipedia (W) or Twitter (T). If we show our model a new input sentence (X) where we do NOT know the origin, our model should be able to tell where the sentence came from. \n",
    "\n",
    "Formally, the $i^{th}$ input is called $x_{i}$ and its true class is called $y_{i}$.\n",
    "For us, $x_{i}$ is the $i^{th}$ sentence we need to label and $y_{i}$ is where this sentence actually came from.\n",
    "Our model predicts some label, $\\hat y_{i}$, for this sentence (so *wikipedia* or *twitter*).\n",
    "We want to train it so that *our* label, $\\hat y_{n}$, is close to the *true* label, $y_{i}$.\n",
    "\n",
    "There are many, many different ways to do this prediction, and we will look at a simple one called Naive Bayes.\n",
    "\n",
    "Lets true a manual version to get the idea:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ac1aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shakespeare', 'trumpSpeech', 'wallstreetbets_comments']\n",
      "Thawing cold fear, that mean and gentle all,\n",
      "\n",
      "Where did this sentence come from? shakespeare or trumpSpeech or wallstreetbets_commentsshakespeare\n",
      "You were Right !\n",
      "Your predicted label (y^) was: shakespeare. The true label (y) was shakespeare\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "s, label_opt1 = random.choice(data_files)\n",
    "s, label_opt2 = random.choice(data_files)\n",
    "source_gen, true_label = random.choice(data_files)\n",
    "label_options = [label_opt1, label_opt2, true_label]\n",
    "random.shuffle(label_options)\n",
    "lines = source_gen()\n",
    "line = random.choice(lines)\n",
    "print(line)\n",
    "predicted_label = input(\"Where did this sentence come from? {} or {} or {}\".format(*label_options))\n",
    "print(\"You were\", \"Right\" if true_label == predicted_label else \"Wrong\", \"!\")\n",
    "print(\"Your predicted label (y^) was: {}. The true label (y) was {}\".format(predicted_label, true_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e61895",
   "metadata": {},
   "source": [
    "## Using a model to make a prediction\n",
    " \n",
    " How can we *train the computer* to predict a class for us? \n",
    " Lets think of a simple example: We are given a sentence ($x$), and need to predict where it came from ($\\hat y$), Wikipedia (W) or Twitter (T). We start by asking a friend where they think it came from, and they say it has a 70% chance of being from Wikipedia and 30% from Twitter. \n",
    "\n",
    " In mathmatical terms we can rewrite the \"_probability of this specific sentence $x$ being from Wikipedia, $\\hat y = W$, is 0.7_\" as $p(\\hat y = W | x) = 0.7$. Similarly, we can write $p(\\hat y = T | x) = 0.3$ for the sentence being from twitter.\n",
    "\n",
    " Now it is pretty easy to make a prediction, Wikipedia has a 70% chance and Twitter only has a 30% chance so we should predict Wikipedia! \n",
    "\n",
    " ### But how did our friend come up with $p(\\hat y| x)$ in the first place?\n",
    " This is basically what the computer needs to do, find the probability of a sentence coming from each source.\n",
    " This is what Naive Bayes solves!\n",
    "\n",
    " ## Naive Bayes - Probability theory (spooky)\n",
    "\n",
    " Note: Naive Bayes is pretty simple, and is quite intuitive when you wrap your head around it, but if this is your first introduction to probability it can be quite confusing! If you don't understand at first don't get discoraged! I find that drawing diagrams and thinking about it from a few directions helps really understand.\n",
    "\n",
    " The end goal of a model is to calculate $p(y = C|X=x)$. In plain english, this can be read as calculate the _probability that the true class of the input is C given what we know about the sentence_. For a concrete example, lets use the sentence \"The University of Auckland was founded on 23 May 1883\". We want to predict the probability that $y = Wikipedia$ or $y = Twitter$ given that the sentence $x$ = \"The University of Auckland was founded on 23 May 1883\". This can be difficult to calculate!\n",
    "\n",
    " Instead of calculating this directly, _Bayes Theorum_ gives us a way to swap things around.\n",
    "\n",
    " $$ p(y=C|X=x) = p(X=x|y=C)p(y=C)$$\n",
    " \n",
    " ## Intuition\n",
    " Instead of directly trying to work out the probability of wikipedia or twitter, lets look at indivdual words. If we see the words \"follow me!\" we can say twitter has a pretty high probability. On the other hand, if we see the words \"Auckland[1][2]\" we could say wikipedia has a high probability. How did we do this?\n",
    " \n",
    " We looked at the probability of each word (or few words) coming from each source! This is the $p(x=X|y=C)$ in the formula!\n",
    " \n",
    "Lets see if we can use this to do some machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a392b0",
   "metadata": {},
   "source": [
    "# Exercise 1: Calculate word probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa7d6ba",
   "metadata": {},
   "source": [
    "We need to *learn* 2 things from our data.\n",
    "1. What is the probability of seeing each word overall?\n",
    "    1. This is $p(x=X)$, or the *probability* that a word is X\n",
    "    2. e.g p('and') is high, it occurs a lot! But p('founded') is low, it doesn't occur too often.\n",
    "2. What is the probability of seeing each word *from each source*?\n",
    "    1. This is $p(x=X|y=Y)$, or the *probability* that a word is X if we *know* it is from e.g wikipedia\n",
    "    2. e.g. p('founded'|Twitter) might be pretty low, its an uncommon word on twitter. But p('founded'|Wikipedia) is high, it occurs all the time on wikipedia!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill word probabilities so it contains every word, and the probability of each\n",
    "# word_probabilities = {\n",
    "#       'and': 0.75,\n",
    "#       'founded': 0.01,\n",
    "#       ....\n",
    "#}\n",
    "word_probabilities = {}\n",
    "# fill word_probs_by_source so it contains a dict for each source\n",
    "# each containing each word and probability only in that source\n",
    "# word_probs_by_source = {\n",
    "#       'Twitter': {\n",
    "#              'founded': 0.001,\n",
    "#               ...,\n",
    "#        },\n",
    "#       'Wikipedia': {\n",
    "#              'founded': 0.05,\n",
    "#              ...,\n",
    "#       }\n",
    "#}\n",
    "word_probs_by_source = {}\n",
    "\n",
    "for data_generator, data_name in data_files:\n",
    "    lines = data_generator()\n",
    "    for line in lines:\n",
    "        words = lines.split()\n",
    "        for word in words:\n",
    "            # ..... Add code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660cd763",
   "metadata": {},
   "source": [
    "Now we can make predictions for single words! Lets write a function which takes a word an returns the most likely source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b894371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_word(word):\n",
    "    # .... Add code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94452de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "s, label_opt1 = random.choice(data_files)\n",
    "s, label_opt2 = random.choice(data_files)\n",
    "source_gen, true_label = random.choice(data_files)\n",
    "label_options = [label_opt1, label_opt2, true_label]\n",
    "random.shuffle(label_options)\n",
    "lines = source_gen()\n",
    "line = random.choice(lines)\n",
    "word = random.choice(line.split())\n",
    "print(\"Word is\", word)\n",
    "predicted_label = input(\"Where did this word come from? {} or {} or {}\".format(*labels_options))\n",
    "ML_predicted_label = predict_single_word\n",
    "print(\"You were\", \"Right\" if true_label == predicted_label else \"Wrong\", \"!\")\n",
    "print(\"The computer was\", \"Right\" if true_label == ML_predicted_label else \"Wrong\", \"!\")\n",
    "print(\"Your predicted label (y^) was: {}. The computer predicted {}. The true label (y) was {}\".format(predicted_label, ML_predicted_label, true_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
