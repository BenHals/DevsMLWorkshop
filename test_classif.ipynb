{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative ML\n",
    "\n",
    "The goal is to learn a model which can tell the difference between classes. For example, consider sentences from class1 = Wikipedia (C1) or class2 = Twitter (C2). If we show our model a new input sentence (X) where we do NOT know the origin, our model should be able to tell where the sentence came from. \n",
    "\n",
    "Formally, the $i^{th}$ input is called $x_{n}$ and its true class is called $y_{n}$. Our model predicts $\\hat y_{n}$ given $x_{n}$, and we want to train it so that $\\hat y_{n}$ is close to $y_{n}$.\n",
    "\n",
    "There are many, many different ways to do this prediction, and we will look at a simple one called Naive Bayes.\n",
    "\n",
    "## Using a model to make a prediction\n",
    " \n",
    " How can we predict a class? \n",
    " Lets think of a simple example: We are given a sentence ($x$), and need to predict where it came from ($\\hat y$), Wikipedia (C1) or Twitter (C2). We start by asking a friend where they think it came from, and they say it has a 70% chance of being from Wikipedia and 30% from Twitter. \n",
    "\n",
    " In mathmatical terms we can rewrite the \"_probability of this specific sentence $x$ being from Wikipedia, $\\hat y = C1$, is 0.7_\" as $p(\\hat y = C1 | x) = 0.7$. Similarly, we can write $p(\\hat y = C2 | x) = 0.3$ for the sentence being from twitter.\n",
    "\n",
    " Now it is pretty easy to make a prediction, Wikipedia has a 70% chance and Twitter only has a 30% chance so we should predict Wikipedia!\n",
    "\n",
    " ### But how did our friend come up with $p(\\hat y| x)$ in the first place?\n",
    "\n",
    " This is what Naive Bayes solves!\n",
    "\n",
    " ## Naive Bayes - Probability theory (spooky)\n",
    "\n",
    " Note: Naive Bayes is pretty simple, and is quite intuitive when you wrap your head around it, but if this is your first introduction to probability it can be quite confusing! If you don't understand at first don't get discoraged! I find that drawing diagrams and thinking about it from a few directions helps really understand.\n",
    "\n",
    " The end goal of a model is to calculate $p(y = C|X=x)$. In plain english, this can be read as calculate the _probability that the true class of the input is C given what we know about the sentence_. For a concrete example, lets use the sentence \"The University of Auckland was founded on 23 May 1883\". We want to predict the probability that $y = Wikipedia$ and $y = Twitter$ given that the sentence $x$ = \"The University of Auckland was founded on 23 May 1883\". This can be difficult to calculate!\n",
    "\n",
    " Instead of calculating this directly, _Bayes Theorum_ gives us a way to swap things around.\n",
    "\n",
    " $$ p(y=C|X=x) = p(X=x|y=C)p(y=C)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dependencies and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "data_path = pathlib.Path('data')\n",
    "chess_filename = \"chess.txt\"\n",
    "music_filename = \"music.txt\"\n",
    "angry_filename = \"angry_topical_chat.txt\"\n",
    "happy_filename = \"happy_topical_chat.txt\"\n",
    "disgusted_filename = \"disgusted_topical_chat.txt\"\n",
    "trumpspeech_filename = \"trumpSpeech.txt\"\n",
    "wallstreetbets_filename = \"wallstreetbets_comments.txt\"\n",
    "javascript_filename = \"javascript.txt\"\n",
    "shakespeare_filename = \"shakespeare.txt\"\n",
    "\n",
    "def get_file_or_cache(path):\n",
    "    cache = None\n",
    "    def get():\n",
    "        nonlocal cache\n",
    "        if not cache:\n",
    "            with path.open('r', encoding='utf8') as f:\n",
    "                cache = f.readlines()\n",
    "        return cache\n",
    "    return (get, path.stem)\n",
    "\n",
    "data_files = [get_file_or_cache(data_path / x) for x in [chess_filename, music_filename, happy_filename, trumpspeech_filename, wallstreetbets_filename, javascript_filename, shakespeare_filename]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct probabilities. Want p(source), p(word) and p(word|Source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_source = {}\n",
    "p_word = {}\n",
    "p_word_given_source = {}\n",
    "total_words = 0\n",
    "for source_constructor, source in data_files:\n",
    "    lines = source_constructor()\n",
    "    for line in lines:\n",
    "        for word in line.split(' '):\n",
    "            if p_source.get(source, 0) > 400000:\n",
    "                continue\n",
    "            p_source[source] = p_source.get(source, 0) + 1\n",
    "            p_word[word] = p_word.get(word, 0) + 1\n",
    "            source_conditional = p_word_given_source.get(source, {})\n",
    "            source_conditional[word] = source_conditional.get(word, 0) + 1\n",
    "            p_word_given_source[source] = source_conditional\n",
    "            total_words += 1\n",
    "num_unique_words = len(p_word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chess': 115493,\n",
       " 'music': 202479,\n",
       " 'happy_topical_chat': 400001,\n",
       " 'trumpSpeech': 400001,\n",
       " 'wallstreetbets_comments': 400001,\n",
       " 'javascript': 5848,\n",
       " 'shakespeare': 7330}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_word_from_source(word, source):\n",
    "    \"\"\" number of times word is seen in source / number of words in source\n",
    "    \"\"\"\n",
    "    # print(word)\n",
    "    source_count_smoothing = (p_word_given_source[source].get(word, 0) + 1)\n",
    "    source_total_smoothing = (p_source[source] + num_unique_words)\n",
    "    likelihood = math.log(source_count_smoothing) - math.log(source_total_smoothing)\n",
    "    # print(f\"word count: {source_count_smoothing} / {source_total_smoothing} = {likelihood}\")\n",
    "    return likelihood\n",
    "\n",
    "def likelihood_sentence_from_source(sentence, source):\n",
    "    \"\"\" Naive Bayes uses the product of words to get sentence\n",
    "    likelihood\n",
    "    \"\"\"\n",
    "    likelihood_product = 0\n",
    "    for word in sentence:\n",
    "        word_likelihood = likelihood_word_from_source(word, source)\n",
    "        \n",
    "        likelihood_product += word_likelihood\n",
    "        \n",
    "    return likelihood_product\n",
    "\n",
    "def likelihood_source_from_sentence(sentence, source, prior):\n",
    "    \"\"\" Using the bayes rule, p(A|B) = p(B|A)p(A) / p(B).\n",
    "    We ignore p(B) as it is the same for any A so we dont need\n",
    "    it to compare As.\n",
    "    Here A is the source and B is the sentence. p(A) is\n",
    "    the number of words in a source / total number of words seen.\n",
    "    \"\"\"\n",
    "    source_likelihood = (p_source[source] / total_words)\n",
    "    sentence_likelihood = likelihood_sentence_from_source(sentence, source)\n",
    "    # print(source, source_likelihood)\n",
    "    # print(sentence_likelihood)\n",
    "    return sentence_likelihood + prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chess\n",
      "-------\n",
      "Sentence is: 1. d4 Nf6 2. c4 e6 3. Nc3 Bb4 4. Qc2 d5 5. a3 Bxc3+ 6. Qxc3 Ne4 7. Qb3 dxc4 8. Qxc4 Nd7 9. Nf3 O-O 10. Bf4 Nd6 11. Bxd6 cxd6 12. Rc1 Nb6 13. Qc7 h6 14. Qxd8 Rxd8 15. e4 Bd7 16. Be2 Rac8 17. O-O a6 18. b3 Rc6 19. b4 Rdc8 20. Rxc6 Rxc6 21. e5 d5 22. b5 Rc3 23. bxa6 bxa6 24. Rb1 Nc4 25. Rb8+ Kh7 26. Rb7 Bb5 27. Rc7 Rc1+ 28. Bf1 Ra1 29. h3 Nxa3 30. Nh2 Bxf1 31. Nxf1 Nc4 32. f3 Nd2 33. Kf2 Nxf1 34. Rxf7 Nd2 35. Re7 Nc4 36. Rxe6 a5 37. Rc6 a4 38. e6 Ra2+ 39. Kg3 a3 40. Rc7 Re2 41. e7 a2 42. Ra7 Rxe7 43. Rxa2 Re3 44. Kf4 Rd3 45. Kg4 Rxd4+ 46. Kf5 Rd1 47. Re2 d4 48. Ra2 d3 49. Ra4 Rc1 50. Ra2 d2 51. Rxd2 Nxd2 52. f4 Rg1 {White resigns} 0-1\n",
      "\n",
      "True label, y = chess\n",
      "chess 0.07542877818219342\n",
      "-1104.6557750591624\n",
      "chess has likelihood -1106.6016852082178\n",
      "music 0.13223956064482126\n",
      "-1941.6145484590863\n",
      "music has likelihood -1943.5604586081417\n",
      "happy_topical_chat 0.26124169171859374\n",
      "-2077.7698904043286\n",
      "happy_topical_chat has likelihood -2079.715800553384\n",
      "trumpSpeech 0.26124169171859374\n",
      "-2058.5474167424145\n",
      "trumpSpeech has likelihood -2060.4933268914697\n",
      "wallstreetbets_comments 0.26124169171859374\n",
      "-2046.9479738495875\n",
      "wallstreetbets_comments has likelihood -2048.8938839986427\n",
      "javascript 0.0038193439845658796\n",
      "-1854.3730889793258\n",
      "javascript has likelihood -1856.3189991283812\n",
      "shakespeare 0.004787242032638149\n",
      "-1856.3888215734169\n",
      "shakespeare has likelihood -1858.3347317224723\n",
      "we predict yhat = chess\n",
      "We were right! :)\n",
      "-------\n",
      "music\n",
      "-------\n",
      "Sentence is: BGcA F2 F||c|AFcF AFcF|BGdG BGdG|AFcF AFcF|BGcA F2 Fc|\n",
      "\n",
      "True label, y = music\n",
      "chess 0.07542877818219342\n",
      "-98.61941659153878\n",
      "chess has likelihood -100.56532674059409\n",
      "music 0.13223956064482126\n",
      "-81.00176343131896\n",
      "music has likelihood -82.94767358037427\n",
      "happy_topical_chat 0.26124169171859374\n",
      "-105.1422363355059\n",
      "happy_topical_chat has likelihood -107.08814648456121\n",
      "trumpSpeech 0.26124169171859374\n",
      "-105.1422363355059\n",
      "trumpSpeech has likelihood -107.08814648456121\n",
      "wallstreetbets_comments 0.26124169171859374\n",
      "-105.1422363355059\n",
      "wallstreetbets_comments has likelihood -107.08814648456121\n",
      "javascript 0.0038193439845658796\n",
      "-93.30179064046898\n",
      "javascript has likelihood -95.24770078952429\n",
      "shakespeare 0.004787242032638149\n",
      "-93.40321114834784\n",
      "shakespeare has likelihood -95.34912129740314\n",
      "we predict yhat = music\n",
      "We were right! :)\n",
      "-------\n",
      "javascript\n",
      "-------\n",
      "Sentence is:     number_pool.pop();\n",
      "\n",
      "True label, y = javascript\n",
      "chess 0.07542877818219342\n",
      "-61.63713536971174\n",
      "chess has likelihood -63.58304551876706\n",
      "music 0.13223956064482126\n",
      "-25.599311479634856\n",
      "music has likelihood -27.54522162869017\n",
      "happy_topical_chat 0.26124169171859374\n",
      "-24.796424984485377\n",
      "happy_topical_chat has likelihood -26.742335133540692\n",
      "trumpSpeech 0.26124169171859374\n",
      "-65.71389770969118\n",
      "trumpSpeech has likelihood -67.65980785874649\n",
      "wallstreetbets_comments 0.26124169171859374\n",
      "-34.94544435130931\n",
      "wallstreetbets_comments has likelihood -36.89135450036463\n",
      "javascript 0.0038193439845658796\n",
      "-24.391621463219934\n",
      "javascript has likelihood -26.33753161227525\n",
      "shakespeare 0.004787242032638149\n",
      "-46.01283715428413\n",
      "shakespeare has likelihood -47.95874730333944\n",
      "we predict yhat = javascript\n",
      "We were right! :)\n",
      "-------\n",
      "javascript\n",
      "-------\n",
      "Sentence is:     _max_hp : 10,\n",
      "\n",
      "True label, y = javascript\n",
      "chess 0.07542877818219342\n",
      "-86.29198951759643\n",
      "chess has likelihood -88.23789966665174\n",
      "music 0.13223956064482126\n",
      "-50.90589250935654\n",
      "music has likelihood -52.851802658411856\n",
      "happy_topical_chat 0.26124169171859374\n",
      "-49.695689707241954\n",
      "happy_topical_chat has likelihood -51.64159985629727\n",
      "trumpSpeech 0.26124169171859374\n",
      "-91.99945679356766\n",
      "trumpSpeech has likelihood -93.94536694262297\n",
      "wallstreetbets_comments 0.26124169171859374\n",
      "-58.74609678539778\n",
      "wallstreetbets_comments has likelihood -60.6920069344531\n",
      "javascript 0.0038193439845658796\n",
      "-43.15272093186934\n",
      "javascript has likelihood -45.098631080924655\n",
      "shakespeare 0.004787242032638149\n",
      "-69.36363994137109\n",
      "shakespeare has likelihood -71.3095500904264\n",
      "we predict yhat = javascript\n",
      "We were right! :)\n",
      "-------\n",
      "wallstreetbets_comments\n",
      "-------\n",
      "Sentence is: [deleted]\n",
      "\n",
      "True label, y = wallstreetbets_comments\n",
      "chess 0.07542877818219342\n",
      "-12.32742707394235\n",
      "chess has likelihood -14.273337222997663\n",
      "music 0.13223956064482126\n",
      "-12.653290514860844\n",
      "music has likelihood -14.599200663916157\n",
      "happy_topical_chat 0.26124169171859374\n",
      "-13.142779541938236\n",
      "happy_topical_chat has likelihood -15.088689690993549\n",
      "trumpSpeech 0.26124169171859374\n",
      "-13.142779541938236\n",
      "trumpSpeech has likelihood -15.088689690993549\n",
      "wallstreetbets_comments 0.26124169171859374\n",
      "-6.619217235788724\n",
      "wallstreetbets_comments has likelihood -8.565127384844036\n",
      "javascript 0.0038193439845658796\n",
      "-11.662723830058622\n",
      "javascript has likelihood -13.608633979113936\n",
      "shakespeare 0.004787242032638149\n",
      "-11.675401393543478\n",
      "shakespeare has likelihood -13.621311542598791\n",
      "we predict yhat = wallstreetbets_comments\n",
      "We were right! :)\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for r in range(5):\n",
    "    # Prior is prob of drawning sentence from pool\n",
    "    priors = {s: math.log(p_source[s]) - math.log(total_words) for g, s in data_files}\n",
    "    # Prior uniformly pick source then sentence\n",
    "    priors = {s: 1 / len(data_files) for g, s in data_files}\n",
    "    data_index = random.choices(range(len(data_files)), [priors[s] for g,s in data_files ])[0]\n",
    "    get_data, name = data_files[data_index]\n",
    "    print(name)\n",
    "    lines = get_data()\n",
    "    sentence_str = random.choice(lines)\n",
    "    sentence = sentence_str.split(' ')\n",
    "    print(\"-------\")\n",
    "    print(f\"Sentence is: {sentence_str}\")\n",
    "    print(f\"True label, y = {name}\")\n",
    "    likelihoods = []\n",
    "    for source in p_source:\n",
    "        source_likelihood = likelihood_source_from_sentence(sentence, source, priors[source])\n",
    "        print(f\"{source} has likelihood {source_likelihood}\")\n",
    "        likelihoods.append((source_likelihood, source))\n",
    "    likelihoods.sort(key = lambda x: x[0], reverse=True)\n",
    "    print(f\"we predict yhat = {likelihoods[0][1]}\")\n",
    "    print(f\"We were {'right! :)' if likelihoods[0][1] == name else 'wrong. :('}\")\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "right = 0\n",
    "wrong = 0\n",
    "for r in range(10000):\n",
    "    # Prior is prob of drawning sentence from pool\n",
    "    priors = {s: p_source[s] / total_words for g, s in data_files}\n",
    "    # Prior uniformly pick source then sentence\n",
    "    priors = {s: 1 / len(data_files) for g, s in data_files}\n",
    "    data_index = random.choices(range(len(data_files)), [priors[s] for g,s in data_files ])[0]\n",
    "    get_data, name = data_files[data_index]\n",
    "    lines = get_data()\n",
    "    sentence_str = random.choice(lines)\n",
    "    sentence = sentence_str.split(' ')\n",
    "    likelihoods = []\n",
    "    for source in p_source:\n",
    "        source_likelihood = likelihood_source_from_sentence(sentence, source, priors[source])\n",
    "        likelihoods.append((source_likelihood, source))\n",
    "    likelihoods.sort(key = lambda x: x[0], reverse=True)\n",
    "    correct = likelihoods[0][1] == name\n",
    "    if correct:\n",
    "        right += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "print(f\"Accuracy: {right / (right + wrong)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "source_IDs = {}\n",
    "ID_to_source = {}\n",
    "source_ID = 0\n",
    "for source_constructor, source in data_files:\n",
    "    source_IDs[source] = source_ID\n",
    "    ID_to_source[source_ID] = source\n",
    "    lines = source_constructor()\n",
    "    for line in lines:\n",
    "        X.append(line)\n",
    "        Y.append(source_ID)\n",
    "    source_ID += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X_cv = cv.fit_transform(X)\n",
    "X_cv, X, Y = shuffle(X_cv, X, Y) \n",
    "train_len = int(len(X) * 0.9)\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X_cv, Y)\n",
    "X_train = X_cv[:train_len]\n",
    "X_test = X_cv[train_len:]\n",
    "Y_train = Y[:train_len]\n",
    "Y_test = Y[train_len:]\n",
    "X_sentence_train = X[:train_len]\n",
    "X_sentence_test = X[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193341"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([], dtype='<U1411')]\n",
      "wallstreetbets_comments\n",
      "wallstreetbets_comments\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i = 3\n",
    "sentence = X_test[i]\n",
    "yhat = clf.predict(X_test[0])[0]\n",
    "print(cv.inverse_transform(sentence))\n",
    "print(ID_to_source[yhat])\n",
    "print(ID_to_source[Y_test[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1074006/1074006 [00:58<00:00, 18208.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "p_source = {}\n",
    "p_word = {}\n",
    "p_word_given_source = {}\n",
    "total_words = 0\n",
    "for line_cv, line, source_cv in tqdm.tqdm(zip(X_train, X_sentence_train, Y_train), total = X_train.shape[0]):\n",
    "    source = ID_to_source[source_cv]\n",
    "    # print(line)\n",
    "    # print(source)\n",
    "    for word in line.split(' '):\n",
    "        p_source[source] = p_source.get(source, 0) + 1\n",
    "        p_word[word] = p_word.get(word, 0) + 1\n",
    "        source_conditional = p_word_given_source.get(source, {})\n",
    "        source_conditional[word] = source_conditional.get(word, 0) + 1\n",
    "        p_word_given_source[source] = source_conditional\n",
    "        total_words += 1\n",
    "num_unique_words = len(p_word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119335/119335 [10:09<00:00, 195.67it/s]Our Acc: 0.9392382787949889\n",
      "SK Acc: 0.9791008505467801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "right = 0\n",
    "wrong = 0\n",
    "sklearn_right = 0\n",
    "sklearn_wrong = 0\n",
    "for r in tqdm.tqdm(range(X_test.shape[0])):\n",
    "    # Prior is prob of drawning sentence from pool\n",
    "    priors = {s: math.log(p_source[s]) - math.log(total_words) for g, s in data_files}\n",
    "    # Prior uniformly pick source then sentence\n",
    "    # priors = {s: 1 / len(data_files) for g, s in data_files}\n",
    "    sentence_cv = X_test[r]\n",
    "    source_ID = Y_test[r]\n",
    "    # print(sentence)\n",
    "    # print(source_ID)\n",
    "    sentence = X_sentence_test[r]\n",
    "    source = ID_to_source[source_ID]\n",
    "    sentence_str = ' '.join(sentence)\n",
    "    likelihoods = []\n",
    "    for s in p_source:\n",
    "        source_likelihood = likelihood_source_from_sentence(sentence, s, priors[s])\n",
    "        likelihoods.append((source_likelihood, s))\n",
    "    likelihoods.sort(key = lambda x: x[0], reverse=True)\n",
    "    # print(likelihoods)\n",
    "    our_yhat = likelihoods[0][1]\n",
    "    correct = our_yhat == source\n",
    "    if correct:\n",
    "        right += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "    \n",
    "    sklearn_yhat  = clf.predict(sentence_cv)[0]\n",
    "    sk_correct = sklearn_yhat == source_ID\n",
    "    if sk_correct:\n",
    "        sklearn_right += 1\n",
    "    else:\n",
    "        sklearn_wrong += 1\n",
    "    if not correct and False:\n",
    "        print(sentence)\n",
    "        print(source_ID)\n",
    "        print(likelihoods)\n",
    "        print(f\"Ours {our_yhat} Theirs {sklearn_yhat} GT: {source}\")\n",
    "print(f\"Our Acc: {right / (right + wrong)}\")\n",
    "print(f\"SK Acc: {sklearn_right / (sklearn_right + sklearn_wrong)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_word_given_source['wallstreetbets_comments']['[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
